<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 03 Nov 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>sparklyr.flint 0.2: ASOF Joins, OLS Regression, and additional summarizers</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</link>
      <description>


&lt;p&gt;Since &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/index.html"&gt;&lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/a&gt;, a &lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; extension for leveraging &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; time series functionalities through &lt;code&gt;sparklyr&lt;/code&gt;, was &lt;a href="https://blogs.rstudio.com/ai/posts/2020-09-07-sparklyr-flint"&gt;introduced&lt;/a&gt; in September, we have made a number of enhancements to it, and have successfully submitted &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 to CRAN.&lt;/p&gt;
&lt;p&gt;In this blog post, we highlight the following new features and improvements from &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#asof-joins"&gt;ASOF Joins&lt;/a&gt; of Timeseries RDDs&lt;/li&gt;
&lt;li&gt;&lt;a href="#ols-regression"&gt;OLS Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#additional-summarizers"&gt;Additional Summarizers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="asof-joins"&gt;ASOF Joins&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the term, ASOF joins are temporal join operations based on inexact matching of timestamps. Within the context of &lt;a href="https://spark.apache.org"&gt;Apache Spark&lt;/a&gt;, a join operation, loosely speaking, matches records from two data frames (let’s call them &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;) based on some criteria. A temporal join implies matching records in &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; based on timestamps, and with inexact matching of timestamps permitted, it is typically useful to join &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; along one of the following temporal directions:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Looking behind: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t&lt;/code&gt;, then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the most recent timestamp less than or equal to &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Looking ahead: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t,&lt;/code&gt; then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the smallest timestamp greater than or equal to (or alternatively, strictly greater than) &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, oftentimes it is not useful to consider two timestamps as “matching” if they are too far apart. Therefore, an additional constraint on the maximum amount of time to look behind or look ahead is usually also part of an ASOF join operation.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, all ASOF join functionalities of Flint are accessible via the &lt;code&gt;asof_join()&lt;/code&gt; method. For example, given 2 timeseries RDDs &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(sparklyr)
library(sparklyr.flint)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
left &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10), u = seq(10))) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)
right &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10) + 1, v = seq(10) + 1L)) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following prints the result of matching each record from &lt;code&gt;left&lt;/code&gt; with the most recent record(s) from &lt;code&gt;right&lt;/code&gt; that are at most 1 second behind.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;gt;=&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1    NA
##  2 1970-01-01 00:00:02     2     2
##  3 1970-01-01 00:00:03     3     3
##  4 1970-01-01 00:00:04     4     4
##  5 1970-01-01 00:00:05     5     5
##  6 1970-01-01 00:00:06     6     6
##  7 1970-01-01 00:00:07     7     7
##  8 1970-01-01 00:00:08     8     8
##  9 1970-01-01 00:00:09     9     9
## 10 1970-01-01 00:00:10    10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas if we change the temporal direction to “&amp;lt;”, then each record from &lt;code&gt;left&lt;/code&gt; will be matched with any record(s) from &lt;code&gt;right&lt;/code&gt; that is strictly in the future and is at most 1 second ahead of the current record from &lt;code&gt;left&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;lt;&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1     2
##  2 1970-01-01 00:00:02     2     3
##  3 1970-01-01 00:00:03     3     4
##  4 1970-01-01 00:00:04     4     5
##  5 1970-01-01 00:00:05     5     6
##  6 1970-01-01 00:00:06     6     7
##  7 1970-01-01 00:00:07     7     8
##  8 1970-01-01 00:00:08     8     9
##  9 1970-01-01 00:00:09     9    10
## 10 1970-01-01 00:00:10    10    11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice regardless of which temporal direction is selected, an outer-left join is always performed (i.e., all timestamp values and &lt;code&gt;u&lt;/code&gt; values of &lt;code&gt;left&lt;/code&gt; from above will always be present in the output, and the &lt;code&gt;v&lt;/code&gt; column in the output will contain &lt;code&gt;NA&lt;/code&gt; whenever there is no record from &lt;code&gt;right&lt;/code&gt; that meets the matching criteria).&lt;/p&gt;
&lt;h2 id="ols-regression"&gt;OLS Regression&lt;/h2&gt;
&lt;p&gt;You might be wondering whether the version of this functionality in Flint is more or less identical to &lt;code&gt;lm()&lt;/code&gt; in R. Turns out it has much more to offer than &lt;code&gt;lm()&lt;/code&gt; does. An OLS regression in Flint will compute useful metrics such as &lt;a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"&gt;Akaike information criterion&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion"&gt;Bayesian information criterion&lt;/a&gt;, both of which are useful for model selection purposes, and the calculations of both are parallelized by Flint to fully utilize computational power available in a Spark cluster. In addition, Flint supports ignoring regressors that are constant or nearly constant, which becomes useful when an intercept term is included. To see why this is the case, we need to briefly examine the goal of the OLS regression, which is to find some column vector of coefficients &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt; that minimizes &lt;span class="math inline"&gt;\(\|\mathbf{y} - \mathbf{X} \mathbf{\beta}\|^2\)&lt;/span&gt;, where &lt;span class="math inline"&gt;\(\mathbf{y}\)&lt;/span&gt; is the column vector of response variables, and &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is a matrix consisting of columns of regressors plus an entire column of &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s representing the intercept terms. The solution to this problem is &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt;, assuming the Gram matrix &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; is non-singular. However, if &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; contains a column of all &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s of intercept terms, and another column formed by a regressor that is constant (or nearly so), then columns of &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; will be linearly dependent (or nearly so) and &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; will be singular (or nearly so), which presents an issue computation-wise. However, if a regressor is constant, then it essentially plays the same role as the intercept terms do. So simply excluding such a constant regressor in &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; solves the problem. Also, speaking of inverting the Gram matrix, readers remembering the concept of “condition number” from numerical analysis must be thinking to themselves how computing &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt; could be numerically unstable if &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; has a large condition number. This is why Flint also outputs the condition number of the Gram matrix in the OLS regression result, so that one can sanity-check the underlying quadratic minimization problem being solved is well-conditioned.&lt;/p&gt;
&lt;p&gt;So, to summarize, the OLS regression functionality implemented in Flint not only outputs the solution to the problem, but also calculates useful metrics that help data scientists assess the sanity and predictive quality of the resulting model.&lt;/p&gt;
&lt;p&gt;To see OLS regression in action with &lt;code&gt;sparklyr.flint&lt;/code&gt;, one can run the following example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mtcars_sdf &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE) %&amp;gt;%
  dplyr::mutate(time = 0L)
mtcars_ts &amp;lt;- from_sdf(mtcars_sdf, is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;)
model &amp;lt;- ols_regression(mtcars_ts, mpg ~ hp + wt) %&amp;gt;% to_sdf()

print(model %&amp;gt;% dplyr::select(akaikeIC, bayesIC, cond))

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##   akaikeIC bayesIC    cond
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     155.    159. 345403.

# ^ output says condition number of the Gram matrix was within reason&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and obtain &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt;, the vector of optimal coefficients, with the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(model %&amp;gt;% dplyr::pull(beta))

## [[1]]
## [1] -0.03177295 -3.87783074&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="additional-summarizers"&gt;Additional Summarizers&lt;/h2&gt;
&lt;p&gt;The EWMA (Exponential Weighted Moving Average), EMA half-life, and the standardized moment summarizers (namely, skewness and kurtosis) along with a few others which were missing in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 are now fully supported in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2.&lt;/p&gt;
&lt;h2 id="better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;While &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 included a &lt;code&gt;collect()&lt;/code&gt; method for exporting data from a Flint time-series RDD to an R data frame, it did not have a similar method for extracting the underlying Spark data frame from a Flint time-series RDD. This was clearly an oversight. In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, one can call &lt;code&gt;to_sdf()&lt;/code&gt; on a timeseries RDD to get back a Spark data frame that is usable in &lt;code&gt;sparklyr&lt;/code&gt; (e.g., as shown by &lt;code&gt;model %&amp;gt;% to_sdf() %&amp;gt;% dplyr::select(...)&lt;/code&gt; examples from above). One can also get to the underlying Spark data frame JVM object reference by calling &lt;code&gt;spark_dataframe()&lt;/code&gt; on a Flint time-series RDD (this is usually unnecessary in vast majority of &lt;code&gt;sparklyr&lt;/code&gt; use cases though).&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have presented a number of new features and improvements introduced in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 and deep-dived into some of them in this blog post. We hope you are as excited about them as we are.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;The author would like to thank Mara (&lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;), Sigrid (&lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;), and Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) for their fantastic editorial inputs on this blog post!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4341b8d32399655443d0be1f44b15976</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</guid>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released/images/sparklyr-flint-0.2.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.4: Weighted Sampling, Tidyr Verbs, Robust Scaler, RAPIDS, and more</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</link>
      <description>


&lt;p&gt;&lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; 1.4 is now available on &lt;a href="https://cran.r-project.org/web/packages/sparklyr/index.html"&gt;CRAN&lt;/a&gt;! To install &lt;code&gt;sparklyr&lt;/code&gt; 1.4 from CRAN, run&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this blog post, we will showcase the following much-anticipated new functionalities from the &lt;code&gt;sparklyr&lt;/code&gt; 1.4 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#parallelized-weighted-sampling"&gt;Parallelized Weighted Sampling&lt;/a&gt; with Spark&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="#tidyr-verbs"&gt;Tidyr Verbs&lt;/a&gt; on Spark Dataframes&lt;/li&gt;
&lt;li&gt;&lt;a href="#robust-scaler"&gt;&lt;code&gt;ft_robust_scaler&lt;/code&gt;&lt;/a&gt; as the R interface for &lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; from Spark 3.0&lt;/li&gt;
&lt;li&gt;Option for enabling &lt;a href="#rapids"&gt;&lt;code&gt;RAPIDS&lt;/code&gt;&lt;/a&gt; GPU acceleration plugin in &lt;code&gt;spark_connect()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#higher-order-functions-and-dplyr-related-improvements"&gt;Higher-order functions and &lt;code&gt;dplyr&lt;/code&gt;-related improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="parallelized-weighted-sampling"&gt;Parallelized Weighted Sampling&lt;/h2&gt;
&lt;p&gt;Readers familiar with &lt;code&gt;dplyr::sample_n()&lt;/code&gt; and &lt;code&gt;dplyr::sample_frac()&lt;/code&gt; functions may have noticed that both of them support weighted-sampling use cases on R dataframes, e.g.,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dplyr::sample_n(mtcars, size = 3, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;               mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Fiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
Merc 280C     17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dplyr::sample_frac(mtcars, size = 0.1, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Honda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Merc 450SE  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Fiat X1-9   27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will select some random subset of &lt;code&gt;mtcars&lt;/code&gt; using the &lt;code&gt;mpg&lt;/code&gt; attribute as the sampling weight for each row. If &lt;code&gt;replace = FALSE&lt;/code&gt; is set, then a row is removed from the sampling population once it gets selected, whereas when setting &lt;code&gt;replace = TRUE&lt;/code&gt;, each row will always stay in the sampling population and can be selected multiple times.&lt;/p&gt;
&lt;p&gt;Now the exact same use cases are supported for Spark dataframes in &lt;code&gt;sparklyr&lt;/code&gt; 1.4! For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
mtcars_sdf &amp;lt;- copy_to(sc, mtcars, repartition = 4L)

dplyr::sample_n(mtcars_sdf, size = 5, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will return a random subset of size 5 from the Spark dataframe &lt;code&gt;mtcars_sdf&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More importantly, the sampling algorithm implemented in &lt;code&gt;sparklyr&lt;/code&gt; 1.4 is something that fits perfectly into the MapReduce paradigm: as we have split our &lt;code&gt;mtcars&lt;/code&gt; data into 4 partitions of &lt;code&gt;mtcars_sdf&lt;/code&gt; by specifying &lt;code&gt;repartition = 4L&lt;/code&gt;, the algorithm will first process each partition independently and in parallel, selecting a sample set of size up to 5 from each, and then reduce all 4 sample sets into a final sample set of size 5 by choosing records having the top 5 highest sampling priorities among all.&lt;/p&gt;
&lt;p&gt;How is such parallelization possible, especially for the sampling without replacement scenario, where the desired result is defined as the outcome of a sequential process? A detailed answer to this question is in &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-29-parallelized-sampling/"&gt;this blog post&lt;/a&gt;, which includes a definition of the problem (in particular, the exact meaning of sampling weights in term of probabilities), a high-level explanation of the current solution and the motivation behind it, and also, some mathematical details all hidden in one link to a PDF file, so that non-math-oriented readers can get the gist of everything else without getting scared away, while math-oriented readers can enjoy working out all the integrals themselves before peeking at the answer.&lt;/p&gt;
&lt;h2 id="tidyr-verbs"&gt;Tidyr Verbs&lt;/h2&gt;
&lt;p&gt;The specialized implementations of the following &lt;a href="https://tidyr.tidyverse.org/"&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt; verbs that work efficiently with Spark dataframes were included as part of &lt;code&gt;sparklyr&lt;/code&gt; 1.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/fill.html"&gt;&lt;code&gt;tidyr::fill&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/nest.html"&gt;&lt;code&gt;tidyr::nest&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/nest.html"&gt;&lt;code&gt;tidyr::unnest&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/pivot_wider.html"&gt;&lt;code&gt;tidyr::pivot_wider&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/pivot_longer.html"&gt;&lt;code&gt;tidyr::pivot_longer&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/separate.html"&gt;&lt;code&gt;tidyr::separate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/unite.html"&gt;&lt;code&gt;tidyr::unite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can demonstrate how those verbs are useful for tidying data through some examples.&lt;/p&gt;
&lt;p&gt;Let’s say we are given &lt;code&gt;mtcars_sdf&lt;/code&gt;, a Spark dataframe containing all rows from &lt;code&gt;mtcars&lt;/code&gt; plus the name of each row:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
mtcars_sdf &amp;lt;- cbind(
  data.frame(model = rownames(mtcars)),
  data.frame(mtcars, row.names = NULL)
) %&amp;gt;%
  copy_to(sc, ., repartition = 4L)

print(mtcars_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model          mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4
2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4
3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1
4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1
5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we would like to turn all numeric attributes in &lt;code&gt;mtcar_sdf&lt;/code&gt; (in other words, all columns other than the &lt;code&gt;model&lt;/code&gt; column) into key-value pairs stored in 2 columns, with the &lt;code&gt;key&lt;/code&gt; column storing the name of each attribute, and the &lt;code&gt;value&lt;/code&gt; column storing each attribute’s numeric value. One way to accomplish that with &lt;code&gt;tidyr&lt;/code&gt; is by utilizing the &lt;code&gt;tidyr::pivot_longer&lt;/code&gt; functionality:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_kv_sdf &amp;lt;- mtcars_sdf %&amp;gt;%
  tidyr::pivot_longer(cols = -model, names_to = &amp;quot;key&amp;quot;, values_to = &amp;quot;value&amp;quot;)
print(mtcars_kv_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 3]
  model     key   value
  &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4 am      1
2 Mazda RX4 carb    4
3 Mazda RX4 cyl     6
4 Mazda RX4 disp  160
5 Mazda RX4 drat    3.9
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To undo the effect of &lt;code&gt;tidyr::pivot_longer&lt;/code&gt;, we can apply &lt;code&gt;tidyr::pivot_wider&lt;/code&gt; to our &lt;code&gt;mtcars_kv_sdf&lt;/code&gt; Spark dataframe, and get back the original data that was present in &lt;code&gt;mtcars_sdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tbl &amp;lt;- mtcars_kv_sdf %&amp;gt;%
  tidyr::pivot_wider(names_from = key, values_from = value)
print(tbl, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model         carb   cyl  drat    hp   mpg    vs    wt    am  disp  gear  qsec
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4        4     6  3.9    110  21       0  2.62     1  160      4  16.5
2 Hornet 4 Dr…     1     6  3.08   110  21.4     1  3.22     0  258      3  19.4
3 Hornet Spor…     2     8  3.15   175  18.7     0  3.44     0  360      3  17.0
4 Merc 280C        4     6  3.92   123  17.8     1  3.44     0  168.     4  18.9
5 Merc 450SLC      3     8  3.07   180  15.2     0  3.78     0  276.     3  18
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to reduce many columns into fewer ones is by using &lt;code&gt;tidyr::nest&lt;/code&gt; to move some columns into nested tables. For instance, we can create a nested table &lt;code&gt;perf&lt;/code&gt; encapsulating all performance-related attributes from &lt;code&gt;mtcars&lt;/code&gt; (namely, &lt;code&gt;hp&lt;/code&gt;, &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;qsec&lt;/code&gt;). However, unlike R dataframes, Spark Dataframes do not have the concept of nested tables, and the closest to nested tables we can get is a &lt;code&gt;perf&lt;/code&gt; column containing named structs with &lt;code&gt;hp&lt;/code&gt;, &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;qsec&lt;/code&gt; attributes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_nested_sdf &amp;lt;- mtcars_sdf %&amp;gt;%
  tidyr::nest(perf = c(hp, mpg, disp, qsec))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then inspect the type of &lt;code&gt;perf&lt;/code&gt; column in &lt;code&gt;mtcars_nested_sdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sdf_schema(mtcars_nested_sdf)$perf$type&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;ArrayType(StructType(StructField(hp,DoubleType,true), StructField(mpg,DoubleType,true), StructField(disp,DoubleType,true), StructField(qsec,DoubleType,true)),true)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and inspect individual struct elements within &lt;code&gt;perf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;perf &amp;lt;- mtcars_nested_sdf %&amp;gt;% dplyr::pull(perf)
unlist(perf[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    hp    mpg   disp   qsec
110.00  21.00 160.00  16.46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can also use &lt;code&gt;tidyr::unnest&lt;/code&gt; to undo the effects of &lt;code&gt;tidyr::nest&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_unnested_sdf &amp;lt;- mtcars_nested_sdf %&amp;gt;%
  tidyr::unnest(col = perf)
print(mtcars_unnested_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model          cyl  drat    wt    vs    am  gear  carb    hp   mpg  disp  qsec
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4        6  3.9   2.62     0     1     4     4   110  21    160   16.5
2 Hornet 4 Dr…     6  3.08  3.22     1     0     3     1   110  21.4  258   19.4
3 Duster 360       8  3.21  3.57     0     0     3     4   245  14.3  360   15.8
4 Merc 280         6  3.92  3.44     1     0     4     4   123  19.2  168.  18.3
5 Lincoln Con…     8  3     5.42     0     0     3     4   215  10.4  460   17.8
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="robust-scaler"&gt;Robust Scaler&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; is a new functionality introduced in Spark 3.0 (&lt;a href="https://issues.apache.org/jira/browse/SPARK-28399"&gt;SPARK-28399&lt;/a&gt;). Thanks to a &lt;a href="https://github.com/sparklyr/sparklyr/pull/2254"&gt;pull request&lt;/a&gt; by &lt;a href="https://github.com/zero323"&gt;@zero323&lt;/a&gt;, an R interface for &lt;code&gt;RobustScaler&lt;/code&gt;, namely, the &lt;code&gt;ft_robust_scaler()&lt;/code&gt; function, is now part of &lt;code&gt;sparklyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is often observed that many machine learning algorithms perform better on numeric inputs that are standardized. Many of us have learned in stats 101 that given a random variable &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, we can compute its mean &lt;span class="math inline"&gt;\(\mu = E[X]\)&lt;/span&gt;, standard deviation &lt;span class="math inline"&gt;\(\sigma = \sqrt{E[X^2] - (E[X])^2}\)&lt;/span&gt;, and then obtain a standard score &lt;span class="math inline"&gt;\(z = \frac{X - \mu}{\sigma}\)&lt;/span&gt; which has mean of 0 and standard deviation of 1.&lt;/p&gt;
&lt;p&gt;However, notice both &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(E[X^2]\)&lt;/span&gt; from above are quantities that can be easily skewed by extreme outliers in &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, causing distortions in &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;. A particular bad case of it would be if all non-outliers among &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; are very close to &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, hence making &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; close to &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, while extreme outliers are all far in the negative direction, hence dragging down &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; while skewing &lt;span class="math inline"&gt;\(E[X^2]\)&lt;/span&gt; upwards.&lt;/p&gt;
&lt;p&gt;An alternative way of standardizing &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; based on its median, 1st quartile, and 3rd quartile values, all of which are robust against outliers, would be the following:&lt;/p&gt;
&lt;p&gt;&lt;span class="math inline"&gt;\(\displaystyle z = \frac{X - \text{Median}(X)}{\text{P75}(X) - \text{P25}(X)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and this is precisely what &lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; offers.&lt;/p&gt;
&lt;p&gt;To see &lt;code&gt;ft_robust_scaler()&lt;/code&gt; in action and demonstrate its usefulness, we can go through a contrived example consisting of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw 500 random samples from the standard normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;sample_values &amp;lt;- rnorm(500)
print(sample_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772
  [6] -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the minimal and maximal values among the &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; random samples:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;print(min(sample_values))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] -3.008049&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;print(max(sample_values))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] 3.810277&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now create &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; other values that are extreme outliers compared to the &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; random samples above. Given that we know all &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; samples are within the range of &lt;span class="math inline"&gt;\((-4, 4)\)&lt;/span&gt;, we can choose &lt;span class="math inline"&gt;\(-501, -502, \ldots, -509, -510\)&lt;/span&gt; as our &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; outliers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;outliers &amp;lt;- -500L - seq(10)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Copy all &lt;span class="math inline"&gt;\(510\)&lt;/span&gt; values into a Spark dataframe named &lt;code&gt;sdf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;)
sdf &amp;lt;- copy_to(sc, data.frame(value = c(sample_values, outliers)))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can then apply &lt;code&gt;ft_robust_scaler()&lt;/code&gt; to obtain the standardized value for each input:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;scaled &amp;lt;- sdf %&amp;gt;%
  ft_vector_assembler(&amp;quot;value&amp;quot;, &amp;quot;input&amp;quot;) %&amp;gt;%
  ft_robust_scaler(&amp;quot;input&amp;quot;, &amp;quot;scaled&amp;quot;) %&amp;gt;%
  dplyr::pull(scaled) %&amp;gt;%
  unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the result shows the non-outlier data points being scaled to values that still more or less form a bell-shaped distribution centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, as expected, so the scaling is robust against influence of the outliers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggplot2)

ggplot(data.frame(scaled = scaled), aes(x = scaled)) +
  xlim(-7, 7) +
  geom_histogram(binwidth = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-30-sparklyr-1.4.0-released/images/scaled.png" id="id" class="class" style="width:60.0%;height:60.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can compare the distribution of the scaled values above with the distribution of z-scores of all input values, and notice how scaling the input with only mean and standard deviation would have caused noticeable skewness – which the robust scaler has successfully avoided:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_values &amp;lt;- c(sample_values, outliers)
z_scores &amp;lt;- (all_values - mean(all_values)) / sd(all_values)
ggplot(data.frame(scaled = z_scores), aes(x = scaled)) +
  xlim(-0.05, 0.2) +
  geom_histogram(binwidth = 0.005)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-30-sparklyr-1.4.0-released/images/skewed.png" id="id" class="class" style="width:60.0%;height:60.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the 2 plots above, one can observe while both standardization processes produced some distributions that were still bell-shaped, the one produced by &lt;code&gt;ft_robust_scaler()&lt;/code&gt; is centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, correctly indicating the average among all non-outlier values, while the z-score distribution is clearly not centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; as its center has been noticeably shifted by the &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; outlier values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rapids"&gt;RAPIDS&lt;/h2&gt;
&lt;p&gt;Readers following Apache Spark releases closely probably have noticed the recent addition of &lt;a href="https://rapids.ai/"&gt;RAPIDS&lt;/a&gt; GPU acceleration support in Spark 3.0. Catching up with this recent development, an option to enable RAPIDS in Spark connections was also created in &lt;code&gt;sparklyr&lt;/code&gt; and shipped in &lt;code&gt;sparklyr&lt;/code&gt; 1.4. On a host with RAPIDS-capable hardware (e.g., an Amazon EC2 instance of type ‘p3.2xlarge’), one can install &lt;code&gt;sparklyr&lt;/code&gt; 1.4 and observe RAPIDS hardware acceleration being reflected in Spark SQL physical query plans:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;, packages = &amp;quot;rapids&amp;quot;)
dplyr::db_explain(sc, &amp;quot;SELECT 4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Physical Plan ==
*(2) GpuColumnarToRow false
+- GpuProject [4 AS 4#45]
   +- GpuRowToColumnar TargetSize(2147483647)
      +- *(1) Scan OneRowRelation[]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="higher-order-functions-and-dplyr-related-improvements"&gt;Higher-Order Functions and &lt;code&gt;dplyr&lt;/code&gt;-Related Improvements&lt;/h2&gt;
&lt;p&gt;All newly introduced higher-order functions from Spark 3.0, such as &lt;code&gt;array_sort()&lt;/code&gt; with custom comparator, &lt;code&gt;transform_keys()&lt;/code&gt;, &lt;code&gt;transform_values()&lt;/code&gt;, and &lt;code&gt;map_zip_with()&lt;/code&gt;, are supported by &lt;code&gt;sparklyr&lt;/code&gt; 1.4.&lt;/p&gt;
&lt;p&gt;In addition, all higher-order functions can now be accessed directly through &lt;code&gt;dplyr&lt;/code&gt; rather than their &lt;code&gt;hof_*&lt;/code&gt; counterparts in &lt;code&gt;sparklyr&lt;/code&gt;. This means, for example, that we can run the following &lt;code&gt;dplyr&lt;/code&gt; queries to calculate the square of all array elements in column &lt;code&gt;x&lt;/code&gt; of &lt;code&gt;sdf&lt;/code&gt;, and then sort them in descending order:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;)
sdf &amp;lt;- copy_to(sc, tibble::tibble(x = list(c(-3, -2, 1, 5), c(6, -7, 5, 8))))

sq_desc &amp;lt;- sdf %&amp;gt;%
  dplyr::mutate(x = transform(x, ~ .x * .x)) %&amp;gt;%
  dplyr::mutate(x = array_sort(x, ~ as.integer(sign(.y - .x)))) %&amp;gt;%
  dplyr::pull(x)

print(sq_desc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 25  9  4  1

[[2]]
[1] 64 49 36 25&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;In chronological order, we would like to thank the following individuals for their contributions to &lt;code&gt;sparklyr&lt;/code&gt; 1.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https:://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/nealrichardson"&gt;@nealrichardson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/yitao-li"&gt;@yitao-li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/wkdavis"&gt;@wkdavis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/Loquats"&gt;@Loquats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/zero323"&gt;@zero323&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also appreciate bug reports, feature requests, and valuable other feedback about &lt;code&gt;sparklyr&lt;/code&gt; from our awesome open-source community (e.g., the weighted sampling feature in &lt;code&gt;sparklyr&lt;/code&gt; 1.4 was largely motivated by this &lt;a href="https://github.com/sparklyr/sparklyr/issues/2592"&gt;Github issue&lt;/a&gt; filed by &lt;a href="https://github.com/ajing"&gt;@ajing&lt;/a&gt;, and some &lt;code&gt;dplyr&lt;/code&gt;-related bug fixes in this release were initiated in &lt;a href="https://github.com/sparklyr/sparklyr/issues/2648"&gt;#2648&lt;/a&gt; and completed with this &lt;a href="https://github.com/sparklyr/sparklyr/pull/2651"&gt;pull request&lt;/a&gt; by &lt;a href="https:://github.com/wkdavis"&gt;@wkdavis&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Last but not least, the author of this blog post is extremely grateful for fantastic editorial suggestions from &lt;a href="https:://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;, &lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;, and &lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you wish to learn more about &lt;code&gt;sparklyr&lt;/code&gt;, we recommend checking out &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, and also some of the previous release posts such as &lt;a href="https://blog.rstudio.com/2020/07/16/sparklyr-1-3/"&gt;sparklyr 1.3&lt;/a&gt; and &lt;a href="https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/"&gt;sparklyr 1.2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">19cbb456871dc547ae3c7f20c08fb027</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</guid>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released/images/sparklyr-1.4.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Please allow me to introduce myself: Torch for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</link>
      <description>


&lt;p&gt;Last January at &lt;a href="https://rstudio.com/conference/"&gt;rstudio::conf&lt;/a&gt;, in that distant past when conferences still used to take place at some physical location, my colleague &lt;a href="https://twitter.com/dfalbel"&gt;Daniel&lt;/a&gt; gave a talk introducing new features and ongoing development in the &lt;code&gt;tensorflow&lt;/code&gt; ecosystem. In the Q&amp;amp;A part, he was asked something unexpected: Were we going to build support for &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;? He hesitated; that was in fact the plan, and he had already played around with natively implementing &lt;code&gt;torch&lt;/code&gt; tensors at a prior time, but he was not completely certain how well “it” would work.&lt;/p&gt;
&lt;p&gt;“It”, that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via &lt;code&gt;reticulate&lt;/code&gt;. Instead, we delegate to the underlying C++ library &lt;code&gt;libtorch&lt;/code&gt; for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, &lt;code&gt;torch&lt;/code&gt; does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.&lt;/p&gt;
&lt;p&gt;So why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against &lt;code&gt;libtorch&lt;/code&gt; would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that &lt;code&gt;torch&lt;/code&gt; can be useful to the R community. Thus, without further ado, let’s train a neural network.&lt;/p&gt;
&lt;p&gt;You’re not at your laptop now? Just follow along in the &lt;a href="https://colab.research.google.com/drive/1NdiN9n_a7NEvFpvjPDvxKTshrSWgxZK5?usp=sharing"&gt;companion notebook on Colaboratory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;h4 id="torch"&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Installing &lt;code&gt;torch&lt;/code&gt; is as straightforward as typing&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will detect whether you have CUDA installed, and either download the CPU or the GPU version of &lt;code&gt;libtorch&lt;/code&gt;. Then, it will install the R package from CRAN. To make use of the very newest features, you can install the development version from GitHub:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;mlverse/torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To quickly check the installation, and whether GPU support works fine (assuming that there &lt;em&gt;is&lt;/em&gt; a CUDA-capable NVidia GPU), create a tensor &lt;em&gt;on the CUDA device&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;torch_tensor(1, device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
[ CUDAFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all our &lt;em&gt;hello torch&lt;/em&gt; example did was run a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="torchvision"&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Whereas &lt;code&gt;torch&lt;/code&gt; is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.&lt;/p&gt;
&lt;p&gt;As of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan”, because &lt;code&gt;torchtext&lt;/code&gt; and &lt;code&gt;torchaudio&lt;/code&gt; are yet to be created. Right now, &lt;code&gt;torchvision&lt;/code&gt; is all we need:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;mlverse/torchvision&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to load the data.&lt;/p&gt;
&lt;h2 id="data-loading-and-pre-processing"&gt;Data loading and pre-processing&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(torchvision)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The list of vision datasets bundled with PyTorch is long, and they’re continually being added to &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin”, &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt; &lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt;. Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution &lt;code&gt;28x28&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are the first 32 characters:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-29-introducing-torch-for-r/images/kmnist.png" alt="Kuzushiji MNIST." width="768" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Kuzushiji MNIST.
&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="dataset"&gt;Dataset&lt;/h4&gt;
&lt;p&gt;The following code will download the data separately for training and test sets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds &amp;lt;- kmnist_dataset(
  &amp;quot;.&amp;quot;,
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)

test_ds &amp;lt;- kmnist_dataset(
  &amp;quot;.&amp;quot;,
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;transform&lt;/code&gt; argument. &lt;code&gt;transform_to_tensor&lt;/code&gt; takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?&lt;/p&gt;
&lt;p&gt;Contrary to what you might expect – if until now, you’ve been using &lt;code&gt;keras&lt;/code&gt; – the additional dimension is &lt;em&gt;not&lt;/em&gt; the batch dimension. Batching will be taken care of by the &lt;code&gt;dataloader&lt;/code&gt;, to be introduced next. Instead, this is the &lt;em&gt;channels&lt;/em&gt; dimension that in &lt;code&gt;torch&lt;/code&gt;, is found &lt;em&gt;before&lt;/em&gt; the width and height dimensions by default.&lt;/p&gt;
&lt;p&gt;One thing I’ve found to be extremely useful about &lt;code&gt;torch&lt;/code&gt; is how easy it is to inspect objects. Even though we’re dealing with a &lt;code&gt;dataset&lt;/code&gt;, a custom object, and not an R array or even a &lt;code&gt;torch&lt;/code&gt; tensor, we can easily peek at what’s inside. Indexing in &lt;code&gt;torch&lt;/code&gt; is 1-based, conforming to the R user’s intuitions. Consequently,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives us the first element in the dataset, an R &lt;em&gt;list&lt;/em&gt; of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)&lt;/p&gt;
&lt;p&gt;Let’s inspect the shape of the input tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1][[1]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 28 28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In &lt;code&gt;torch&lt;/code&gt;, this is the task of data loaders.&lt;/p&gt;
&lt;h4 id="data-loader"&gt;Data loader&lt;/h4&gt;
&lt;p&gt;Each of the training and test sets gets their own data loader:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dl &amp;lt;- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl &amp;lt;- dataloader(test_ds, batch_size = 32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, &lt;code&gt;torch&lt;/code&gt; makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_iter &amp;lt;- train_dl$.iter()
train_iter$.next()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functionality like this may not seem indispensable when working with a well-known dataset, but it will turn out to be very useful when a lot of domain-specific pre-processing is required.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(4,8), mar = rep(0, 4))
images &amp;lt;- train_dl$.iter()$.next()[[1]][1:32, 1, , ] 
images %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re ready to define our network – a simple convnet.&lt;/p&gt;
&lt;h2 id="network"&gt;Network&lt;/h2&gt;
&lt;p&gt;If you’ve been using &lt;code&gt;keras&lt;/code&gt; &lt;em&gt;custom models&lt;/em&gt; (or have some experience with &lt;em&gt;Py&lt;/em&gt;Torch), the following way of defining a network may not look too surprising.&lt;/p&gt;
&lt;p&gt;You use &lt;code&gt;nn_module()&lt;/code&gt; to define an R6 class that will hold the network’s components. Its layers are created in &lt;code&gt;initialize()&lt;/code&gt;; &lt;code&gt;forward()&lt;/code&gt; describes what happens during the network’s forward pass. One thing on terminology: In &lt;code&gt;torch&lt;/code&gt;, layers are called &lt;em&gt;modules&lt;/em&gt;, as are networks. This makes sense: The design is truly &lt;em&gt;modular&lt;/em&gt; in that any module can be used as a component in a larger one.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;net &amp;lt;- nn_module(
  
  &amp;quot;KMNIST-CNN&amp;quot;,
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    self$conv1 &amp;lt;- nn_conv2d(1, 32, 3)
    self$conv2 &amp;lt;- nn_conv2d(32, 64, 3)
    self$dropout1 &amp;lt;- nn_dropout2d(0.25)
    self$dropout2 &amp;lt;- nn_dropout2d(0.5)
    self$fc1 &amp;lt;- nn_linear(9216, 128)
    self$fc2 &amp;lt;- nn_linear(128, 10)
  },
  
  forward = function(x) {
    x %&amp;gt;% 
      self$conv1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$conv2() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      nnf_max_pool2d(2) %&amp;gt;%
      self$dropout1() %&amp;gt;%
      torch_flatten(start_dim = 2) %&amp;gt;%
      self$fc1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$dropout2() %&amp;gt;%
      self$fc2()
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The layers – apologies: modules – themselves may look familiar. Unsurprisingly, &lt;code&gt;nn_conv2d()&lt;/code&gt; performs two-dimensional convolution; &lt;code&gt;nn_linear()&lt;/code&gt; multiplies by a weight matrix and adds a vector of biases. But what are those numbers: &lt;code&gt;nn_linear(128, 10)&lt;/code&gt;, say?&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, &lt;code&gt;nn_linear(128, 10)&lt;/code&gt; has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about the previous module? How do we arrive at &lt;code&gt;9216&lt;/code&gt; input connections?&lt;/p&gt;
&lt;p&gt;Here, a bit of calculation is necessary. We go through all actions that happen in &lt;code&gt;forward()&lt;/code&gt; – if they affect shapes, we keep track of the transformation; if they don’t, we ignore them.&lt;/p&gt;
&lt;p&gt;So, we start with input tensors of shape &lt;code&gt;batch_size x 1 x 28 x 28&lt;/code&gt;. Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(1, 32, 3)&lt;/code&gt; , or equivalently, &lt;code&gt;nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),&lt;/code&gt;applies a convolution with kernel size 3, stride 1 (the default), and no padding (the default). We can consult the &lt;a href="https://mlverse.github.io/torch/reference/nn_conv2d.html"&gt;documentation&lt;/a&gt; to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of &lt;code&gt;26 x 26&lt;/code&gt;. &lt;em&gt;Per channel&lt;/em&gt;, that is. Thus, the actual output shape is &lt;code&gt;batch_size x 32 x 26 x 26&lt;/code&gt; . Next,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; applies ReLU activation, in no way touching the shape. Next is&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(32, 64, 3)&lt;/code&gt;, another convolution with zero padding and kernel size 3. Output size now is &lt;code&gt;batch_size x 64 x 24 x 24&lt;/code&gt; . Now, the second&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; again does nothing to the output shape, but&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_max_pool2d(2)&lt;/code&gt; (equivalently: &lt;code&gt;nnf_max_pool2d(kernel_size = 2)&lt;/code&gt;) does: It applies max pooling over regions of extension &lt;code&gt;2 x 2&lt;/code&gt;, thus downsizing the output to a format of &lt;code&gt;batch_size x 64 x 12 x 12&lt;/code&gt; . Now,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_dropout2d(0.25)&lt;/code&gt; is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the &lt;em&gt;channels&lt;/em&gt;, &lt;em&gt;height&lt;/em&gt; and &lt;em&gt;width&lt;/em&gt; axes into a single dimension. This is done in&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;torch_flatten(start_dim = 2)&lt;/code&gt;. Output shape is now &lt;code&gt;batch_size * 9216&lt;/code&gt; , since &lt;code&gt;64 * 12 * 12 = 9216&lt;/code&gt; . Thus here we have the &lt;code&gt;9216&lt;/code&gt; input connections fed into the&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(9216, 128)&lt;/code&gt; discussed above. Again,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; and &lt;code&gt;nn_dropout2d(0.5)&lt;/code&gt; leave dimensions as they are, and finally,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(128, 10)&lt;/code&gt; gives us the desired output scores, one for each of the ten classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with &lt;code&gt;torch&lt;/code&gt;’s flexibility, there is another way. Since every layer is callable &lt;em&gt;in isolation&lt;/em&gt;, we can just … create some sample data and see what happens!&lt;/p&gt;
&lt;p&gt;Here is a sample “image” – or more precisely, a one-item batch containing it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- torch_randn(c(1, 1, 28, 28))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we call the first &lt;em&gt;conv2d&lt;/em&gt; module on it?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv1 &amp;lt;- nn_conv2d(1, 32, 3)
conv1(x)$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 32 26 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or both &lt;em&gt;conv2d&lt;/em&gt; modules?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv2 &amp;lt;- nn_conv2d(32, 64, 3)
(conv1(x) %&amp;gt;% conv2())$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 64 24 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And so on. This is just one example illustrating how &lt;code&gt;torch&lt;/code&gt;s flexibility makes developing neural nets easier.&lt;/p&gt;
&lt;p&gt;Back to the main thread. We instantiate the model, and we ask &lt;code&gt;torch&lt;/code&gt; to allocate its weights (parameters) on the GPU:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- net()
model$to(device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.&lt;/p&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the loss function? For classification with more than two classes, we use &lt;em&gt;cross entropy&lt;/em&gt;, in &lt;code&gt;torch&lt;/code&gt;: &lt;code&gt;nnf_cross_entropy(prediction, ground_truth)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# this will be called for every batch, see training loop below
loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike categorical cross entropy in &lt;code&gt;keras&lt;/code&gt; , which would expect &lt;code&gt;prediction&lt;/code&gt; to contain probabilities, as obtained by applying a &lt;em&gt;softmax&lt;/em&gt; activation, &lt;code&gt;torch&lt;/code&gt;’s &lt;code&gt;nnf_cross_entropy()&lt;/code&gt; works with the raw outputs (the &lt;em&gt;logits&lt;/em&gt;). This is why the network’s last linear layer was not followed by any activation.&lt;/p&gt;
&lt;p&gt;The training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in 1:5) {

  l &amp;lt;- c()

  for (b in enumerate(train_dl)) {
    # make sure each batch&amp;#39;s gradient updates are calculated from a fresh start
    optimizer$zero_grad()
    # get model predictions
    output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate loss
    loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate gradient
    loss$backward()
    # apply weight updates
    optimizer$step()
    # track losses
    l &amp;lt;- c(l, loss$item())
  }

  cat(sprintf(&amp;quot;Loss at epoch %d: %3f\n&amp;quot;, epoch, mean(l)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: 1.795564
Loss at epoch 2: 1.540063
Loss at epoch 3: 1.495343
Loss at epoch 4: 1.461649
Loss at epoch 5: 1.446628&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there is a lot more that &lt;em&gt;could&lt;/em&gt; be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a &lt;code&gt;torch&lt;/code&gt; training loop.&lt;/p&gt;
&lt;p&gt;The optimizer-related idioms in particular&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$zero_grad()
# ...
loss$backward()
# ...
optimizer$step()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you’ll keep encountering over and over.&lt;/p&gt;
&lt;p&gt;Finally, let’s evaluate model performance on the test set.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Putting a model in &lt;code&gt;eval&lt;/code&gt; mode tells &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;not&lt;/em&gt; to calculate gradients and perform backprop during the operations that follow:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We iterate over the test set, keeping track of losses and accuracies obtained on the batches.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_losses &amp;lt;- c()
total &amp;lt;- 0
correct &amp;lt;- 0

for (b in enumerate(test_dl)) {
  output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
  labels &amp;lt;- b[[2]]$to(device = &amp;quot;cuda&amp;quot;)
  loss &amp;lt;- nnf_cross_entropy(output, labels)
  test_losses &amp;lt;- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values 
  # and position 2 containing the respective indices
  predicted &amp;lt;- torch_max(output$data(), dim = 2)[[2]]
  total &amp;lt;- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct &amp;lt;- correct + (predicted == labels)$sum()$item()
}

mean(test_losses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.53784480643349&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is mean accuracy, computed as proportion of correct classifications:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_accuracy &amp;lt;-  correct/total
test_accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9449&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for our first &lt;code&gt;torch&lt;/code&gt; example. Where to from here?&lt;/p&gt;
&lt;h2 id="learn"&gt;Learn&lt;/h2&gt;
&lt;p&gt;To learn more, check out our vignettes on the &lt;a href="https://mlverse.github.io/torch"&gt;&lt;code&gt;torch&lt;/code&gt; website&lt;/a&gt;. To begin, you may want to check out these in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Getting started” series: Build a simple neural network from scratch, starting from &lt;a href="https://mlverse.github.io/tohttps://mlverse.github.io/torch/articles/getting-started/tensors.html"&gt;low-level tensor manipulation&lt;/a&gt; and gradually adding in higher-level features like &lt;a href="https://mlverse.github.io/torch/articles/getting-started/tensors-and-autograd.html"&gt;automatic differentiation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/getting-started/nn.html"&gt;network modules&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;More on tensors: &lt;a href="https://mlverse.github.io/torch/articles/tensor-creation.html"&gt;Tensor creation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/indexing.html"&gt;indexing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Backpropagation in &lt;code&gt;torch&lt;/code&gt;: &lt;a href="https://mlverse.github.io/torch/articles/using-autograd.html"&gt;autograd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have questions, or run into problems, please feel free to ask on &lt;a href="https://github.com/mlverse/torch"&gt;GitHub&lt;/a&gt; or on the &lt;a href="https://community.rstudio.com/c/ml/15"&gt;RStudio community forum&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="we-need-you"&gt;We need you&lt;/h2&gt;
&lt;p&gt;We very much hope that the R community will find the new functionality useful. But that’s not all. We hope that you, many of you, will take part in the journey.&lt;/p&gt;
&lt;p&gt;There is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.&lt;/p&gt;
&lt;p&gt;There is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps &lt;em&gt;the&lt;/em&gt; essential factor in how usable a framework is.&lt;/p&gt;
&lt;p&gt;Then, there is the ever-expanding ecosystem of libraries built on top of PyTorch: &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/CrypTen"&gt;CrypTen&lt;/a&gt; for privacy-preserving machine learning, &lt;a href="https://github.com/rusty1s/pytorch_geometric"&gt;PyTorch Geometric&lt;/a&gt; for deep learning on manifolds, and &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt; for probabilistic programming, to name just a few.&lt;/p&gt;
&lt;p&gt;All this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely &lt;em&gt;any&lt;/em&gt; scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add or improve documentation, add introductory examples&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement missing layers (modules), activations, helper functions…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement model architectures&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Port some of the PyTorch ecosystem&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One component that should be of special interest to the R community is &lt;a href="https://pytorch.org/docs/stable/distributions.html"&gt;Torch distributions&lt;/a&gt;, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt;; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.&lt;/p&gt;
&lt;p&gt;To reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with &lt;code&gt;torch&lt;/code&gt;, and thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-clanuwat2018deep"&gt;
&lt;p&gt;Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. &lt;a href="http://arxiv.org/abs/cs.CV/1812.01718"&gt;http://arxiv.org/abs/cs.CV/1812.01718&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;In a nutshell, &lt;a href="https://twitter.com/javierluraschi"&gt;Javier&lt;/a&gt; had the idea of wrapping &lt;code&gt;libtorch&lt;/code&gt; into &lt;a href="https://github.com/mlverse/lantern"&gt;lantern&lt;/a&gt;, a C interface to &lt;code&gt;libtorch&lt;/code&gt;, thus avoiding cross-compiler issues between MinGW and Visual Studio.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4e028984b62685f317fbdf6861edc437</distill:md5>
      <category>Packages/Releases</category>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</guid>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/pt.png" medium="image" type="image/png" width="919" height="264"/>
    </item>
    <item>
      <title>sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</link>
      <description>


&lt;p&gt;&lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; 1.3 is now available on &lt;a href="https://cran.r-project.org/web/packages/sparklyr/index.html"&gt;CRAN&lt;/a&gt;, with the following major new features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#higher-order-functions"&gt;Higher-order Functions&lt;/a&gt; to easily manipulate arrays and structs&lt;/li&gt;
&lt;li&gt;Support for Apache &lt;a href="#avro"&gt;Avro&lt;/a&gt;, a row-oriented data serialization framework&lt;/li&gt;
&lt;li&gt;&lt;a href="#custom-serialization"&gt;Custom Serialization&lt;/a&gt; using R functions to read and write any data format&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-improvements"&gt;Other Improvements&lt;/a&gt; such as compatibility with EMR 6.0 &amp;amp; Spark 3.0, and initial support for Flint time series library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install &lt;code&gt;sparklyr&lt;/code&gt; 1.3 from CRAN, run&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this post, we shall highlight some major new features introduced in sparklyr 1.3, and showcase scenarios where such features come in handy. While a number of enhancements and bug fixes (especially those related to &lt;code&gt;spark_apply()&lt;/code&gt;, &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and secondary Spark connections) were also an important part of this release, they will not be the topic of this post, and it will be an easy exercise for the reader to find out more about them from the sparklyr &lt;a href="https://github.com/sparklyr/sparklyr/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;h2 id="higher-order-functions"&gt;Higher-order Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/SPARK-19480"&gt;Higher-order functions&lt;/a&gt; are built-in Spark SQL constructs that allow user-defined lambda expressions to be applied efficiently to complex data types such as arrays and structs. As a quick demo to see why higher-order functions are useful, let’s say one day Scrooge McDuck dove into his huge vault of money and found large quantities of pennies, nickels, dimes, and quarters. Having an impeccable taste in data structures, he decided to store the quantities and face values of everything into two Spark SQL array columns:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4.5&amp;quot;)
coins_tbl &amp;lt;- copy_to(
  sc,
  tibble::tibble(
    quantities = list(c(4000, 3000, 2000, 1000)),
    values = list(c(1, 5, 10, 25))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus declaring his net worth of 4k pennies, 3k nickels, 2k dimes, and 1k quarters. To help Scrooge McDuck calculate the total value of each type of coin in sparklyr 1.3 or above, we can apply &lt;code&gt;hof_zip_with()&lt;/code&gt;, the sparklyr equivalent of &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html#zip_with"&gt;ZIP_WITH&lt;/a&gt;, to &lt;code&gt;quantities&lt;/code&gt; column and &lt;code&gt;values&lt;/code&gt; column, combining pairs of elements from arrays in both columns. As you might have guessed, we also need to specify how to combine those elements, and what better way to accomplish that than a concise one-sided formula   &lt;code&gt;~ .x * .y&lt;/code&gt;   in R, which says we want (quantity * value) for each type of coin? So, we have the following:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result_tbl &amp;lt;- coins_tbl %&amp;gt;%
  hof_zip_with(~ .x * .y, dest_col = total_values) %&amp;gt;%
  dplyr::select(total_values)

result_tbl %&amp;gt;% dplyr::pull(total_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  4000 15000 20000 25000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the result &lt;code&gt;4000 15000 20000 25000&lt;/code&gt; telling us there are in total $40 dollars worth of pennies, $150 dollars worth of nickels, $200 dollars worth of dimes, and $250 dollars worth of quarters, as expected.&lt;/p&gt;
&lt;p&gt;Using another sparklyr function named &lt;code&gt;hof_aggregate()&lt;/code&gt;, which performs an &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html#aggregate"&gt;AGGREGATE&lt;/a&gt; operation in Spark, we can then compute the net worth of Scrooge McDuck based on &lt;code&gt;result_tbl&lt;/code&gt;, storing the result in a new column named &lt;code&gt;total&lt;/code&gt;. Notice for this aggregate operation to work, we need to ensure the starting value of aggregation has data type (namely, &lt;code&gt;BIGINT&lt;/code&gt;) that is consistent with the data type of &lt;code&gt;total_values&lt;/code&gt; (which is &lt;code&gt;ARRAY&amp;lt;BIGINT&amp;gt;&lt;/code&gt;), as shown below:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result_tbl %&amp;gt;%
  dplyr::mutate(zero = dplyr::sql(&amp;quot;CAST (0 AS BIGINT)&amp;quot;)) %&amp;gt;%
  hof_aggregate(start = zero, ~ .x + .y, expr = total_values, dest_col = total) %&amp;gt;%
  dplyr::select(total) %&amp;gt;%
  dplyr::pull(total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 64000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So Scrooge McDuck’s net worth is $640 dollars.&lt;/p&gt;
&lt;p&gt;Other higher-order functions supported by Spark SQL so far include &lt;code&gt;transform&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;exists&lt;/code&gt;, as documented in &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html"&gt;here&lt;/a&gt;, and similar to the example above, their counterparts (namely, &lt;code&gt;hof_transform()&lt;/code&gt;, &lt;code&gt;hof_filter()&lt;/code&gt;, and &lt;code&gt;hof_exists()&lt;/code&gt;) all exist in sparklyr 1.3, so that they can be integrated with other &lt;code&gt;dplyr&lt;/code&gt; verbs in an idiomatic manner in R.&lt;/p&gt;
&lt;h2 id="avro"&gt;Avro&lt;/h2&gt;
&lt;p&gt;Another highlight of the sparklyr 1.3 release is its built-in support for Avro data sources. Apache Avro is a widely used data serialization protocol that combines the efficiency of a binary data format with the flexibility of JSON schema definitions. To make working with Avro data sources simpler, in sparklyr 1.3, as soon as a Spark connection is instantiated with &lt;code&gt;spark_connect(..., package = "avro")&lt;/code&gt;, sparklyr will automatically figure out which version of &lt;code&gt;spark-avro&lt;/code&gt; package to use with that connection, saving a lot of potential headaches for sparklyr users trying to determine the correct version of &lt;code&gt;spark-avro&lt;/code&gt; by themselves. Similar to how &lt;code&gt;spark_read_csv()&lt;/code&gt; and &lt;code&gt;spark_write_csv()&lt;/code&gt; are in place to work with CSV data, &lt;code&gt;spark_read_avro()&lt;/code&gt; and &lt;code&gt;spark_write_avro()&lt;/code&gt; methods were implemented in sparklyr 1.3 to facilitate reading and writing Avro files through an Avro-capable Spark connection, as illustrated in the example below:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

# The `package = &amp;quot;avro&amp;quot;` option is only supported in Spark 2.4 or higher
sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4.5&amp;quot;, package = &amp;quot;avro&amp;quot;)

sdf &amp;lt;- sdf_copy_to(
  sc,
  tibble::tibble(
    a = c(1, NaN, 3, 4, NaN),
    b = c(-2L, 0L, 1L, 3L, 2L),
    c = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;d&amp;quot;)
  )
)

# This example Avro schema is a JSON string that essentially says all columns
# (&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;) of `sdf` are nullable.
avro_schema &amp;lt;- jsonlite::toJSON(list(
  type = &amp;quot;record&amp;quot;,
  name = &amp;quot;topLevelRecord&amp;quot;,
  fields = list(
    list(name = &amp;quot;a&amp;quot;, type = list(&amp;quot;double&amp;quot;, &amp;quot;null&amp;quot;)),
    list(name = &amp;quot;b&amp;quot;, type = list(&amp;quot;int&amp;quot;, &amp;quot;null&amp;quot;)),
    list(name = &amp;quot;c&amp;quot;, type = list(&amp;quot;string&amp;quot;, &amp;quot;null&amp;quot;))
  )
), auto_unbox = TRUE)

# persist the Spark data frame from above in Avro format
spark_write_avro(sdf, &amp;quot;/tmp/data.avro&amp;quot;, as.character(avro_schema))

# and then read the same data frame back
spark_read_avro(sc, &amp;quot;/tmp/data.avro&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;data&amp;gt; [?? x 3]
      a     b c
  &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;
  1     1    -2 &amp;quot;a&amp;quot;
  2   NaN     0 &amp;quot;b&amp;quot;
  3     3     1 &amp;quot;c&amp;quot;
  4     4     3 &amp;quot;&amp;quot;
  5   NaN     2 &amp;quot;d&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="custom-serialization"&gt;Custom Serialization&lt;/h2&gt;
&lt;p&gt;In addition to commonly used data serialization formats such as CSV, JSON, Parquet, and Avro, starting from sparklyr 1.3, customized data frame serialization and deserialization procedures implemented in R can also be run on Spark workers via the newly implemented &lt;code&gt;spark_read()&lt;/code&gt; and &lt;code&gt;spark_write()&lt;/code&gt; methods. We can see both of them in action through a quick example below, where &lt;code&gt;saveRDS()&lt;/code&gt; is called from a user-defined writer function to save all rows within a Spark data frame into 2 RDS files on disk, and &lt;code&gt;readRDS()&lt;/code&gt; is called from a user-defined reader function to read the data from the RDS files back to Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
sdf &amp;lt;- sdf_len(sc, 7)
paths &amp;lt;- c(&amp;quot;/tmp/file1.RDS&amp;quot;, &amp;quot;/tmp/file2.RDS&amp;quot;)

spark_write(sdf, writer = function(df, path) saveRDS(df, path), paths = paths)
spark_read(sc, paths, reader = function(path) readRDS(path), columns = c(id = &amp;quot;integer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 1]
     id
  &amp;lt;int&amp;gt;
1     1
2     2
3     3
4     4
5     5
6     6
7     7&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="other-improvements"&gt;Other Improvements&lt;/h2&gt;
&lt;h3 id="sparklyr.flint"&gt;Sparklyr.flint&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/r-spark/sparklyr.flint"&gt;Sparklyr.flint&lt;/a&gt; is a sparklyr extension that aims to make functionalities from the &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; time-series library easily accessible from R. It is currently under active development. One piece of good news is that, while the original &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; library was designed to work with Spark 2.x, a slightly modified &lt;a href="https://github.com/yl790/flint"&gt;fork&lt;/a&gt; of it will work well with Spark 3.0, and within the existing sparklyr extension framework. &lt;code&gt;sparklyr.flint&lt;/code&gt; can automatically determine which version of the Flint library to load based on the version of Spark it’s connected to. Another bit of good news is, as previously mentioned, &lt;code&gt;sparklyr.flint&lt;/code&gt; doesn’t know too much about its own destiny yet. Maybe you can play an active part in shaping its future!&lt;/p&gt;
&lt;h3 id="emr-6.0"&gt;EMR 6.0&lt;/h3&gt;
&lt;p&gt;This release also features a small but important change that allows sparklyr to correctly connect to the version of Spark 2.4 that is included in Amazon EMR 6.0.&lt;/p&gt;
&lt;p&gt;Previously, sparklyr automatically assumed any Spark 2.x it was connecting to was built with Scala 2.11 and attempted to load any required Scala artifacts built with Scala 2.11 as well. This became problematic when connecting to Spark 2.4 from Amazon EMR 6.0, which is built with Scala 2.12. Starting from sparklyr 1.3, such problem can be fixed by simply specifying &lt;code&gt;scala_version = "2.12"&lt;/code&gt; when calling &lt;code&gt;spark_connect()&lt;/code&gt; (e.g., &lt;code&gt;spark_connect(master = "yarn-client", scala_version = "2.12")&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id="spark-3.0"&gt;Spark 3.0&lt;/h3&gt;
&lt;p&gt;Last but not least, it is worthwhile to mention sparklyr 1.3.0 is known to be fully compatible with the recently released Spark 3.0. We highly recommend upgrading your copy of sparklyr to 1.3.0 if you plan to have Spark 3.0 as part of your data workflow in future.&lt;/p&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;In chronological order, we want to thank the following individuals for submitting pull requests towards sparklyr 1.3:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jozefhajnala"&gt;Jozef Hajnala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/falaki"&gt;Hossein Falaki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/samuelmacedo83"&gt;Samuel Macêdo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yl790"&gt;Yitao Li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Loquats"&gt;Andy Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/javierluraschi"&gt;Javier Luraschi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nealrichardson"&gt;Neal Richardson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are also grateful for valuable input on the sparklyr 1.3 roadmap, &lt;a href="https://github.com/sparklyr/sparklyr/pull/2434"&gt;#2434&lt;/a&gt;, and &lt;a href="https://github.com/sparklyr/sparklyr/pull/2551"&gt;#2551&lt;/a&gt; from &lt;span class="citation"&gt;[@javierluraschi]&lt;/span&gt;(&lt;a href="https://github.com/javierluraschi" class="uri"&gt;https://github.com/javierluraschi&lt;/a&gt;), and great spiritual advice on &lt;a href="https://github.com/sparklyr/sparklyr/issues/1773"&gt;#1773&lt;/a&gt; and &lt;a href="https://github.com/sparklyr/sparklyr/issues/2514"&gt;#2514&lt;/a&gt; from &lt;a href="https://github.com/mattpollock"&gt;@mattpollock&lt;/a&gt; and &lt;a href="https://github.com/benmwhite"&gt;@benmwhite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note if you believe you are missing from the acknowledgement above, it may be because your contribution has been considered part of the next sparklyr release rather than part of the current release. We do make every effort to ensure all contributors are mentioned in this section. In case you believe there is a mistake, please feel free to contact the author of this blog post via e-mail (yitao at rstudio dot com) and request a correction.&lt;/p&gt;
&lt;p&gt;If you wish to learn more about &lt;code&gt;sparklyr&lt;/code&gt;, we recommend visiting &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, and some of the previous release posts such as &lt;a href="https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/"&gt;sparklyr 1.2&lt;/a&gt; and &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/"&gt;sparklyr 1.1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">717262dc0660d693753eb9688f417a6e</distill:md5>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</guid>
      <pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>


&lt;p&gt;Behold the glory that is &lt;a href="https://sparklyr.ai"&gt;sparklyr&lt;/a&gt; 1.2! In this release, the following new hotnesses have emerged into spotlight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;registerDoSpark&lt;/code&gt; method to create a &lt;a href="#foreach"&gt;foreach&lt;/a&gt; parallel backend powered by Spark that enables hundreds of existing R packages to run in Spark.&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="#databricks-connect"&gt;Databricks Connect&lt;/a&gt;, allowing &lt;code&gt;sparklyr&lt;/code&gt; to connect to remote Databricks clusters.&lt;/li&gt;
&lt;li&gt;Improved support for Spark &lt;a href="#structures"&gt;structures&lt;/a&gt; when collecting and querying their nested attributes with &lt;code&gt;dplyr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of inter-op issues observed with &lt;code&gt;sparklyr&lt;/code&gt; and Spark 3.0 preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, &lt;code&gt;sparklyr&lt;/code&gt; will be fully ready to work with it. Most notably, key features such as &lt;code&gt;spark_submit&lt;/code&gt;, &lt;code&gt;sdf_bind_rows&lt;/code&gt;, and standalone connections are now finally working with Spark 3.0 preview.&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;sparklyr&lt;/code&gt; 1.2 from CRAN run,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full list of changes are available in the sparklyr &lt;a href="https://github.com/sparklyr/sparklyr/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;h2 id="foreach"&gt;Foreach&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package provides the &lt;code&gt;%dopar%&lt;/code&gt; operator to iterate over elements in a collection in parallel. Using &lt;code&gt;sparklyr&lt;/code&gt; 1.2, you can now register Spark as a backend using &lt;code&gt;registerDoSpark()&lt;/code&gt; and then easily iterate over R objects using Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
library(foreach)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4&amp;quot;)

registerDoSpark(sc)
foreach(i = 1:3, .combine = &amp;#39;c&amp;#39;) %dopar% {
  sqrt(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.000000 1.414214 1.732051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since many R packages are based on &lt;code&gt;foreach&lt;/code&gt; to perform parallel computation, we can now make use of all those great packages in Spark as well!&lt;/p&gt;
&lt;p&gt;For instance, we can use &lt;a href="https://tidymodels.github.io/parsnip/"&gt;parsnip&lt;/a&gt; and the &lt;a href="https://tidymodels.github.io/tune/"&gt;tune&lt;/a&gt; package with data from &lt;a href="https://CRAN.R-project.org/package=mlbench"&gt;mlbench&lt;/a&gt; to perform hyperparameter tuning in Spark with ease:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tune)
library(parsnip)
library(mlbench)

data(Ionosphere)
svm_rbf(cost = tune(), rbf_sigma = tune()) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;kernlab&amp;quot;) %&amp;gt;%
  tune_grid(Class ~ .,
    resamples = rsample::bootstraps(dplyr::select(Ionosphere, -V2), times = 30),
    control = control_grid(verbose = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Bootstrap sampling
# A tibble: 30 x 4
   splits            id          .metrics          .notes
 * &amp;lt;list&amp;gt;            &amp;lt;chr&amp;gt;       &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;
 1 &amp;lt;split [351/124]&amp;gt; Bootstrap01 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 2 &amp;lt;split [351/126]&amp;gt; Bootstrap02 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 3 &amp;lt;split [351/125]&amp;gt; Bootstrap03 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 4 &amp;lt;split [351/135]&amp;gt; Bootstrap04 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 5 &amp;lt;split [351/127]&amp;gt; Bootstrap05 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 6 &amp;lt;split [351/131]&amp;gt; Bootstrap06 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 7 &amp;lt;split [351/141]&amp;gt; Bootstrap07 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 8 &amp;lt;split [351/123]&amp;gt; Bootstrap08 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 9 &amp;lt;split [351/118]&amp;gt; Bootstrap09 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
10 &amp;lt;split [351/136]&amp;gt; Bootstrap10 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
# … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Spark connection was already registered, so the code ran in Spark without any additional changes. We can verify this was the case by navigating to the Spark web interface:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-backend-foreach-package.png" /&gt;&lt;/p&gt;
&lt;h2 id="databricks-connect"&gt;Databricks Connect&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.databricks.com/dev-tools/databricks-connect.html"&gt;Databricks Connect&lt;/a&gt; allows you to connect your favorite IDE (like &lt;a href="https://rstudio.com/products/rstudio/download/"&gt;RStudio&lt;/a&gt;!) to a Spark &lt;a href="https://databricks.com/"&gt;Databricks&lt;/a&gt; cluster.&lt;/p&gt;
&lt;p&gt;You will first have to install the &lt;code&gt;databricks-connect&lt;/code&gt; package as described in our &lt;a href="https://github.com/sparklyr/sparklyr#connecting-through-databricks-connect"&gt;README&lt;/a&gt; and start a Databricks cluster, but once that’s ready, connecting to the remote cluster is as easy as running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sc &amp;lt;- spark_connect(
  method = &amp;quot;databricks&amp;quot;,
  spark_home = system2(&amp;quot;databricks-connect&amp;quot;, &amp;quot;get-spark-home&amp;quot;, stdout = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-databricks-connect-rstudio.png" /&gt;&lt;/p&gt;
&lt;p&gt;That’s about it, you are now remotely connected to a Databricks cluster from your local R session.&lt;/p&gt;
&lt;h2 id="structures"&gt;Structures&lt;/h2&gt;
&lt;p&gt;If you previously used &lt;code&gt;collect&lt;/code&gt; to deserialize structurally complex Spark dataframes into their equivalents in R, you likely have noticed Spark SQL struct columns were only mapped into JSON strings in R, which was non-ideal. You might also have run into a much dreaded &lt;code&gt;java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt; error when using &lt;code&gt;dplyr&lt;/code&gt; to query nested attributes from any struct column of a Spark dataframe in sparklyr.&lt;/p&gt;
&lt;p&gt;Unfortunately, often times in real-world Spark use cases, data describing entities comprising of sub-entities (e.g., a product catalog of all hardware components of some computers) needs to be denormalized / shaped in an object-oriented manner in the form of Spark SQL structs to allow efficient read queries. When sparklyr had the limitations mentioned above, users often had to invent their own workarounds when querying Spark struct columns, which explained why there was a mass popular demand for sparklyr to have better support for such use cases.&lt;/p&gt;
&lt;p&gt;The good news is with &lt;code&gt;sparklyr&lt;/code&gt; 1.2, those limitations no longer exist any more when working running with Spark 2.4 or above.&lt;/p&gt;
&lt;p&gt;As a concrete example, consider the following catalog of computers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

computers &amp;lt;- tibble::tibble(
  id = seq(1, 2),
  attributes = list(
    list(
      processor = list(freq = 2.4, num_cores = 256),
      price = 100
   ),
   list(
     processor = list(freq = 1.6, num_cores = 512),
     price = 133
   )
  )
)

computers &amp;lt;- copy_to(sc, computers, overwrite = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A typical &lt;code&gt;dplyr&lt;/code&gt; use case involving &lt;code&gt;computers&lt;/code&gt; would be the following:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;high_freq_computers &amp;lt;- computers %&amp;gt;%
                       filter(attributes.processor.freq &amp;gt;= 2) %&amp;gt;%
                       collect()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As previously mentioned, before &lt;code&gt;sparklyr&lt;/code&gt; 1.2, such query would fail with &lt;code&gt;Error: java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Whereas with &lt;code&gt;sparklyr&lt;/code&gt; 1.2, the expected result is returned in the following form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 2
     id attributes
  &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;
1     1 &amp;lt;named list [2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;high_freq_computers$attributes&lt;/code&gt; is what we would expect:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[[1]]$price
[1] 100

[[1]]$processor
[[1]]$processor$freq
[1] 2.4

[[1]]$processor$num_cores
[1] 256&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="and-more"&gt;And More!&lt;/h2&gt;
&lt;p&gt;Last but not least, we heard about a number of pain points &lt;code&gt;sparklyr&lt;/code&gt; users have run into, and have addressed many of them in this release as well. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date type in R is now correctly serialized into Spark SQL date type by &lt;code&gt;copy_to&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;spark dataframe&amp;gt; %&amp;gt;% print(n = 20)&lt;/code&gt; now actually prints 20 rows as expected instead of 10&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spark_connect(master = "local")&lt;/code&gt; will emit a more informative error message if it’s failing because the loopback interface is not up&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;… to just name a few. We want to thank the open source community for their continuous feedback on &lt;code&gt;sparklyr&lt;/code&gt;, and are looking forward to incorporating more of that feedback to make &lt;code&gt;sparklyr&lt;/code&gt; even better in the future.&lt;/p&gt;
&lt;p&gt;Finally, in chronological order, we wish to thank the following individuals for contributing to &lt;code&gt;sparklyr&lt;/code&gt; 1.2: &lt;a href="https://github.com/zero323"&gt;zero323&lt;/a&gt;, &lt;a href="https://github.com/Loquats"&gt;Andy Zhang&lt;/a&gt;, &lt;a href="https://github.com/yl790"&gt;Yitao Li&lt;/a&gt;, &lt;a href="https://github.com/javierluraschi"&gt;Javier Luraschi&lt;/a&gt;, &lt;a href="https://github.com/falaki"&gt;Hossein Falaki&lt;/a&gt;, &lt;a href="https://github.com/lu-wang-dl"&gt;Lu Wang&lt;/a&gt;, &lt;a href="https://github.com/samuelmacedo83"&gt;Samuel Macedo&lt;/a&gt; and &lt;a href="https://github.com/jozefhajnala"&gt;Jozef Hajnala&lt;/a&gt;. Great job everyone!&lt;/p&gt;
&lt;p&gt;If you need to catch up on &lt;code&gt;sparklyr&lt;/code&gt;, please visit &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, or some of the previous release posts: &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/"&gt;sparklyr 1.1&lt;/a&gt; and &lt;a href="https://blog.rstudio.com/2019/03/15/sparklyr-1-0/"&gt;sparklyr 1.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this post.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">10308072097bf27ae84c0456eefaa91d</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>


&lt;p&gt;A new version of &lt;code&gt;pins&lt;/code&gt; is available on CRAN today, which adds support for &lt;a href="http://pins.rstudio.com/articles/advanced-versions.html"&gt;versioning&lt;/a&gt; your datasets and &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean Spaces&lt;/a&gt; boards!&lt;/p&gt;
&lt;p&gt;As a quick recap, the pins package allows you to cache, discover and share resources. You can use &lt;code&gt;pins&lt;/code&gt; in a wide range of situations, from downloading a dataset from a URL to creating complex automation workflows (learn more at &lt;a href="https://pins.rstudio.com"&gt;pins.rstudio.com&lt;/a&gt;). You can also use &lt;code&gt;pins&lt;/code&gt; in combination with TensorFlow and Keras; for instance, use &lt;a href="https://tensorflow.rstudio.com/tools/cloudml"&gt;cloudml&lt;/a&gt; to train models in cloud GPUs, but rather than manually copying files into the GPU instance, you can store them as pins directly from R.&lt;/p&gt;
&lt;p&gt;To install this new version of &lt;code&gt;pins&lt;/code&gt; from CRAN, simply run:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;pins&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find a detailed list of improvements in the pins &lt;a href="https://github.com/rstudio/pins/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;h1 id="versioning"&gt;Versioning&lt;/h1&gt;
&lt;p&gt;To illustrate the new versioning functionality, let’s start by downloading and caching a remote dataset with pins. For this example, we will download the weather in London, this happens to be in JSON format and requires &lt;code&gt;jsonlite&lt;/code&gt; to be parsed:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)

weather_url &amp;lt;- &amp;quot;https://samples.openweathermap.org/data/2.5/weather?q=London,uk&amp;amp;appid=b6907d289e10d714a6e88b30761fae22&amp;quot;

pin(weather_url, &amp;quot;weather&amp;quot;) %&amp;gt;%
  jsonlite::read_json() %&amp;gt;%
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  coord.lon coord.lat weather.id weather.main     weather.description weather.icon
1     -0.13     51.51        300      Drizzle light intensity drizzle          09d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One advantage of using &lt;code&gt;pins&lt;/code&gt; is that, even if the URL or your internet connection becomes unavailable, the above code will still work.&lt;/p&gt;
&lt;p&gt;But back to &lt;code&gt;pins 0.4&lt;/code&gt;! The new &lt;code&gt;signature&lt;/code&gt; parameter in &lt;code&gt;pin_info()&lt;/code&gt; allows you to retrieve the “version” of this dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_info(&amp;quot;weather&amp;quot;, signature = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: local&amp;lt;weather&amp;gt; [files]
# Signature: 624cca260666c6f090b93c37fd76878e3a12a79b
# Properties:
#   - path: weather&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then validate the remote dataset has not changed by specifying its signature:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin(weather_url, &amp;quot;weather&amp;quot;, signature = &amp;quot;624cca260666c6f090b93c37fd76878e3a12a79b&amp;quot;) %&amp;gt;%
  jsonlite::read_json()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the remote dataset changes, &lt;code&gt;pin()&lt;/code&gt; will fail and you can take the appropriate steps to accept the changes by updating the signature or properly updating your code. The previous example is useful as a way of detecting version changes, but we might also want to retrieve specific versions even when the dataset changes.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pins 0.4&lt;/code&gt; allows you to display and retrieve versions from services like GitHub, Kaggle and RStudio Connect. Even in boards that don’t support versioning natively, you can opt-in by registering a board with &lt;code&gt;versions = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To keep this simple, let’s focus on GitHub first. We will register a GitHub board and pin a dataset to it. Notice that you can also specify the &lt;code&gt;commit&lt;/code&gt; parameter in GitHub boards as the commit message for this change.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register_github(repo = &amp;quot;javierluraschi/datasets&amp;quot;, branch = &amp;quot;datasets&amp;quot;)

pin(iris, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;, commit = &amp;quot;use iris as the main dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now suppose that a colleague comes along and updates this dataset as well:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin(mtcars, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;, commit = &amp;quot;slight preference to mtcars&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From now on, your code could be broken or, even worse, produce incorrect results!&lt;/p&gt;
&lt;p&gt;However, since GitHub was designed as a version control system and &lt;code&gt;pins 0.4&lt;/code&gt; adds support for &lt;code&gt;pin_versions()&lt;/code&gt;, we can now explore particular versions of this dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_versions(&amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 4
  version created              author         message                     
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                       
1 6e6c320 2020-04-02T21:28:07Z javierluraschi slight preference to mtcars 
2 01f8ddf 2020-04-02T21:27:59Z javierluraschi use iris as the main dataset&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then retrieve the version you are interested in as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_get(&amp;quot;versioned&amp;quot;, version = &amp;quot;01f8ddf&amp;quot;, board = &amp;quot;github&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 150 x 5
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# … with 140 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can follow similar steps for &lt;a href="http://pins.rstudio.com/articles/boards-rsconnect.html"&gt;RStudio Connect&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-kaggle.html"&gt;Kaggle&lt;/a&gt; boards, even for existing pins! Other boards like &lt;a href="http://pins.rstudio.com/articles/boards-s3.html"&gt;Amazon S3&lt;/a&gt;, &lt;a href="http://pins.rstudio.com/articles/boards-gcloud.html"&gt;Google Cloud&lt;/a&gt;, &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;Digital Ocean&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-azure.html"&gt;Microsoft Azure&lt;/a&gt; require you explicitly enable versioning when registering your boards.&lt;/p&gt;
&lt;h1 id="digitalocean"&gt;DigitalOcean&lt;/h1&gt;
&lt;p&gt;To try out the new &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean Spaces board&lt;/a&gt;, first you will have to register this board and enable versioning by setting &lt;code&gt;versions&lt;/code&gt; to &lt;code&gt;TRUE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)
board_register_dospace(space = &amp;quot;pinstest&amp;quot;,
                       key = &amp;quot;AAAAAAAAAAAAAAAAAAAA&amp;quot;,
                       secret = &amp;quot;ABCABCABCABCABCABCABCABCABCABCABCABCABCA==&amp;quot;,
                       datacenter = &amp;quot;sfo2&amp;quot;,
                       versions = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use all the functionality pins provides, including versioning:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create pin and replace content in digitalocean
pin(iris, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)
pin(mtcars, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)

# retrieve versions from digitalocean
pin_versions(name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 1
  version
  &amp;lt;chr&amp;gt;  
1 c35da04
2 d9034cd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that enabling versions in cloud services requires additional storage space for each version of the dataset being stored:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-13-pins-04/images/digitalocean-spaces-pins-versioned.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;To learn more visit the &lt;a href="http://pins.rstudio.com/articles/advanced-versions.html"&gt;Versioning&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean&lt;/a&gt; articles. To catch up with previous releases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pins.rstudio.com/blog/posts/pins-0-3-0/"&gt;pins 0.3&lt;/a&gt;: Azure, GCloud and S3&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.rstudio.com/2019/09/09/pin-discover-and-share-resources/"&gt;pins 0.2&lt;/a&gt;: Pin, Discover and Share Resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading along!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">0237a491d92720994e751dc6fdfc1a55</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>tfhub: R interface to TensorFlow Hub</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</link>
      <description>


&lt;p&gt;We are pleased to announce that the first version of &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub&lt;/a&gt; is now on CRAN. tfhub is an R interface to TensorFlow Hub - a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.&lt;/p&gt;
&lt;p&gt;The CRAN version of tfhub can be installed with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;tfhub&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installing the R package you need to install the TensorFlow Hub python package. You can do it by running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tfhub::install_tfhub()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;The essential function of tfhub is &lt;code&gt;layer_hub&lt;/code&gt; which works just like a &lt;a href="https://github.com/rstudio/keras"&gt;keras&lt;/a&gt; layer but allows you to load a complete pre-trained deep learning model.&lt;/p&gt;
&lt;p&gt;For example you can:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfhub)
layer_mobilenet &amp;lt;- layer_hub(
  handle = &amp;quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will download the MobileNet model pre-trained on the ImageNet dataset. tfhub models are cached locally and don’t need to be downloaded the next time you use the same model.&lt;/p&gt;
&lt;p&gt;You can now use &lt;code&gt;layer_mobilenet&lt;/code&gt; as a usual Keras layer. For example you can define a model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
input &amp;lt;- layer_input(shape = c(224, 224, 3))
output &amp;lt;- layer_mobilenet(input)
model &amp;lt;- keras_model(input, output)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
____________________________________________________________________
Layer (type)                  Output Shape               Param #    
====================================================================
input_2 (InputLayer)          [(None, 224, 224, 3)]      0          
____________________________________________________________________
keras_layer_1 (KerasLayer)    (None, 1001)               3540265    
====================================================================
Total params: 3,540,265
Trainable params: 0
Non-trainable params: 3,540,265
____________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model can now be used to predict Imagenet labels for an image. For example, let’s see the results for the famous Grace Hopper’s photo:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-18-tfhub-0.7.0/images/grace-hopper.jpg" style="width:30.0%" alt="" /&gt;
&lt;p class="caption"&gt;Grace Hopper&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="r"&gt;&lt;code&gt;img &amp;lt;- image_load(&amp;quot;images/grace-hopper.jpg&amp;quot;, target_size = c(224,224)) %&amp;gt;% 
  image_to_array()
img &amp;lt;- img/255
dim(img) &amp;lt;- c(1, dim(img))
pred &amp;lt;- predict(model, img)
imagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  class_name class_description    score
1  n03763968  military_uniform 9.760404
2  n02817516          bearskin 5.922512
3  n04350905              suit 5.729345
4  n03787032       mortarboard 5.400651
5  n03929855       pickelhaube 5.008665&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TensorFlow Hub also offers many other pre-trained image, text and video models. All possible models can be found on the TensorFlow hub &lt;a href="https://tfhub.dev"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" alt="" /&gt;
&lt;p class="caption"&gt;TensorFlow Hub&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can find more examples of &lt;code&gt;layer_hub&lt;/code&gt; usage in the following articles on the TensorFlow for R website:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;Transfer Learning with tfhub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/hub-with-keras/"&gt;Using tfhub with Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/intro/"&gt;tfhub Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/text_classification/"&gt;Text classification example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="usage-with-recipes-and-the-feature-spec-api"&gt;Usage with Recipes and the Feature Spec API&lt;/h2&gt;
&lt;p&gt;tfhub also offers &lt;a href="https://github.com/tidymodels/recipes"&gt;recipes&lt;/a&gt; steps to make it easier to use pre-trained deep learning models in your machine learning workflow.&lt;/p&gt;
&lt;p&gt;For example, you can define a recipe that uses a pre-trained text embedding model with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rec &amp;lt;- recipe(obscene ~ comment_text, data = train) %&amp;gt;%
  step_pretrained_text_embedding(
    comment_text,
    handle = &amp;quot;https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1&amp;quot;
  ) %&amp;gt;%
  step_bin2factor(obscene)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see a complete running example &lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/recipes/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can also use tfhub with the new &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;Feature Spec API&lt;/a&gt; implemented in tfdatasets. You can see a complete example &lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/feature_column/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We hope our readers have fun experimenting with Hub models and/or can put them to good use. If you run into any problems, let us know by creating an issue in the tfhub repository&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4d8a05b040d0af75c6540549d2fc9dc5</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</guid>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" medium="image" type="image/png" width="1365" height="909"/>
    </item>
    <item>
      <title>Getting started with Keras from R - the 2020 edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</link>
      <description>


&lt;p&gt;If you’ve been thinking about diving into deep learning for a while – using R, preferentially –, now is a good time. For TensorFlow / Keras, one of the predominant deep learning frameworks on the market, last year was a year of substantial changes; for users, this sometimes would mean ambiguity and confusion about the “right” (or: recommended) way to do things. By now, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/"&gt;TensorFlow 2.0&lt;/a&gt; has been the current stable release for about two months; the mists have cleared away, and patterns have emerged, enabling leaner, more modular code that accomplishes a lot in just a few lines.&lt;/p&gt;
&lt;p&gt;To give the new features the space they deserve, and assemble central contributions from related packages all in one place, we have significantly remodeled the &lt;a href="https://tensorflow.rstudio.com/"&gt;TensorFlow for R website&lt;/a&gt;. So this post really has two objectives.&lt;/p&gt;
&lt;p&gt;First, it would like to do exactly what is suggested by the title: Point new users to resources that make for an effective start into the subject.&lt;/p&gt;
&lt;p&gt;Second, it could be read as a “best of new website content”. Thus, as an existing user, you might still be interested in giving it a quick skim, checking for pointers to new features that appear in familiar contexts. To make this easier, we’ll add side notes to highlight new features.&lt;/p&gt;
&lt;p&gt;Overall, the structure of what follows is this. We start from the core question: &lt;em&gt;How do you build a model?&lt;/em&gt;, then frame it from both sides; i.e.: &lt;em&gt;What comes before?&lt;/em&gt; (data loading / preprocessing) and &lt;em&gt;What comes after?&lt;/em&gt; (model saving / deployment).&lt;/p&gt;
&lt;p&gt;After that, we quickly go into creating models for different types of data: images, text, tabular.&lt;/p&gt;
&lt;p&gt;Then, we touch on where to find background information, such as: How do I add a custom callback? How do I create a custom layer? How can I define my own training loop?&lt;/p&gt;
&lt;p&gt;Finally, we round up with something that looks like a tiny technical addition but has far greater impact: integrating modules from TensorFlow (TF) Hub.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;h3 id="how-to-build-a-model"&gt;How to build a model?&lt;/h3&gt;
&lt;p&gt;If linear regression is the Hello World of machine learning, non-linear regression has to be the Hello World of neural networks. The &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/"&gt;Basic Regression tutorial&lt;/a&gt; shows how to train a dense network on the Boston Housing dataset. This example uses the Keras &lt;a href="https://tensorflow.rstudio.com/guide/keras/functional_api/"&gt;Functional API&lt;/a&gt;, one of the two “classical” model-building approaches – the one that tends to be used when some sort of flexibility is required. In this case, the desire for flexibility comes from the use of &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_columns/"&gt;feature columns&lt;/a&gt; - a nice new addition to TensorFlow that allows for convenient integration of e.g. feature normalization (more about this in the next section).&lt;/p&gt;
&lt;aside&gt;
The &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/"&gt;regression tutorial&lt;/a&gt; now uses feature columns for convenient data preprocessing.
&lt;/aside&gt;
&lt;p&gt;This introduction to regression is complemented by a &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_classification/"&gt;tutorial on multi-class classification&lt;/a&gt; using “Fashion MNIST”. It is equally suited for a first encounter with Keras.&lt;/p&gt;
&lt;p&gt;A third tutorial in this section is dedicated to &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/"&gt;text classification&lt;/a&gt;. Here too, there is a hidden gem in the current version that makes text preprocessing a lot easier: &lt;code&gt;layer_text_vectorization&lt;/code&gt;, one of the brand new &lt;a href="https://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md"&gt;Keras preprocessing layers&lt;/a&gt;.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; If you’ve used Keras for NLP before: No more messing with &lt;code&gt;text_tokenizer&lt;/code&gt;!&lt;/p&gt;
&lt;aside&gt;
Check out the new text vectorization layer in the &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/"&gt;text classification tutorial&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;These tutorials are nice introductions explaining code as well as concepts. What if you’re familiar with the basic procedure and just need a quick reminder (or: something to quickly copy-paste from)? The ideal document to consult for those purposes is the &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/"&gt;Overview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now – knowledge how to build models is fine, but as in data science overall, there is no modeling without data.&lt;/p&gt;
&lt;h3 id="data-ingestion-and-preprocessing"&gt;Data ingestion and preprocessing&lt;/h3&gt;
&lt;p&gt;Two detailed, end-to-end tutorials show how to load &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/load/load_csv/"&gt;csv data&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/load/load_image/"&gt;images&lt;/a&gt;, respectively.&lt;/p&gt;
&lt;p&gt;In current Keras, two mechanisms are central to data preparation. One is the use of &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/"&gt;tfdatasets pipelines&lt;/a&gt;. &lt;code&gt;tfdatasets&lt;/code&gt; lets you load data in a streaming fashion (batch-by-batch), optionally applying transformations as you go. The other handy device here is &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;feature specs&lt;/a&gt; and&lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_columns/"&gt;feature columns&lt;/a&gt;. Together with a matching Keras layer, these allow for transforming the input data without having to think about what the new format will mean to Keras.&lt;/p&gt;
&lt;p&gt;While there are other types of data not discussed in the docs, the principles – pre-processing pipelines and feature extraction – generalize.&lt;/p&gt;
&lt;h3 id="model-saving"&gt;Model saving&lt;/h3&gt;
&lt;p&gt;The best-performing model is of little use if ephemeral. Straightforward ways of saving Keras models are explained in a dedicated &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_save_and_restore/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
Advanced users: Additional options exist; see the &lt;a href="https://tensorflow.rstudio.com/guide/saving/checkpoints/"&gt;tutorial on checkpoints&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;And unless one’s just tinkering around, the question will often be: How can I deploy my model? There is a complete new section on &lt;a href="https://tensorflow.rstudio.com/deploy/"&gt;deployment&lt;/a&gt;, featuring options like &lt;code&gt;plumber&lt;/code&gt;, Shiny, TensorFlow Serving and RStudio Connect.&lt;/p&gt;
&lt;aside&gt;
Check out the new section on &lt;a href="https://tensorflow.rstudio.com/deploy/"&gt;deployment options&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;After this workflow-oriented run-through, let’s see about different types of data you might want to model.&lt;/p&gt;
&lt;h2 id="neural-networks-for-different-kinds-of-data"&gt;Neural networks for different kinds of data&lt;/h2&gt;
&lt;p&gt;No introduction to deep learning is complete without image classification. The “Fashion MNIST” classification tutorial mentioned in the beginning is a good introduction, but it uses a fully connected neural network to make it easy to remain focused on the overall approach. Standard models for image recognition, however, are commonly based on a convolutional architecture. &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/"&gt;Here&lt;/a&gt; is a nice introductory tutorial.&lt;/p&gt;
&lt;p&gt;For text data, the concept of &lt;em&gt;embeddings&lt;/em&gt; – distributed representations endowed with a measure of similarity – is central. As in the aforementioned text classification tutorial, embeddings can be learned using the respective Keras layer (&lt;code&gt;layer_embedding&lt;/code&gt;); in fact, the more idiosyncratic the dataset, the more recommendable this approach. Often though, it makes a lot of sense to use &lt;em&gt;pre-trained embeddings&lt;/em&gt;, obtained from large language models trained on enormous amounts of data. With TensorFlow Hub, discussed in more detail in the last section, pre-trained embeddings can be made use of simply by integrating an adequate &lt;em&gt;hub layer&lt;/em&gt;, as shown in &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;one of the Hub tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
Models from TF Hub can now conveniently be integrated into a model as Keras layers.
&lt;/aside&gt;
&lt;p&gt;As opposed to images and text, “normal”, a.k.a. &lt;em&gt;tabular&lt;/em&gt;, a.k.a. &lt;em&gt;structured&lt;/em&gt; data often seems like less of a candidate for deep learning. Historically, the mix of data types – numeric, binary, categorical –, together with different handling in the network (“leave alone” or embed) used to require a fair amount of manual fiddling. In contrast, the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/structured/classify/"&gt;Structured data tutorial&lt;/a&gt; shows the, quote-unquote, modern way, again using feature columns and feature specs. The consequence: If you’re not sure that in the area of tabular data, deep learning will lead to improved performance – if it’s as easy as that, why not give it a try?&lt;/p&gt;
&lt;aside&gt;
If you’re working with structured data, definitely check out the &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;feature spec way to do it&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;Before rounding up with a special on TensorFlow Hub, let’s quickly see where to get more information on immediate and background-level technical questions.&lt;/p&gt;
&lt;h2 id="guides-topic-related-and-background-information"&gt;Guides: topic-related and background information&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/guide/"&gt;Guide section&lt;/a&gt; has lots of additional information, covering specific questions that will come up when coding Keras models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can I define a &lt;a href="https://tensorflow.rstudio.com/guide/keras/custom_layers/"&gt;custom layer&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;A &lt;a href="https://tensorflow.rstudio.com/guide/keras/custom_models/"&gt;custom model&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;What are &lt;a href="https://tensorflow.rstudio.com/guide/keras/training_callbacks/"&gt;training callbacks&lt;/a&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;as well as background knowledge and terminology: What are &lt;a href="https://tensorflow.rstudio.com/guide/tensorflow/tensors/"&gt;tensors&lt;/a&gt;, &lt;a href="https://tensorflow.rstudio.com/guide/tensorflow/variables/"&gt;&lt;code&gt;Variables&lt;/code&gt;&lt;/a&gt;, how does &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/customization/autodiff/"&gt;automatic differentiation&lt;/a&gt; work in TensorFlow?&lt;/p&gt;
&lt;p&gt;Like for the basics, above we pointed out a document called “Quickstart”, for advanced topics here too is a &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;Quickstart&lt;/a&gt; that in one end-to-end example, shows how to define and train a custom model. One especially nice aspect is the use of &lt;a href="https://github.com/t-kalinowski/tfautograph"&gt;tfautograph&lt;/a&gt;, a package developed by T. Kalinowski that – among others – allows for concisely iterating over a dataset in a &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;
&lt;aside&gt;
Power users: Check out the custom training &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;Quickstart&lt;/a&gt; featuring custom models, &lt;code&gt;GradientTape&lt;/code&gt;s and &lt;code&gt;tfautograph&lt;/code&gt;.
&lt;/aside&gt;
&lt;p&gt;Finally, let’s talk about TF Hub.&lt;/p&gt;
&lt;h2 id="a-special-highlight-hub-layers"&gt;A special highlight: Hub layers&lt;/h2&gt;
&lt;p&gt;One of the most interesting aspects of contemporary neural network architectures is the use of transfer learning. Not everyone has the data, or computing facilities, to train big networks on big data from scratch. Through transfer learning, existing pre-trained models can be used for similar (but not identical) applications and in similar (but not identical) domains.&lt;/p&gt;
&lt;p&gt;Depending on one’s requirements, building on an existing model could be more or less cumbersome. Some time ago, TensorFlow Hub was created as a mechanism to publicly share models, or &lt;em&gt;modules&lt;/em&gt;, that is, reusable building blocks that could be made use of by others. Until recently, there was no convenient way to incorporate these modules, though.&lt;/p&gt;
&lt;p&gt;Starting from TensorFlow 2.0, Hub modules can now seemlessly be integrated in Keras models, using &lt;code&gt;layer_hub&lt;/code&gt;. This is demonstrated in two tutorials, for &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;text&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/"&gt;images&lt;/a&gt;, respectively. But really, these two documents are just starting points: Starting points into a journey of experimentation, with other modules, combination of modules, areas of applications…&lt;/p&gt;
&lt;aside&gt;
Don’t miss out on the new TensorFlow Hub layer available in Keras… potentially, an extremely powerful way to enhance your models.
&lt;/aside&gt;
&lt;p&gt;In sum, we hope you have fun with the “new” (TF 2.0) Keras and find the documentation useful. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;In fact, it is so new that as of this writing, you will have to install the nightly build of TensorFlow – as well as &lt;code&gt;tensorflow&lt;/code&gt; from github – to use it.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">bd27a7916bb64b881fb8a8813910618d</distill:md5>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</guid>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020/images/website.png" medium="image" type="image/png" width="1591" height="725"/>
    </item>
    <item>
      <title>tfprobability 0.8 on CRAN: Now how can you use it?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</link>
      <description>


&lt;p&gt;About a week ago, &lt;code&gt;tfprobability&lt;/code&gt; 0.8 was accepted on CRAN. While we’ve been using this package quite frequently already on this blog, in this post we’d like to (re-)introduce it on a high level, especially addressing new users.&lt;/p&gt;
&lt;h2 id="tfprobability-what-is-it"&gt;tfprobability, what is it?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tfprobability&lt;/code&gt; is an R wrapper for &lt;a href="https://www.tensorflow.org/probability/"&gt;TensorFlow Probability&lt;/a&gt;, a Python library built on top of the &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework. So now the question is, what is TensorFlow Probability?&lt;/p&gt;
&lt;p&gt;If – let’s call it “probabilistic programming” – is not something you do every day, an enumeration of features, or even a hierarchical listing of modules as given on the &lt;a href="https://www.tensorflow.org/probability/overview"&gt;TensorFlow Probability website&lt;/a&gt; might leave you a bit helpless, informative though it may be.&lt;/p&gt;
&lt;p&gt;Let’s start from use cases instead. We’ll look at three high-level example scenarios, before rounding up with a quick tour of more basic building blocks provided by TFP. (Short note aside: We’ll use &lt;em&gt;TFP&lt;/em&gt; as an acronym for the Python library as well as the R package, unless we’re referring to the R wrapper specifically, in which case we’ll say &lt;code&gt;tfprobability&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id="use-case-1-extending-deep-learning"&gt;Use case 1: Extending deep learning&lt;/h2&gt;
&lt;p&gt;We start with the type of use case that might be the most interesting to the majority of our readers: extending deep learning.&lt;/p&gt;
&lt;h3 id="distribution-layers"&gt;Distribution layers&lt;/h3&gt;
&lt;p&gt;In deep learning, usually output layers are deterministic. True, in classification we are used to talking about “class probabilities”. Take the multi-class case: We may attribute to the network the conclusion, “with 80% probability this is a Bernese mountain dog” – but we can only do this because the last layer’s output has been squished, by a softmax activation, to values between &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;. Nonetheless, the actual output is a tensor (a number).&lt;/p&gt;
&lt;p&gt;TFP, however, augments TensorFlow by means of &lt;em&gt;distribution layers&lt;/em&gt;: a hybrid species that can be used just like a normal TensorFlow/Keras layer but that, internally, contains the defining characteristics of some probability distribution.&lt;/p&gt;
&lt;p&gt;Concretely, for multi-class classification, we could use a categorical layer (&lt;a href="https://rstudio.github.io/tfprobability/reference/layer_one_hot_categorical.html"&gt;layer_one_hot_categorical&lt;/a&gt;), replacing something like&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_dense(
  num_classes, 
  activation = &amp;quot;softmax&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;by&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_one_hot_categorical(event_size = num_classes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model, thus modified, now outputs a distribution, not a tensor. However, it can still be trained passing “normal” target tensors. This is what was meant by &lt;em&gt;use like a normal layer&lt;/em&gt;, above: TFP will take the distribution, obtain a tensor from it &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, and compare that to the target. The other side of the layer’s personality is seen when generating predictions: Calling the model on fresh data will result in a bunch of distributions, one for every data point. You then call &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_mean.html"&gt;tfd_mean&lt;/a&gt; to elicit actual predictions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_dists &amp;lt;- model(x_test)
pred_means &amp;lt;- pred_dists %&amp;gt;% tfd_mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may be wondering, what good is this? One way or the other, we’ll decide on picking the class with the highest probability, right?&lt;/p&gt;
&lt;p&gt;Right, but there are a number of interesting things you can do with these layers. We’ll quickly introduce three well-known ones here, but wouldn’t be surprised if we saw a lot more emerging in the near future.&lt;/p&gt;
&lt;h3 id="variational-autoencoders-the-elegant-way"&gt;Variational autoencoders, the elegant way&lt;/h3&gt;
&lt;p&gt;Variational autoencoders are a prime example of something that got way easier to code when TF-2 style custom models and custom training appeared (TF-2 &lt;em&gt;style&lt;/em&gt;, not TF-2, as these techniques actually became available more than a year before TF 2 was finally released). &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evolutionarily, the next step was to use TFP distributions &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, but back in the time some fiddling was required, as distributions could not yet alias as layers.&lt;/p&gt;
&lt;p&gt;Due to those hybrids though, we now can do something like this &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# encoder
encoder_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...] %&amp;gt;%
  layer_multivariate_normal_tri_l(event_size = encoded_size) %&amp;gt;%
  layer_kl_divergence_add_loss([...])

# decoder
decoder_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...] %&amp;gt;%
 layer_independent_bernoulli([...])

# complete VAE
vae_model &amp;lt;- keras_model(inputs = encoder_model$inputs,
                         outputs = decoder_model(encoder_model$outputs[1]))

# loss function
vae_loss &amp;lt;- function (x, rv_x) - (rv_x %&amp;gt;% tfd_log_prob(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both the encoder and the decoder are “just” sequential models, joined through the functional API(&lt;code&gt;keras_model&lt;/code&gt;). The loss is just the negative log-probability of the data given the model. So where is the other part of the (negative) ELBO, the KL divergence? It is implicit in the encoder’s output layer, &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_kl_divergence_add_loss.html"&gt;layer_kl_divergence_add_loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our two other examples involve quantifying uncertainty.&lt;/p&gt;
&lt;h3 id="learning-the-spread-in-the-data"&gt;Learning the spread in the data&lt;/h3&gt;
&lt;p&gt;If a model’s last layer wraps a distribution parameterized by location and scale, like the normal distribution, we can train the network to learn not just the mean, but also the spread in the data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(# use unit 1 of previous layer
               loc = x[, 1, drop = FALSE],
               # use unit 2 of previous layer
               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])
               )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In essence, this means the network’s predictions will reflect any existing heteroscedasticity in the data. Here is an example: Given simulated training data of shape&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/simdata.png" alt="Simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)" width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;the network’s predictions show the same spread:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/g_aleatoric_relu_8.png" alt="Aleatoric uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/) " width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Aleatoric uncertainty on simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Please see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/"&gt;Adding uncertainty estimates to Keras models with tfprobability&lt;/a&gt; for a detailed explanation.&lt;/p&gt;
&lt;p&gt;Using a normal distribution layer as the output, we can capture irreducible variability in the data, also known as &lt;em&gt;aleatoric uncertainty&lt;/em&gt;. A different type of probabilistic layer allows to model what is called &lt;em&gt;epistemic uncertainty&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="putting-distributions-over-network-weights"&gt;Putting distributions over network weights&lt;/h3&gt;
&lt;p&gt;Using &lt;em&gt;variational layers&lt;/em&gt;, we can make neural networks probabilistic. A simple example could look like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 1,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x, scale = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines a network with a single dense layer, containing a single neuron, that has a prior distribution put over its weights. The network will be trained to minimize the KL divergence between that prior and an approximate posterior weight distribution, as well as maximize the probability of the data under the posterior weights. (For details, please again see the aforementioned post.)&lt;/p&gt;
&lt;p&gt;As a consequence of this setup, each test run will now yield different predictions. For the above simulated data, we might get an ensemble of predictions, like so:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/g_epistemic_linear_kl150.png" alt="Epistemic uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/) " width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-9)Epistemic uncertainty on simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Variational layers for non-dense layers exist, and we’ll see an example next week. Now let’s move on to the next type of use cases, from big data to small data, in a way.&lt;/p&gt;
&lt;h2 id="use-case-2-fitting-bayesian-models-with-monte-carlo-methods"&gt;Use case 2: Fitting Bayesian models with Monte Carlo methods&lt;/h2&gt;
&lt;p&gt;In sciences where data aren’t abound, Markov Chain Monte Carlo (MCMC) methods are common. We’ve shown some examples how to this with TFP (&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability&lt;/a&gt;, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/"&gt;Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability&lt;/a&gt;, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/"&gt;Modeling censored data with tfprobability&lt;/a&gt;, best read in this order), as well as tried to explain, in an accessible way, some of the background (&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc/"&gt;On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;MCMC software may roughly be divided into two flavors: “low-level” and “high-level”. Low-level software, like &lt;a href="https://mc-stan.org/"&gt;Stan&lt;/a&gt; – or TFP, for that matter – requires you to write code in either some programming language, or in a DSL that is pretty close in syntax and semantics to an existing programming language. High-level tools, on the other hand, are DSLs that resemble the way you’d express a model using mathematical notation. (Put differently, the former read like C or Python; the latter read like LaTeX.)&lt;/p&gt;
&lt;p&gt;In general, low-level software tends to offer more flexibility, while high-level interfaces may be more convenient to use and easier to learn. To start with MCMC in TFP, we recommend checking out the first of the posts listed above, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability&lt;/a&gt;. If you prefer a higher-level interface, you might be interested in &lt;a href="https://greta-stats.org/"&gt;greta&lt;/a&gt;, which is built on TFP.&lt;/p&gt;
&lt;p&gt;Our last use case is of a Bayesian nature as well.&lt;/p&gt;
&lt;h2 id="use-case-3-state-space-models"&gt;Use case 3: State space models&lt;/h2&gt;
&lt;p&gt;State space models are a perhaps lesser used, but highly conceptually attractive way of performing inference and prediction on signals evolving in time. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/"&gt;Dynamic linear models with tfprobability&lt;/a&gt; is an introduction to dynamic linear models with TFP, showcasing two of their (many) great strengths: ease of performing dynamic regression and additive (de)composition.&lt;/p&gt;
&lt;p&gt;In dynamic regression, coefficients are allowed to vary over time. Here is an example from the above-mentioned post showing, for both a single predictor and the regression intercept, the filtered (in the sense of Kálmán filtering) estimates over time:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/capm_filtered.png" alt="Filtering estimates from the Kálmán filter (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/)." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Filtering estimates from the Kálmán filter (from: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/&lt;/a&gt;).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And here is the ubiquitous &lt;em&gt;AirPassengers&lt;/em&gt; dataset, decomposed into a trend and a seasonal component:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/airp_components.png" alt="AirPassengers, decomposition into a linear trend and a seasonal component (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/)." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)AirPassengers, decomposition into a linear trend and a seasonal component (from: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/&lt;/a&gt;).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If this interests you, you might want to take a look at &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-structural-time-series-models"&gt;available state space models&lt;/a&gt;. Given how rapidly TFP is evolving, plus the model-inherent composability, we expect the number of options in this area to grow quite a bit.&lt;/p&gt;
&lt;p&gt;That is it for our tour of use cases. To wrap up, let’s talk about the basic building blocks of TFP.&lt;/p&gt;
&lt;h2 id="the-basics-distributions-and-bijectors"&gt;The basics: Distributions and bijectors&lt;/h2&gt;
&lt;p&gt;No probabilistic framework without probability distributions – that’s for sure. In release 0.8, TFP has &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-distributions"&gt;about 80 distributions&lt;/a&gt;. But what are bijectors?&lt;/p&gt;
&lt;p&gt;Bijectors are invertible, differentiable maps. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/"&gt;Getting into the flow: Bijectors in TensorFlow Probability&lt;/a&gt; introduces the main ideas and shows how to chain such bijective transformations into a &lt;em&gt;flow&lt;/em&gt;. Bijectors are being used by TFP internally all the time, and as a user too you’ll likely encounter situations where you need them.&lt;/p&gt;
&lt;p&gt;One example is doing MCMC (for example, Hamiltonian Monte Carlo) with TFP. In your model, you might have a prior on a standard deviation. Standard deviations are positive, so you’d like to specify, for example, an exponential distribution for it, resulting in exclusively positive values. But Hamiltonian Monte Carlo has to run in unconstrained space to work. This is where a bijector comes in, mapping between the two spaces used for model specification and sampling.&lt;/p&gt;
&lt;p&gt;For bijectors too, there are &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-bijectors"&gt;many of them&lt;/a&gt; - about 40, ranging from straightforward affine maps to more complex operations on Cholesky factors or the discrete cosine transform.&lt;/p&gt;
&lt;p&gt;To see both building blocks in action, let’s end with a “Hello World” of TFP, or two, rather. Here first is the direct way to obtain samples from a standard normal distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)

# create a normal distribution
d &amp;lt;- tfd_normal(loc = 0, scale = 1)
# sample from it
d %&amp;gt;% tfd_sample(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([-1.0863057  -0.61655647  1.8151687 ], shape=(3,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you thought that was too easy, here’s how to do the same using a bijector instead of a distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# generate 3 values uniformly distributed between 0 and 1
u &amp;lt;- runif(3)

# a bijector that in the inverse direction, maps values between 0 and 1
# to a normal distribution
b &amp;lt;- tfb_normal_cdf()

# call bijector&amp;#39;s inverse transform op
b %&amp;gt;% tfb_inverse(u) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([ 0.96157753  1.0103974  -1.4986734 ], shape=(3,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this we conclude our introduction. If you run into problems using &lt;em&gt;tfprobability&lt;/em&gt;, or have questions about it, please open an issue in the &lt;a href="https://github.com/rstudio/tfprobability"&gt;github repo&lt;/a&gt;. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;By default, just sampling from the distribution – but this is something the user can influence if desired, making use of the &lt;code&gt;convert_to_tensor_fn&lt;/code&gt; argument.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt; for an example.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Done so, for example, in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/"&gt;Getting started with TensorFlow Probability from R&lt;/a&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;For a complete running example, see the &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability README&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">832773038fec3abda000e4fa72b90c73</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</guid>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran/images/tfprobability.png" medium="image" type="image/png" width="518" height="600"/>
    </item>
    <item>
      <title>Innocent unicorns considered harmful? How to experiment with GPT-2 from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</link>
      <description>


&lt;p&gt;When this year in February, OpenAI presented &lt;a href="https://openai.com/blog/better-language-models/"&gt;GPT-2&lt;/a&gt;&lt;span class="citation"&gt;(Radford et al. 2019)&lt;/span&gt;, a large &lt;em&gt;Transformer&lt;/em&gt;-based language model trained on an enormous amount of web-scraped text, their announcement caught great attention, not just in the NLP community. This was primarily due to two facts. First, the samples of generated text were stunning.&lt;/p&gt;
&lt;p&gt;Presented with the following input&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a shocking finding, scientist [sic] discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;this was how the model continued:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. […]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Second, “due to our concerns about malicious applications” (quote) they didn’t release the full model, but a smaller one that has less than one tenth the number of parameters. Neither did they make public the dataset, nor the training code.&lt;/p&gt;
&lt;p&gt;While at first glance, this may look like a marketing move (&lt;em&gt;we created something so powerful that it’s too dangerous to be released to the public!&lt;/em&gt;), let’s not make things that easy on ourselves.&lt;/p&gt;
&lt;h2 id="with-great-power"&gt;With great power …&lt;/h2&gt;
&lt;p&gt;Whatever your take on the “innate priors in deep learning” discussion – how much knowledge needs to be hardwired into neural networks for them to solve tasks that involve more than pattern matching? – there is no doubt that in many areas, systems driven by “AI”&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; will impact our lives in an essential, and ever more powerful, way. Although there may be some awareness of the ethical, legal, and political problems this poses, it is probably fair to say that by and large, society is closing its eyes and holding its hands over its ears.&lt;/p&gt;
&lt;p&gt;If you were a deep learning researcher working in an area susceptible to abuse, generative ML say, what options would you have? As always in the history of science, what can be done will be done; all that remains is the search for antidotes. You may doubt that on a political level, constructive responses could evolve. But you can encourage other researchers to scrutinize the artifacts your algorithm created and develop other algorithms designed to spot the fakes – essentially like in malware detection. Of course this is a feedback system: Like with GANs, impostor algorithms will happily take the feedback and go on working on their shortcomings. But still, deliberately entering this circle &lt;em&gt;might&lt;/em&gt; be the only viable action to take.&lt;/p&gt;
&lt;p&gt;Although it may be the first thing that comes to mind, the question of veracity here isn’t the only one. With ML systems, it’s always: garbage in - garbage out. What is fed as training data determines the quality of the output, and any biases in its upbringing will carry through to an algorithm’s grown-up behavior. Without interventions, software designed to do translation, autocompletion and the like will be biased.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this light, all we can sensibly do is – constantly – point out the biases, analyze the artifacts, and conduct adversarial attacks. These are the kinds of responses OpenAI was asking for. In appropriate modesty, they called their approach an &lt;em&gt;experiment&lt;/em&gt;. Put plainly, no-one today knows how to deal with the threats emerging from powerful AI appearing in our lives. But there is no way around exploring our options.&lt;/p&gt;
&lt;h2 id="the-story-unwinding"&gt;The story unwinding&lt;/h2&gt;
&lt;p&gt;Three months later, OpenAI published an update to the initial post, stating that they had decided on a staged-release strategy. In addition to making public the next-in-size, 355M-parameters version of the model, they also released a dataset of &lt;a href="https://github.com/openai/gpt-2-output-dataset"&gt;generated outputs from all model sizes&lt;/a&gt;, to facilitate research. Last not least, they announced partnerships with academic and non-academic institutions, to increase “societal preparedness” (quote).&lt;/p&gt;
&lt;p&gt;Again after three months, in a &lt;a href="https://openai.com/blog/gpt-2-6-month-follow-up/"&gt;new post&lt;/a&gt; OpenAI announced the release of a yet larger – 774M-parameter – version of the model. At the same time, they reported evidence demonstrating insufficiencies in current statistical fake detection, as well as study results suggesting that indeed, text generators exist that can trick humans.&lt;/p&gt;
&lt;p&gt;Due to those results, they said, no decision had yet been taken as to the release of the biggest, the “real” model, of size 1.5 billion parameters.&lt;/p&gt;
&lt;h2 id="gpt-2"&gt;GPT-2&lt;/h2&gt;
&lt;p&gt;So what is GPT-2? Among state-of-the-art NLP models, GPT-2 stands out due to the gigantic (40G) dataset it was trained on, as well as its enormous number of weights. The architecture, in contrast, wasn’t new when it appeared. GPT-2, as well as its predecessor GPT &lt;span class="citation"&gt;(Radford 2018)&lt;/span&gt;, is based on a transformer architecture.&lt;/p&gt;
&lt;p&gt;The original Transformer &lt;span class="citation"&gt;(Vaswani et al. 2017)&lt;/span&gt; is an encoder-decoder architecture designed for sequence-to-sequence tasks, like machine translation. The paper introducing it was called “Attention is all you need”, emphasizing – by absence – what you don’t need: RNNs.&lt;/p&gt;
&lt;p&gt;Before its publication, the prototypical model for e.g. machine translation would use some form of RNN as an encoder, some form of RNN as a decoder, and an attention mechanism that at each time step of output generation, told the decoder where in the encoded input to look. Now the transformer was disposing with RNNs, essentially replacing them by a mechanism called &lt;em&gt;self-attention&lt;/em&gt; where already during &lt;em&gt;encoding&lt;/em&gt;, the encoder stack would encode each token not independently, but as a weighted sum of tokens encountered before (including itself).&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Many subsequent NLP models built on the Transformer, but – depending on purpose – either picked up the encoder stack only, or just the decoder stack. GPT-2 was trained to predict consecutive words in a sequence. It is thus a &lt;em&gt;language model&lt;/em&gt;, a term resounding the conception that an algorithm which can predict future words and sentences somehow has to &lt;em&gt;understand&lt;/em&gt; language (and a lot more, we might add). As there is no input to be encoded (apart from an optional one-time prompt), all that is needed is the stack of decoders.&lt;/p&gt;
&lt;p&gt;In our experiments, we’ll be using the biggest as-yet released pretrained model, but this being a pretrained model our degrees of freedom are limited. We can, of course, condition on different input prompts. In addition, we can influence the sampling algorithm used.&lt;/p&gt;
&lt;h2 id="sampling-options-with-gpt-2"&gt;Sampling options with GPT-2&lt;/h2&gt;
&lt;p&gt;Whenever a new token is to be predicted, a &lt;em&gt;softmax&lt;/em&gt; is taken over the vocabulary.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; Directly taking the softmax output amounts to maximum likelihood estimation. In reality, however, always choosing the maximum likelihood estimate results in highly repetitive output.&lt;/p&gt;
&lt;p&gt;A natural option seems to be using the softmax outputs as probabilities: Instead of just taking the &lt;em&gt;argmax&lt;/em&gt;, we sample from the output distribution. Unfortunately, this procedure has negative ramifications of its own. In a big vocabulary, very improbable words together make up a substantial part of the probability mass; at every step of generation, there is thus a non-negligible probability that an improbable word may be chosen. This word will now exert great influence on what is chosen next. In that manner, highly improbable sequences can build up.&lt;/p&gt;
&lt;p&gt;The task thus is to navigate between the Scylla of determinism and the Charybdis of weirdness. With the GPT-2 model presented below, we have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vary the &lt;em&gt;temperature&lt;/em&gt; (parameter &lt;code&gt;temperature&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;vary &lt;code&gt;top_k&lt;/code&gt;, the number of tokens considered; or&lt;/li&gt;
&lt;li&gt;vary &lt;code&gt;top_p&lt;/code&gt;, the probability mass considered.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;temperature&lt;/em&gt; concept is rooted in statistical mechanics. Looking at the Boltzmann distribution used to model state probabilities &lt;span class="math inline"&gt;\(p_i\)&lt;/span&gt;dependent on energy &lt;span class="math inline"&gt;\(\epsilon_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p_i \sim e^{-\frac{\epsilon_i}{kT}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see there is a moderating variable &lt;em&gt;temperature&lt;/em&gt; &lt;span class="math inline"&gt;\(T\)&lt;/span&gt;&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; that dependent on whether it’s below or above 1, will exert an either amplifying or attenuating influence on differences between probabilities.&lt;/p&gt;
&lt;p&gt;Analogously, in the context of predicting the next token, the individual logits are scaled by the temperature, and only then is the softmax taken. Temperatures below zero would make the model even more rigorous in choosing the maximum likelihood candidate; instead, we’d be interested in experimenting with temperatures above 1 to give higher chances to less likely candidates – hopefully, resulting in more human-like text.&lt;/p&gt;
&lt;p&gt;In top-&lt;span class="math inline"&gt;\(k\)&lt;/span&gt; sampling, the softmax outputs are sorted, and only the top-&lt;span class="math inline"&gt;\(k\)&lt;/span&gt; tokens are considered for sampling. The difficulty here is how to choose &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;. Sometimes a few words make up for almost all probability mass, in which case we’d like to choose a low number; in other cases the distribution is flat, and a higher number would be adequate.&lt;/p&gt;
&lt;p&gt;This sounds like rather than the number of candidates, a target probability mass should be specified. This is the approach suggested by &lt;span class="citation"&gt;(Holtzman et al. 2019)&lt;/span&gt;. Their method, called top-&lt;span class="math inline"&gt;\(p\)&lt;/span&gt;, or Nucleus sampling, computes the cumulative distribution of softmax outputs and picks a cut-off point &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;. Only the tokens constituting the top-&lt;span class="math inline"&gt;\(p\)&lt;/span&gt; portion of probability mass is retained for sampling.&lt;/p&gt;
&lt;p&gt;Now all you need to experiment with GPT-2 is the model.&lt;/p&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;p&gt;Install &lt;code&gt;gpt2&lt;/code&gt; from &lt;a href="https://github.com/r-tensorflow/gpt2"&gt;github&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;remotes::install_github(&amp;quot;r-tensorflow/gpt2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The R package being a wrapper to the implementation &lt;a href="https://github.com/openai/gpt-2"&gt;provided by OpenAI&lt;/a&gt;, we then need to install the Python runtime.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gpt2::install_gpt2(envname = &amp;quot;r-gpt2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will also install TensorFlow into the designated environment. All TensorFlow-related installation options (resp. recommendations) apply. Python 3 is required.&lt;/p&gt;
&lt;p&gt;While OpenAI indicates a dependency on TensorFlow 1.12, the R package was adapted to work with more current versions. The following versions have been found to be working fine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if running on GPU: TF 1.15&lt;/li&gt;
&lt;li&gt;CPU-only: TF 2.0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unsurprisingly, with GPT-2, running on GPU vs. CPU makes a huge difference.&lt;/p&gt;
&lt;p&gt;As a quick test if installation was successful, just run &lt;code&gt;gpt2()&lt;/code&gt; with the default parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# equivalent to:
# gpt2(prompt = &amp;quot;Hello my name is&amp;quot;, model = &amp;quot;124M&amp;quot;, seed = NULL, batch_size = 1, total_tokens = NULL,
#      temperature = 1, top_k = 0, top_p = 1)
# see ?gpt2 for an explanation of the parameters
#
# available models as of this writing: 124M, 355M, 774M
#
# on first run of a given model, allow time for download
gpt2()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="things-to-try-out"&gt;Things to try out&lt;/h2&gt;
&lt;p&gt;So &lt;em&gt;how dangerous exactly&lt;/em&gt; is GPT-2? We can’t say, as we don’t have access to the “real” model. But we can compare outputs, given the same prompt, obtained from all available models. The number of parameters has approximately doubled at every release – 124M, 355M, 774M. The biggest, yet unreleased, model, again has twice the number of weights: about 1.5B. In light of the evolution we observe, what do we expect to get from the 1.5B version?&lt;/p&gt;
&lt;p&gt;In performing these kinds of experiments, don’t forget about the different sampling strategies explained above. Non-default parameters might yield more real-looking results.&lt;/p&gt;
&lt;p&gt;Needless to say, the prompt we specify will make a difference. The models have been trained on a web-scraped dataset, &lt;a href="https://openai.com/blog/better-language-models/"&gt;subject to the quality criterion “3 stars on reddit”&lt;/a&gt;. We expect more fluency in certain areas than in others, to put it in a cautious way.&lt;/p&gt;
&lt;p&gt;Most definitely, we expect various biases in the outputs.&lt;/p&gt;
&lt;p&gt;Undoubtedly, by now the reader will have her own ideas about what to test. But there is more.&lt;/p&gt;
&lt;h2 id="language-models-are-unsupervised-multitask-learners"&gt;“Language Models are Unsupervised Multitask Learners”&lt;/h2&gt;
&lt;p&gt;Here we are citing the title of the official GPT-2 paper &lt;span class="citation"&gt;(Radford et al. 2019)&lt;/span&gt;. What is that supposed to mean? It means that a model like GPT-2, trained to predict the next token in naturally occurring text, can be used to “solve” standard NLP tasks that, in the majority of cases, are approached via supervised training (translation, for example).&lt;/p&gt;
&lt;p&gt;The clever idea is to present the model with cues about the task at hand. Some information on how to do this is given in the paper; more (unofficial; conflicting or confirming) hints can be found on the net. From what we found, here are some things you could try.&lt;/p&gt;
&lt;h3 id="summarization"&gt;Summarization&lt;/h3&gt;
&lt;p&gt;The clue to induce summarization is “TL;DR:”, written on a line by itself. The authors report that this worked best setting &lt;code&gt;top_k = 2&lt;/code&gt; and asking for 100 tokens. Of the generated output, they took the first three sentences as a summary.&lt;/p&gt;
&lt;p&gt;To try this out, we chose a sequence of content-wise standalone paragraphs from &lt;a href="https://climate.nasa.gov/evidence/"&gt;a NASA website dedicated to climate change&lt;/a&gt;, the idea being that with a clearly structured text like this, it should be easier to establish relationships between input and output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# put this in a variable called text

The planet&amp;#39;s average surface temperature has risen about 1.62 degrees Fahrenheit
(0.9 degrees Celsius) since the late 19th century, a change driven largely by
increased carbon dioxide and other human-made emissions into the atmosphere.4 Most
of the warming occurred in the past 35 years, with the five warmest years on record
taking place since 2010. Not only was 2016 the warmest year on record, but eight of
the 12 months that make up the year — from January through September, with the
exception of June — were the warmest on record for those respective months.

The oceans have absorbed much of this increased heat, with the top 700 meters
(about 2,300 feet) of ocean showing warming of more than 0.4 degrees Fahrenheit
since 1969.

The Greenland and Antarctic ice sheets have decreased in mass. Data from NASA&amp;#39;s
Gravity Recovery and Climate Experiment show Greenland lost an average of 286
billion tons of ice per year between 1993 and 2016, while Antarctica lost about 127
billion tons of ice per year during the same time period. The rate of Antarctica
ice mass loss has tripled in the last decade.

Glaciers are retreating almost everywhere around the world — including in the Alps,
Himalayas, Andes, Rockies, Alaska and Africa.

Satellite observations reveal that the amount of spring snow cover in the Northern
Hemisphere has decreased over the past five decades and that the snow is melting
earlier.

Global sea level rose about 8 inches in the last century. The rate in the last two
decades, however, is nearly double that of the last century and is accelerating
slightly every year.

Both the extent and thickness of Arctic sea ice has declined rapidly over the last
several decades.

The number of record high temperature events in the United States has been
increasing, while the number of record low temperature events has been decreasing,
since 1950. The U.S. has also witnessed increasing numbers of intense rainfall events.

Since the beginning of the Industrial Revolution, the acidity of surface ocean
waters has increased by about 30 percent.13,14 This increase is the result of humans
emitting more carbon dioxide into the atmosphere and hence more being absorbed into
the oceans. The amount of carbon dioxide absorbed by the upper layer of the oceans
is increasing by about 2 billion tons per year.

TL;DR:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gpt2(prompt = text,
     model = &amp;quot;774M&amp;quot;,
     total_tokens = 100,
     top_k = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the generated result, whose quality on purpose we don’t comment on. (Of course one can’t help having “gut reactions”; but to actually present an evaluation we’d want to conduct a systematic experiment, varying not only input prompts but also function parameters. All we want to show in this post is how you can set up such experiments yourself.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;\nGlobal temperatures are rising, but the rate of warming has been accelerating.
\n\nThe oceans have absorbed much of the increased heat, with the top 700 meters of
ocean showing warming of more than 0.4 degrees Fahrenheit since 1969.
\n\nGlaciers are retreating almost everywhere around the world, including in the
Alps, Himalayas, Andes, Rockies, Alaska and Africa.
\n\nSatellite observations reveal that the amount of spring snow cover in the
Northern Hemisphere has decreased over the past&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Speaking of parameters to vary, – they fall into two classes, in a way. It is unproblematic to vary the sampling strategy, let alone the prompt. But for tasks like summarization, or the ones we’ll see below, it doesn’t feel right to have to tell the model how many tokens to generate. Finding the right length of the answer seems to be part of the task.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; Breaking our “we don’t judge” rule just a single time, we can’t help but remark that even in less clear-cut tasks, language generation models that are meant to approach human-level competence would have to fulfill a criterion of &lt;em&gt;relevance&lt;/em&gt; &lt;span class="citation"&gt;(Grice 1975)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id="question-answering"&gt;Question answering&lt;/h3&gt;
&lt;p&gt;To trick GPT-2 into question answering, the common approach seems to be presenting it with a number of &lt;em&gt;Q:&lt;/em&gt; / &lt;em&gt;A:&lt;/em&gt; pairs, followed by a final question and a final &lt;em&gt;A:&lt;/em&gt; on its own line.&lt;/p&gt;
&lt;p&gt;We tried like this, asking questions on the above climate change - related text:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;q &amp;lt;- str_c(str_replace(text, &amp;quot;\nTL;DR:\n&amp;quot;, &amp;quot;&amp;quot;), &amp;quot; \n&amp;quot;, &amp;quot;
Q: What time period has seen the greatest increase in global temperature? 
A: The last 35 years. 
Q: What is happening to the Greenland and Antarctic ice sheets? 
A: They are rapidly decreasing in mass. 
Q: What is happening to glaciers? 
A: &amp;quot;)

gpt2(prompt = q,
     model = &amp;quot;774M&amp;quot;,
     total_tokens = 10,
     top_p = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This did not turn out so well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;\nQ: What is happening to the Arctic sea&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But maybe, more successful tricks exist.&lt;/p&gt;
&lt;h3 id="translation"&gt;Translation&lt;/h3&gt;
&lt;p&gt;For translation, the strategy presented in the paper is juxtaposing sentences in two languages, joined by " = “, followed by a single sentence on its own and a” =". Thinking that English &amp;lt;-&amp;gt; French might be the combination best represented in the training corpus, we tried the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# save this as eng_fr

The issue of climate change concerns all of us. = La question du changement
climatique nous affecte tous. \n
The problems of climate change and global warming affect all of humanity, as well as
the entire ecosystem. = Les problèmes créés par les changements climatiques et le
réchauffement de la planète touchent toute l&amp;#39;humanité, de même que l&amp;#39;écosystème tout
entier.\n
Climate Change Central is a not-for-profit corporation in Alberta, and its mandate
is to reduce Alberta&amp;#39;s greenhouse gas emissions. = Climate Change Central est une
société sans but lucratif de l&amp;#39;Alberta ayant pour mission de réduire les émissions
de gaz. \n
Climate change will affect all four dimensions of food security: food availability,
food accessibility, food utilization and food systems stability. = &amp;quot;

gpt2(prompt = eng_fr,
     model = &amp;quot;774M&amp;quot;,
     total_tokens = 25,
     top_p = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results varied a lot between different runs. Here are three examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;ét durant les pages relevantes du Centre d&amp;#39;Action des Sciences Humaines et dans sa
species situé,&amp;quot;

&amp;quot;études des loi d&amp;#39;affaires, des reasons de demande, des loi d&amp;#39;abord and de&amp;quot;

&amp;quot;étiquettes par les changements changements changements et les bois d&amp;#39;escalier,
ainsi que des&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With that, we conclude our tour of “what to explore with GPT-2”. Keep in mind that the yet-unreleased model has double the number of parameters; essentially, &lt;em&gt;what we see is not what we get&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This post’s goal was to show how you can experiment with GPT-2 from R. But it also reflects the decision to, from time to time, widen the narrow focus on technology and allow ourselves to think about ethical and societal implications of ML/DL.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-grice1975logic"&gt;
&lt;p&gt;Grice, H. P. 1975. “Logic and Conversation.” In &lt;em&gt;Syntax and Semantics: Vol. 3: Speech Acts&lt;/em&gt;, 41–58. Academic Press. &lt;a href="http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf"&gt;http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2019arXiv190409751H"&gt;
&lt;p&gt;Holtzman, Ari, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. “The Curious Case of Neural Text Degeneration.” &lt;em&gt;arXiv E-Prints&lt;/em&gt;, April, arXiv:1904.09751. &lt;a href="http://arxiv.org/abs/1904.09751"&gt;http://arxiv.org/abs/1904.09751&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Radford2018ImprovingLU"&gt;
&lt;p&gt;Radford, Alec. 2018. “Improving Language Understanding by Generative Pre-Training.” In.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-radford2019language"&gt;
&lt;p&gt;Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1906-08976"&gt;
&lt;p&gt;Sun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. “Mitigating Gender Bias in Natural Language Processing: Literature Review.” &lt;em&gt;CoRR&lt;/em&gt; abs/1906.08976. &lt;a href="http://arxiv.org/abs/1906.08976"&gt;http://arxiv.org/abs/1906.08976&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-NIPS2017_7181"&gt;
&lt;p&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. &lt;a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"&gt;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The acronym here is used for convenience only, not to imply any specific view on what is, or is not, “artificial intelligence”.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;For an overview of bias detection and mitigation specific to gender bias, see e.g. &lt;span class="citation"&gt;(Sun et al. 2019)&lt;/span&gt;&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;For a detailed, and exceptionally visual, explanation of the Transformer, &lt;em&gt;the&lt;/em&gt; place to go is &lt;a href="https://jalammar.github.io/illustrated-transformer/"&gt;Jay Alammar’s post&lt;/a&gt;. Also check out &lt;a href="http://jalammar.github.io/illustrated-bert/"&gt;The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning&lt;/a&gt;, the article that might be held mainly responsible for the pervasive sesame-streetification of NLP.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;For an introduction to how softmax activation behaves, see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/"&gt;Winner takes all: A look at activations and cost functions&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;&lt;span class="math inline"&gt;\(k\)&lt;/span&gt; is the Boltzmann constant&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;Formally, &lt;code&gt;total_tokens&lt;/code&gt; isn’t a required parameter. If not passed, a default based on model size will be applied, resulting in lengthy output that definitely will have to be processed by some human-made rule.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">cef46361309ee0d6289fbbf1a3580ce0</distill:md5>
      <category>Natural Language Processing</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</guid>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>TensorFlow 2.0 is here - what changes for R users?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</link>
      <description>


&lt;p&gt;The wait is over – TensorFlow 2.0 (TF 2) is now officially here! What does this mean for us, users of R packages &lt;code&gt;keras&lt;/code&gt; and/or &lt;code&gt;tensorflow&lt;/code&gt;, which, as we know, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/"&gt;rely on the Python TensorFlow backend&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;Before we go into details and explanations, here is an &lt;em&gt;all-clear&lt;/em&gt;, for the concerned user who fears their &lt;code&gt;keras&lt;/code&gt; code might become obsolete (it won’t).&lt;/p&gt;
&lt;h2 id="dont-panic"&gt;Don’t panic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you are using &lt;code&gt;keras&lt;/code&gt; in standard ways, such as those depicted in most code examples and tutorials seen on the web, and things have been working fine for you in recent &lt;code&gt;keras&lt;/code&gt; releases (&amp;gt;= 2.2.4.1), don’t worry. Most everything should work without major changes.&lt;/li&gt;
&lt;li&gt;If you are using an older release of &lt;code&gt;keras&lt;/code&gt; (&amp;lt; 2.2.4.1), syntactically things should work fine as well, but you will want to check for changes in behavior/performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And now for some news and background. This post aims to do three things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explain the above &lt;em&gt;all-clear&lt;/em&gt; statement. Is it really that simple – what exactly is going on?&lt;/li&gt;
&lt;li&gt;Characterize the changes brought about by TF 2, from the &lt;em&gt;point of view of the R user&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;And, perhaps most interestingly: Take a look at what is going on, in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem, around new functionality related to the advent of TF 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="some-background"&gt;Some background&lt;/h2&gt;
&lt;p&gt;So if all still works fine (assuming standard usage), why so much ado about TF 2 in Python land?&lt;/p&gt;
&lt;p&gt;The difference is that on the R side, for the vast majority of users, the framework you used to do deep learning was &lt;code&gt;keras&lt;/code&gt;. &lt;code&gt;tensorflow&lt;/code&gt; was needed just occasionally, or not at all.&lt;/p&gt;
&lt;p&gt;Between &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt;, there was a clear separation of responsibilities: &lt;code&gt;keras&lt;/code&gt; was the frontend, depending on TensorFlow as a low-level backend, just like the &lt;a href="http://keras.io"&gt;original Python Keras&lt;/a&gt; it was wrapping did. &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In some cases, this lead to people using the words &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt; almost synonymously: Maybe they said &lt;code&gt;tensorflow&lt;/code&gt;, but the code they wrote was &lt;code&gt;keras&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Things were different in Python land. There was original Python Keras, but TensorFlow had its own &lt;code&gt;layers&lt;/code&gt; API, and there were a number of third-party high-level APIs built on TensorFlow. Keras, in contrast, was a separate library that just happened to rely on TensorFlow.&lt;/p&gt;
&lt;p&gt;So in Python land, now we have a big change: &lt;em&gt;With TF 2, Keras (as incorporated in the TensorFlow codebase) is now the official high-level API for TensorFlow&lt;/em&gt;. To bring this across has been a major point of Google’s TF 2 information campaign since the early stages.&lt;/p&gt;
&lt;p&gt;As R users, who have been focusing on &lt;code&gt;keras&lt;/code&gt; all the time, we are essentially less affected. Like we said above, syntactically most everything stays the way it was. So why differentiate between different &lt;code&gt;keras&lt;/code&gt; versions?&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;keras&lt;/code&gt; was written, there was original Python Keras, and that was the library we were binding to. However, Google started to incorporate original Keras code into their TensorFlow codebase as a fork, to continue development independently. For a while there were two “Kerases”: Original Keras and &lt;code&gt;tf.keras&lt;/code&gt;. Our R &lt;code&gt;keras&lt;/code&gt; offered to switch between implementations &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, the default being original Keras.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;keras&lt;/code&gt; release 2.2.4.1, anticipating discontinuation of original Keras and wanting to get ready for TF 2, we switched to using &lt;code&gt;tf.keras&lt;/code&gt; as the default. While in the beginning, the &lt;code&gt;tf.keras&lt;/code&gt; fork and original Keras developed more or less in sync, the latest developments for TF 2 brought with them bigger changes in the &lt;code&gt;tf.keras&lt;/code&gt; codebase, especially as regards optimizers. This is why, if you are using a &lt;code&gt;keras&lt;/code&gt; version &amp;lt; 2.2.4.1, upgrading to TF 2 you will want to check for changes in behavior and/or performance. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;That’s it for some background. In sum, we’re happy most existing code will run just fine. But for us R users, &lt;em&gt;something&lt;/em&gt; must be changing as well, right?&lt;/p&gt;
&lt;h2 id="tf-2-in-a-nutshell-from-an-r-perspective"&gt;TF 2 in a nutshell, from an R perspective&lt;/h2&gt;
&lt;p&gt;In fact, the most evident-on-user-level change is something we wrote several posts about, more than a year ago &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. By then, &lt;em&gt;eager execution&lt;/em&gt; was a brand-new option that had to be turned on explicitly; TF 2 now makes it the default. Along with it came &lt;em&gt;custom models&lt;/em&gt; (a.k.a. subclassed models, in Python land) and &lt;em&gt;custom training&lt;/em&gt;, making use of &lt;code&gt;tf$GradientTape&lt;/code&gt;. Let’s talk about what those termini refer to, and how they are relevant to R users.&lt;/p&gt;
&lt;h3 id="eager-execution"&gt;Eager Execution&lt;/h3&gt;
&lt;p&gt;In TF 1, it was all about the &lt;em&gt;graph&lt;/em&gt; you built when defining your model. The graph, that was – and is – an &lt;em&gt;Abstract Syntax Tree&lt;/em&gt; (AST), with operations as nodes and &lt;em&gt;tensors “flowing”&lt;/em&gt; along the edges. Defining a graph and running it (on actual data) were different steps.&lt;/p&gt;
&lt;p&gt;In contrast, with eager execution, operations are run directly when defined.&lt;/p&gt;
&lt;p&gt;While this is a more-than-substantial change that must have required lots of resources to implement, if you use &lt;code&gt;keras&lt;/code&gt; you won’t notice. Just as previously, the typical &lt;code&gt;keras&lt;/code&gt; workflow of &lt;code&gt;create model&lt;/code&gt; -&amp;gt; &lt;code&gt;compile model&lt;/code&gt; -&amp;gt; &lt;code&gt;train model&lt;/code&gt; never made you think about there being two distinct phases (define and run), now again you don’t have to do anything. Even though the overall execution mode is eager, Keras models are trained in graph mode, to maximize performance. We will talk about how this is done in part 3 when introducing the &lt;code&gt;tfautograph&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;keras&lt;/code&gt; runs in graph mode, how can you even see that eager execution is “on”? Well, in TF 1, when you ran a TensorFlow operation on a tensor &lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, like so&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tf$math$cumprod(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is what you saw:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tensor(&amp;quot;Cumprod:0&amp;quot;, shape=(5,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract the actual values, you had to create a TensorFlow &lt;em&gt;Session&lt;/em&gt; and &lt;code&gt;run&lt;/code&gt; the tensor, or alternatively, use &lt;code&gt;keras::k_eval&lt;/code&gt; that did this under the hood:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
tf$math$cumprod(1:5) %&amp;gt;% k_eval()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]   1   2   6  24 120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With TF 2’s execution mode defaulting to &lt;em&gt;eager&lt;/em&gt;, we now automatically see the values contained in the tensor: &lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tf$math$cumprod(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([  1   2   6  24 120], shape=(5,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that’s eager execution. In our last year’s &lt;em&gt;Eager&lt;/em&gt;-category blog posts, it was always accompanied by &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom models&lt;/a&gt;, so let’s turn there next.&lt;/p&gt;
&lt;h3 id="custom-models"&gt;Custom models&lt;/h3&gt;
&lt;p&gt;As a &lt;code&gt;keras&lt;/code&gt; user, probably you’re familiar with the &lt;em&gt;sequential&lt;/em&gt; and &lt;em&gt;functional&lt;/em&gt; styles of building a model. Custom models allow for even greater flexibility than functional-style ones. Check out the &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;documentation&lt;/a&gt; for how to create one.&lt;/p&gt;
&lt;p&gt;Last year’s series on eager execution has plenty of examples using custom models, featuring not just their flexibility, but another important aspect as well: the way they allow for modular, easily-intelligible code. &lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Encoder-decoder scenarios are a natural match. If you have seen, or written, “old-style” code for a Generative Adversarial Network (GAN), imagine something like this instead:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# define the generator (simplified)
generator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      # define layers for the generator 
      self$fc1 &amp;lt;- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)
      self$batchnorm1 &amp;lt;- layer_batch_normalization()
      # more layers ...
      
      # define what should happen in the forward pass
      function(inputs, mask = NULL, training = TRUE) {
        self$fc1(inputs) %&amp;gt;%
          self$batchnorm1(training = training) %&amp;gt;%
          # call remaining layers ...
      }
    })
  }

# define the discriminator
discriminator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$conv1 &amp;lt;- layer_conv_2d(filters = 64, #...)
      self$leaky_relu1 &amp;lt;- layer_activation_leaky_relu()
      # more layers ...
    
      function(inputs, mask = NULL, training = TRUE) {
        inputs %&amp;gt;% self$conv1() %&amp;gt;%
          self$leaky_relu1() %&amp;gt;%
          # call remaining layers ...
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coded like this, picture the generator and the discriminator as agents, ready to engage in what is actually the opposite of a zero-sum game.&lt;/p&gt;
&lt;p&gt;The game, then, can be nicely coded using &lt;em&gt;custom training&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="custom-training"&gt;Custom training&lt;/h3&gt;
&lt;p&gt;Custom training, as opposed to using &lt;code&gt;keras&lt;/code&gt; &lt;code&gt;fit&lt;/code&gt;, allows to interleave the training of several models. Models are &lt;em&gt;called&lt;/em&gt; on data, and all calls have to happen inside the context of a &lt;code&gt;GradientTape&lt;/code&gt;. In eager mode, &lt;code&gt;GradientTape&lt;/code&gt;s are used to keep track of operations such that during backprop, their gradients can be calculated.&lt;/p&gt;
&lt;p&gt;The following code example shows how using &lt;code&gt;GradientTape&lt;/code&gt;-style training, we can &lt;em&gt;see&lt;/em&gt; our actors play against each other:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# zooming in on a single batch of a single epoch
with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {
  
  # first, it&amp;#39;s the generator&amp;#39;s call (yep pun intended)
  generated_images &amp;lt;- generator(noise)
  # now the discriminator gives its verdict on the real images 
  disc_real_output &amp;lt;- discriminator(batch, training = TRUE)
  # as well as the fake ones
  disc_generated_output &amp;lt;- discriminator(generated_images, training = TRUE)
  
  # depending on the discriminator&amp;#39;s verdict we just got,
  # what&amp;#39;s the generator&amp;#39;s loss?
  gen_loss &amp;lt;- generator_loss(disc_generated_output)
  # and what&amp;#39;s the loss for the discriminator?
  disc_loss &amp;lt;- discriminator_loss(disc_real_output, disc_generated_output)
}) })

# now outside the tape&amp;#39;s context compute the respective gradients
gradients_of_generator &amp;lt;- gen_tape$gradient(gen_loss, generator$variables)
gradients_of_discriminator &amp;lt;- disc_tape$gradient(disc_loss, discriminator$variables)
 
# and apply them!
generator_optimizer$apply_gradients(
  purrr::transpose(list(gradients_of_generator, generator$variables)))
discriminator_optimizer$apply_gradients(
  purrr::transpose(list(gradients_of_discriminator, discriminator$variables)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, compare this with pre-TF 2 GAN training – it makes for a &lt;em&gt;lot&lt;/em&gt; more readable code.&lt;/p&gt;
&lt;p&gt;As an aside, last year’s post series may have created the impression that with eager execution, you &lt;em&gt;have&lt;/em&gt; to use custom (&lt;code&gt;GradientTape&lt;/code&gt;) training instead of Keras-style &lt;code&gt;fit&lt;/code&gt;. In fact, that was the case at the time those posts were written. Today, Keras-style code works just fine with eager execution.&lt;/p&gt;
&lt;p&gt;So now with TF 2, we are in an optimal position. We &lt;em&gt;can&lt;/em&gt; use custom training when we want to, but we don’t have to if declarative &lt;code&gt;fit&lt;/code&gt; is all we need.&lt;/p&gt;
&lt;p&gt;That’s it for a flashlight on what TF 2 means to R users. We now take a look around in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem to see new developments – recent-past, present and future – in areas like data loading, preprocessing, and more.&lt;/p&gt;
&lt;h2 id="new-developments-in-the-r-tensorflow-ecosystem"&gt;New developments in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem&lt;/h2&gt;
&lt;p&gt;These are what we’ll cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tfdatasets&lt;/code&gt;: Over the recent past, &lt;code&gt;tfdatasets&lt;/code&gt; pipelines have become the preferred way for data loading and preprocessing.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;feature columns&lt;/em&gt; and &lt;em&gt;feature specs&lt;/em&gt;: Specify your features &lt;code&gt;recipes&lt;/code&gt;-style and have &lt;code&gt;keras&lt;/code&gt; generate the adequate layers for them.&lt;/li&gt;
&lt;li&gt;Keras preprocessing layers: Keras preprocessing pipelines integrating functionality such as data augmentation (currently in planning).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tfhub&lt;/code&gt;: Use pretrained models as &lt;code&gt;keras&lt;/code&gt; layers, and/or as feature columns in a &lt;code&gt;keras&lt;/code&gt; model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf_function&lt;/code&gt; and &lt;code&gt;tfautograph&lt;/code&gt;: Speed up training by running parts of your code in graph mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="tfdatasets-input-pipelines"&gt;&lt;em&gt;tfdatasets&lt;/em&gt; input pipelines&lt;/h3&gt;
&lt;p&gt;For 2 years now, the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; package has been available to load data for training Keras models in a streaming way.&lt;/p&gt;
&lt;p&gt;Logically, there are three steps involved:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;First, data has to be loaded from some place. This could be a csv file, a directory containing images, or other sources. In this recent example from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet/"&gt;Image segmentation with U-Net&lt;/a&gt;, information about file names was first stored into an R &lt;code&gt;tibble&lt;/code&gt;, and then &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/tensor_slices_dataset.html"&gt;tensor_slices_dataset&lt;/a&gt; was used to create a &lt;code&gt;dataset&lt;/code&gt; from it:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- tibble(
  img = list.files(here::here(&amp;quot;data-raw/train&amp;quot;), full.names = TRUE),
  mask = list.files(here::here(&amp;quot;data-raw/train_masks&amp;quot;), full.names = TRUE)
)

data &amp;lt;- initial_split(data, prop = 0.8)

dataset &amp;lt;- training(data) %&amp;gt;%  
  tensor_slices_dataset() &lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;Once we have a &lt;code&gt;dataset&lt;/code&gt;, we perform any required transformations, &lt;em&gt;mapping&lt;/em&gt; over the batch dimension. Continuing with the example from the U-Net post, here we use functions from the &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/"&gt;tf.image&lt;/a&gt; module to (1) load images according to their file type, (2) scale them to values between 0 and 1 (converting to &lt;code&gt;float32&lt;/code&gt; at the same time), and (3) resize them to the desired format:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;%
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
  )) %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
  )) %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$resize(.x$img, size = shape(128, 128)),
    mask = tf$image$resize(.x$mask, size = shape(128, 128))
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how once you know what these functions do, they free you of a lot of thinking (remember how in the “old” Keras approach to image preprocessing, you were doing things like dividing pixel values by 255 “by hand”?)&lt;/p&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;After transformation, a third conceptual step relates to item arrangement. You will often want to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/tensor_slices_dataset.html"&gt;shuffle&lt;/a&gt;, and you certainly will want to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/dataset_batch.html"&gt;batch&lt;/a&gt; the data:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt; if (train) {
    dataset &amp;lt;- dataset %&amp;gt;% 
      dataset_shuffle(buffer_size = batch_size*128)
  }

dataset &amp;lt;- dataset %&amp;gt;%  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summing up, using &lt;code&gt;tfdatasets&lt;/code&gt; you build a pipeline, from loading over transformations to batching, that can then be fed directly to a Keras model. From preprocessing, let’s go a step further and look at a new, extremely convenient way to do feature engineering.&lt;/p&gt;
&lt;h3 id="feature-columns-and-feature-specs"&gt;Feature columns and feature specs&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;Feature columns&lt;/a&gt; as such are a Python-TensorFlow feature, while &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_spec.html"&gt;feature specs&lt;/a&gt; are an R-only idiom modeled after the popular &lt;a href="https://cran.r-project.org/web/packages/recipes/index.html"&gt;recipes&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;It all starts off with creating a feature spec object, using formula syntax to indicate what’s predictor and what’s target:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfdatasets)
hearts_dataset &amp;lt;- tensor_slices_dataset(hearts)
spec &amp;lt;- feature_spec(hearts_dataset, target ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That specification is then refined by successive information about how we want to make use of the raw predictors. This is where feature columns come into play. Different column types exist, of which you can see a few in the following code snippet:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;spec &amp;lt;- feature_spec(hearts, target ~ .) %&amp;gt;% 
  step_numeric_column(
    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,
    normalizer_fn = scaler_standard()
  ) %&amp;gt;% 
  step_categorical_column_with_vocabulary_list(thal) %&amp;gt;% 
  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %&amp;gt;% 
  step_indicator_column(thal) %&amp;gt;% 
  step_embedding_column(thal, dimension = 2) %&amp;gt;% 
  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %&amp;gt;%
  step_indicator_column(crossed_thal_bucketized_age)

spec %&amp;gt;% fit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened here is that we told TensorFlow, please take all numeric columns (besides a few ones listed exprès) and scale them; take column &lt;code&gt;thal&lt;/code&gt;, treat it as categorical and create an embedding for it; discretize &lt;code&gt;age&lt;/code&gt; according to the given ranges; and finally, create a &lt;em&gt;crossed column&lt;/em&gt; to capture interaction between &lt;code&gt;thal&lt;/code&gt; and that discretized age-range column. &lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is nice, but when creating the model, we’ll still have to define all those layers, right? (Which would be pretty cumbersome, having to figure out all the right dimensions…) Luckily, we don’t have to. In sync with &lt;code&gt;tfdatasets&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt; now provides &lt;a href="https://tensorflow.rstudio.com/keras/reference/layer_dense_features.html"&gt;layer_dense_features&lt;/a&gt; to create a layer tailor-made to accommodate the specification.&lt;/p&gt;
&lt;p&gt;And we don’t need to create separate input layers either, due to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/layer_input_from_dataset.html"&gt;layer_input_from_dataset&lt;/a&gt;. Here we see both in action:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input &amp;lt;- layer_input_from_dataset(hearts %&amp;gt;% select(-target))

output &amp;lt;- input %&amp;gt;% 
  layer_dense_features(feature_columns = dense_features(spec)) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From then on, it’s just normal &lt;code&gt;keras&lt;/code&gt; &lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt;. See the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;vignette&lt;/a&gt; for the complete example. There also is a &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/"&gt;post on feature columns&lt;/a&gt; explaining more of how this works, and illustrating the time-and-nerve-saving effect by comparing with the pre-feature-spec way of working with heterogeneous datasets.&lt;/p&gt;
&lt;p&gt;As a last item on the topics of preprocessing and feature engineering, let’s look at a promising thing to come in what we hope is the near future.&lt;/p&gt;
&lt;h3 id="keras-preprocessing-layers"&gt;Keras preprocessing layers&lt;/h3&gt;
&lt;p&gt;Reading what we wrote above about using &lt;code&gt;tfdatasets&lt;/code&gt; for building a input pipeline, and seeing how we gave an image loading example, you may have been wondering: What about data augmentation functionality available, historically, through &lt;code&gt;keras&lt;/code&gt;? Like &lt;code&gt;image_data_generator&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This functionality does not seem to fit. But a nice-looking solution is in preparation. In the Keras community, the recent &lt;a href="https://github.com/keras-team/governance/blob/master/rfcs/20190729-keras-preprocessing-redesign.md"&gt;RFC on preprocessing layers for Keras&lt;/a&gt; addresses this topic. The RFC is still under discussion, but as soon as it gets implemented in Python we’ll follow up on the R side.&lt;/p&gt;
&lt;p&gt;The idea is to provide (chainable) preprocessing layers to be used for data transformation and/or augmentation in areas such as image classification, image segmentation, object detection, text processing, and more. &lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; The envisioned, in the RFC, pipeline of preprocessing layers should return a &lt;code&gt;dataset&lt;/code&gt;, for compatibility with &lt;code&gt;tf.data&lt;/code&gt; (our &lt;code&gt;tfdatasets&lt;/code&gt;). We’re definitely looking forward to having available this sort of workflow!&lt;/p&gt;
&lt;p&gt;Let’s move on to the next topic, the common denominator being convenience. But now convenience means not having to build billion-parameter models yourself!&lt;/p&gt;
&lt;h3 id="tensorflow-hub-and-the-tfhub-package"&gt;Tensorflow Hub and the &lt;code&gt;tfhub&lt;/code&gt; package&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/hub"&gt;Tensorflow Hub&lt;/a&gt; is a library for publishing and using pretrained models. Existing models can be browsed on &lt;a href="https://tfhub.dev/"&gt;tfhub.dev&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As of this writing, the original Python library is still under development, so complete stability is not guaranteed. That notwithstanding, the &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub&lt;/a&gt; R package already allows for some instructive experimentation.&lt;/p&gt;
&lt;p&gt;The traditional Keras idea of using pretrained models typically involved either (1) applying a model like &lt;em&gt;MobileNet&lt;/em&gt; as a whole, including its output layer, or (2) chaining a “custom head” to its penultimate layer &lt;a href="#fn10" class="footnote-ref" id="fnref10"&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. In contrast, the TF Hub idea is to use a pretrained model as a &lt;em&gt;module&lt;/em&gt; in a larger setting.&lt;/p&gt;
&lt;p&gt;There are two main ways to accomplish this, namely, integrating a module as a &lt;code&gt;keras&lt;/code&gt; layer and using it as a feature column. The &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub README&lt;/a&gt; shows the first option:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfhub)
library(keras)

input &amp;lt;- layer_input(shape = c(32, 32, 3))

output &amp;lt;- input %&amp;gt;%
  # we are using a pre-trained MobileNet model!
  layer_hub(handle = &amp;quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&amp;quot;) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)

model &amp;lt;- keras_model(input, output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the &lt;a href="https://github.com/rstudio/tfhub/blob/master/vignettes/examples/feature_column.R"&gt;tfhub feature columns vignette&lt;/a&gt; illustrates the second one:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;spec &amp;lt;- dataset_train %&amp;gt;%
  feature_spec(AdoptionSpeed ~ .) %&amp;gt;%
  step_text_embedding_column(
    Description,
    module_spec = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder/2&amp;quot;
    ) %&amp;gt;%
  step_image_embedding_column(
    img,
    module_spec = &amp;quot;https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3&amp;quot;
  ) %&amp;gt;%
  step_numeric_column(Age, Fee, Quantity, normalizer_fn = scaler_standard()) %&amp;gt;%
  step_categorical_column_with_vocabulary_list(
    has_type(&amp;quot;string&amp;quot;), -Description, -RescuerID, -img_path, -PetID, -Name
  ) %&amp;gt;%
  step_embedding_column(Breed1:Health, State)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both usage modes illustrate the high potential of working with Hub modules. Just be cautioned that, as of today, not every model published will work with TF 2.&lt;/p&gt;
&lt;h3 id="tf_function-tf-autograph-and-the-r-package-tfautograph"&gt;&lt;code&gt;tf_function&lt;/code&gt;, TF autograph and the R package &lt;code&gt;tfautograph&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;As explained above, the default execution mode in TF 2 is eager. For performance reasons however, in many cases it will be desirable to compile parts of your code into a graph. Calls to Keras layers, for example, are run in graph mode.&lt;/p&gt;
&lt;p&gt;To compile a function into a graph, wrap it in a call to &lt;code&gt;tf_function&lt;/code&gt;, as done e.g. in the post &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/"&gt;Modeling censored data with tfprobability&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;run_mcmc &amp;lt;- function(kernel) {
  kernel %&amp;gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = tf$ones_like(initial_betas),
    trace_fn = trace_fn
  )
}

# important for performance: run HMC in graph mode
run_mcmc &amp;lt;- tf_function(run_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the Python side, the &lt;code&gt;tf.autograph&lt;/code&gt; module automatically translates Python control flow statements into appropriate graph operations.&lt;/p&gt;
&lt;p&gt;Independently of &lt;code&gt;tf.autograph&lt;/code&gt;, the R package &lt;a href="https://t-kalinowski.github.io/tfautograph/index.html"&gt;tfautograph&lt;/a&gt;, developed by Tomasz Kalinowski, implements control flow conversion directly from R to TensorFlow. This lets you use R’s &lt;code&gt;if&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;break&lt;/code&gt;, and &lt;code&gt;next&lt;/code&gt; when writing custom training flows. Check out the package’s extensive documentation for instructive examples!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With that, we end our introduction of TF 2 and the new developments that surround it.&lt;/p&gt;
&lt;p&gt;If you have been using &lt;code&gt;keras&lt;/code&gt; in traditional ways, how much changes &lt;em&gt;for you&lt;/em&gt; is mainly up to you: Most everything will still work, but new options exist to write more performant, more modular, more elegant code. In particular, check out &lt;code&gt;tfdatasets&lt;/code&gt; pipelines for efficient data loading.&lt;/p&gt;
&lt;p&gt;If you’re an advanced user requiring non-standard setup, have a look into custom training and custom models, and consult the &lt;code&gt;tfautograph&lt;/code&gt; documentation to see how the package can help.&lt;/p&gt;
&lt;p&gt;In any case, stay tuned for upcoming posts showing some of the above-mentioned functionality in action. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Original Python Keras, and thus, R &lt;code&gt;keras&lt;/code&gt;, supported additional backends: Theano and CNTK. But the default backend in R &lt;code&gt;keras&lt;/code&gt; always was TensorFlow.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Note the terminology: in R &lt;code&gt;keras&lt;/code&gt;, &lt;em&gt;implementation&lt;/em&gt; referred to the Python library (Keras or TensorFlow, with its module &lt;code&gt;tf.keras&lt;/code&gt;) bound to, while &lt;em&gt;backend&lt;/em&gt; referred to the framework providing low-level operations, which could be one of Theano, TensorFlow and CNTK.)&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;E.g., parameters like &lt;em&gt;learning_rate&lt;/em&gt; may have to be adapted.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/"&gt;More flexible models with TensorFlow eager execution and Keras&lt;/a&gt; for an overview and annotated links.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Here the nominal input is an R vector that gets converted to a Python list by &lt;code&gt;reticulate&lt;/code&gt;, and to a tensor by TensorFlow.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;This is still a tensor though. To continue working with its values in R, we need to convert it to R using &lt;code&gt;as.numeric&lt;/code&gt;, &lt;code&gt;as.matrix&lt;/code&gt;, &lt;code&gt;as.array&lt;/code&gt; etc.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;For example, see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/"&gt;Generating images with Keras and TensorFlow eager execution&lt;/a&gt; on GANs, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/"&gt;Neural style transfer with eager execution and Keras&lt;/a&gt; on neural style transfer, or &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt; on Variational Autoencoders.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;&lt;code&gt;step_indicator_column&lt;/code&gt; is there (twice) for technical reasons. Our &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/"&gt;post on feature columns&lt;/a&gt; explains.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;As readers working in e.g. image segmentation will know, data augmentation is not as easy as just using &lt;code&gt;image_data_generator&lt;/code&gt; on the input images, as analogous distortions have to be applied to the masks.&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn10"&gt;&lt;p&gt;or block of layers&lt;a href="#fnref10" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c184304f260d68951aaedf2326fc05d7</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</guid>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/images/thumb.png" medium="image" type="image/png" width="400" height="400"/>
    </item>
    <item>
      <title>Auto-Keras: Tuning-free deep learning from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Cruz Rodriguez</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</link>
      <description>


&lt;p&gt;&lt;em&gt;Today, we’re happy to feature a guest post written by Juan Cruz, showing how to use Auto-Keras from R. Juan holds a master’s degree in Computer Science. Currently, he is finishing his master’s degree in Applied Statistics, as well as a Ph.D. in Computer Science, at the Universidad Nacional de Córdoba. He started his R journey almost six years ago, applying statistical methods to biology data. He enjoys software projects focused on making machine learning and data science available to everyone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the past few years, artificial intelligence has been a subject of intense media hype. Machine learning, deep learning, and artificial intelligence come up in countless articles, often outside of technology-minded publications. For most any topic, a brief search on the web yields dozens of texts suggesting the application of one or the other deep learning model.&lt;/p&gt;
&lt;p&gt;However, tasks such as feature engineering, hyperparameter tuning, or network design, are by no means easy for people without a rich computer science background. Lately, research started to emerge in the area of what is known as Neural Architecture Search (NAS) &lt;span class="citation"&gt;(Baker et al. 2016; Pham et al. 2018; Zoph and Le 2016; Luo et al. 2018; Liu et al. 2017; Real et al. 2018; Jin, Song, and Hu 2018)&lt;/span&gt;. The main goal of NAS algorithms is, given a specific tagged dataset, to search for the most optimal neural network to perform a certain task on that dataset. In this sense, NAS algorithms allow the user to not have to worry about any task related to data science engineering. In other words, given a tagged dataset and a task, e.g., image classification, or text classification among others, the NAS algorithm will train several high-performance deep learning models and return the one that outperforms the rest.&lt;/p&gt;
&lt;p&gt;Several NAS algorithms were developed on different platforms (e.g. &lt;a href="https://cloud.google.com/automl/"&gt;Google Cloud AutoML&lt;/a&gt;), or as libraries of certain programming languages (e.g. &lt;a href="https://autokeras.com/"&gt;Auto-Keras&lt;/a&gt;, &lt;a href="https://epistasislab.github.io/tpot/"&gt;TPOT&lt;/a&gt;, &lt;a href="https://www.automl.org/automl/auto-sklearn/"&gt;Auto-Sklearn&lt;/a&gt;). However, for a language that brings together experts from such diverse disciplines as is the R programming language, to the best of our knowledge, there is no NAS tool to this day. In this post, we present the Auto-Keras R package, an interface from R to the &lt;a href="https://autokeras.com/"&gt;Auto-Keras Python library&lt;/a&gt; &lt;span class="citation"&gt;(Jin, Song, and Hu 2018)&lt;/span&gt;. Thanks to the use of Auto-Keras, R programmers with few lines of code will be able to train several deep learning models for their data and get the one that outperforms the others.&lt;/p&gt;
&lt;p&gt;Let’s dive into Auto-Keras!&lt;/p&gt;
&lt;h2 id="auto-keras"&gt;Auto-Keras&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the Python Auto-Keras library is only compatible with Python 3.6. So make sure this version is currently installed, and correctly set to be used by the &lt;a href="https://rstudio.github.io/reticulate/"&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; R library.&lt;/p&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;
&lt;p&gt;To begin, install the autokeras R package from GitHub as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;if (!require(&amp;quot;remotes&amp;quot;)) {
  install.packages(&amp;quot;remotes&amp;quot;)
}
remotes::install_github(&amp;quot;jcrodriguez1989/autokeras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Auto-Keras R interface uses the Keras and TensorFlow backend engines by default. To install both the core Auto-Keras library as well as the Keras and TensorFlow backends use the &lt;code&gt;install_autokeras()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;autokeras&amp;quot;)
install_autokeras()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for &lt;a href="https://keras.rstudio.com/reference/install_keras.html"&gt;&lt;code&gt;install_keras()&lt;/code&gt;&lt;/a&gt; from the &lt;code&gt;keras&lt;/code&gt; R library.&lt;/p&gt;
&lt;h3 id="mnist-example"&gt;MNIST Example&lt;/h3&gt;
&lt;p&gt;We can learn the basics of Auto-Keras by walking through a simple example: recognizing handwritten digits from the &lt;a href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-16-autokeras/images/mnist_ggplotted_num.png" /&gt;&lt;/p&gt;
&lt;p&gt;The dataset also includes labels for each image, telling us which digit it is. For example, the label for the above image is 2.&lt;/p&gt;
&lt;h4 id="loading-the-data"&gt;Loading the Data&lt;/h4&gt;
&lt;p&gt;The MNIST dataset is included with Keras and can be accessed using the &lt;a href="https://keras.rstudio.com/reference/index.html#section-datasets"&gt;&lt;code&gt;dataset_mnist()&lt;/code&gt;&lt;/a&gt; function from the &lt;code&gt;keras&lt;/code&gt; R library. Here we load the dataset, and then create variables for our test and training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;keras&amp;quot;)
mnist &amp;lt;- dataset_mnist() # load mnist dataset
c(x_train, y_train) %&amp;lt;-% mnist$train # get train
c(x_test, y_test) %&amp;lt;-% mnist$test # and test data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; data is a 3-d array &lt;code&gt;(images,width,height)&lt;/code&gt; of grayscale integer values ranging between 0 to 255.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x_train[1, 14:20, 14:20] # show some pixels from the first image&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]  241  225  160  108    1    0    0
[2,]   81  240  253  253  119   25    0
[3,]    0   45  186  253  253  150   27
[4,]    0    0   16   93  252  253  187
[5,]    0    0    0    0  249  253  249
[6,]    0   46  130  183  253  253  207
[7,]  148  229  253  253  253  250  182&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;y&lt;/code&gt; data is an integer vector with values ranging from 0 to 9.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_imgs &amp;lt;- 8
head(y_train, n = n_imgs) # show first 8 labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5 0 4 1 9 2 1 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of these images can be plotted in R:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;tidyr&amp;quot;)
# get each of the first n_imgs from the x_train dataset and
# convert them to wide format
mnist_to_plot &amp;lt;-
  do.call(rbind, lapply(seq_len(n_imgs), function(i) {
    samp_img &amp;lt;- x_train[i, , ] %&amp;gt;%
      as.data.frame()
    colnames(samp_img) &amp;lt;- seq_len(ncol(samp_img))
    data.frame(
      img = i,
      gather(samp_img, &amp;quot;x&amp;quot;, &amp;quot;value&amp;quot;, convert = TRUE),
      y = seq_len(nrow(samp_img))
    )
  }))
ggplot(mnist_to_plot, aes(x = x, y = y, fill = value)) + geom_tile() +
  scale_fill_gradient(low = &amp;quot;black&amp;quot;, high = &amp;quot;white&amp;quot;, na.value = NA) +
  scale_y_reverse() + theme_minimal() + theme(panel.grid = element_blank()) +
  theme(aspect.ratio = 1) + xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) + facet_wrap(~img, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-16-autokeras/images/some_mnist_nums.png" /&gt;&lt;/p&gt;
&lt;h4 id="data-ready-lets-get-the-model"&gt;Data ready, let’s get the model!&lt;/h4&gt;
&lt;p&gt;Data pre-processing? Model definition? Metrics, epochs definition, anyone? No, none of them are required by Auto-Keras. For image classification tasks, it is enough for Auto-Keras to be passed the &lt;code&gt;x_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; objects as defined above.&lt;/p&gt;
&lt;p&gt;So, to train several deep learning models for two hours, it is enough to run:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# train an Image Classifier for two hours
clf &amp;lt;- model_image_classifier(verbose = TRUE) %&amp;gt;%
  fit(x_train, y_train, time_limit = 2 * 60 * 60)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Saving Directory: /tmp/autokeras_ZOG76O
Preprocessing the images.
Preprocessing finished.

Initializing search.
Initialization finished.


+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+

No loss decrease after 5 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |  0.19463148526847363   |   0.9843999999999999   |
+--------------------------------------------------------------------------+


+----------------------------------------------+
|               Training model 1               |
+----------------------------------------------+

No loss decrease after 5 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           1            |   0.210642946138978    |         0.984          |
+--------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluate it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% evaluate(x_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then just get the best-trained model with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% final_fit(x_train, y_train, x_test, y_test, retrain = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;No loss decrease after 30 epochs.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluate the final model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% evaluate(x_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the model can be saved to take it into production with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% export_autokeras_model(&amp;quot;./myMnistModel.pkl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="conclusions"&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;In this post, the Auto-Keras R package was presented. It was shown that, with almost no deep learning knowledge, it is possible to train models and get the one that returns the best results for the desired task. Here we trained models for two hours. However, we have also tried training for 24 hours, resulting in 15 models being trained, to a final accuracy of 0.9928. Although Auto-Keras will not return a model as efficient as one generated manually by an expert, this new library has its place as an excellent starting point in the world of deep learning. Auto-Keras is an open-source R package, and is freely available in &lt;a href="https://github.com/jcrodriguez1989/autokeras/"&gt;https://github.com/jcrodriguez1989/autokeras/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although the Python Auto-Keras library is currently in a pre-release version and comes with not too many types of training tasks, this is likely to change soon, as the project it was recently added to the &lt;em&gt;keras-team&lt;/em&gt; set of repositories. This will undoubtedly further its progress a lot. So stay tuned, and thanks for reading!&lt;/p&gt;
&lt;h3 id="reproducibility"&gt;Reproducibility&lt;/h3&gt;
&lt;p&gt;To correctly reproduce the results of this post, we recommend using the Auto-Keras docker image by typing:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;docker pull jcrodriguez1989/r-autokeras:0.1.0
docker run -it jcrodriguez1989/r-autokeras:0.1.0 /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-baker2016designing"&gt;
&lt;p&gt;Baker, Bowen, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. “Designing Neural Network Architectures Using Reinforcement Learning.” &lt;em&gt;arXiv Preprint arXiv:1611.02167&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-jin2018efficient"&gt;
&lt;p&gt;Jin, Haifeng, Qingquan Song, and Xia Hu. 2018. “Auto-Keras: An Efficient Neural Architecture Search System.” &lt;em&gt;arXiv Preprint arXiv:1806.10282&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-liu2017hierarchical"&gt;
&lt;p&gt;Liu, Hanxiao, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. “Hierarchical Representations for Efficient Architecture Search.” &lt;em&gt;arXiv Preprint arXiv:1711.00436&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-luo2018neural"&gt;
&lt;p&gt;Luo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. “Neural Architecture Optimization.” In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 7816–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-pham2018efficient"&gt;
&lt;p&gt;Pham, Hieu, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. “Efficient Neural Architecture Search via Parameter Sharing.” &lt;em&gt;arXiv Preprint arXiv:1802.03268&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-real2018regularized"&gt;
&lt;p&gt;Real, Esteban, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2018. “Regularized Evolution for Image Classifier Architecture Search.” &lt;em&gt;arXiv Preprint arXiv:1802.01548&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-zoph2016neural"&gt;
&lt;p&gt;Zoph, Barret, and Quoc V Le. 2016. “Neural Architecture Search with Reinforcement Learning.” &lt;em&gt;arXiv Preprint arXiv:1611.01578&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">25bc177d120925e2c39826f9222308cf</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</guid>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/images/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>lime v0.4: The Kitten Picture Edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Lin Pedersen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I’m happy to report a new major release of &lt;code&gt;lime&lt;/code&gt; has landed on CRAN. &lt;code&gt;lime&lt;/code&gt; is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis. It works by modelling the outcome of the black box in the local neighborhood around the observation to explain and using this local model to explain why (not how) the black box did what it did. For more information about the theory of &lt;code&gt;lime&lt;/code&gt; I will direct you to the article &lt;a href="https://arxiv.org/abs/1602.04938"&gt;introducing the methodology&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-features"&gt;New features&lt;/h2&gt;
&lt;p&gt;The meat of this release centers around two new features that are somewhat linked: Native support for keras models and support for explaining image models.&lt;/p&gt;
&lt;h3 id="keras-and-images"&gt;keras and images&lt;/h3&gt;
&lt;p&gt;J.J. Allaire was kind enough to namedrop &lt;code&gt;lime&lt;/code&gt; during his keynote introduction of the &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt; packages and I felt compelled to support them natively. As keras is by far the most popular way to interface with tensorflow it is first in line for build-in support. The addition of keras means that &lt;code&gt;lime&lt;/code&gt; now directly supports models from the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/topepo/caret"&gt;caret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mlr-org/mlr"&gt;mlr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmlc/xgboost"&gt;xgboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/h2oai/h2o-3"&gt;h2o&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rstudio/keras"&gt;keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re working on something too obscure or cutting edge to not be able to use these packages it is still possible to make your model &lt;code&gt;lime&lt;/code&gt; compliant by providing &lt;code&gt;predict_model()&lt;/code&gt; and &lt;code&gt;model_type()&lt;/code&gt; methods for it.&lt;/p&gt;
&lt;p&gt;keras models are used just like any other model, by passing it into the &lt;code&gt;lime()&lt;/code&gt; function along with the training data in order to create an explainer object. Because we’re soon going to talk about image models, we’ll be using one of the pre-trained ImageNet models that is available from keras itself:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(lime)
library(magick)

model &amp;lt;- application_vgg16(
  weights = &amp;quot;imagenet&amp;quot;,
  include_top = TRUE
)
model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model
______________________________________________________________________________________________
Layer (type)                              Output Shape                         Param #        
==============================================================================================
input_1 (InputLayer)                      (None, 224, 224, 3)                  0              
______________________________________________________________________________________________
block1_conv1 (Conv2D)                     (None, 224, 224, 64)                 1792           
______________________________________________________________________________________________
block1_conv2 (Conv2D)                     (None, 224, 224, 64)                 36928          
______________________________________________________________________________________________
block1_pool (MaxPooling2D)                (None, 112, 112, 64)                 0              
______________________________________________________________________________________________
block2_conv1 (Conv2D)                     (None, 112, 112, 128)                73856          
______________________________________________________________________________________________
block2_conv2 (Conv2D)                     (None, 112, 112, 128)                147584         
______________________________________________________________________________________________
block2_pool (MaxPooling2D)                (None, 56, 56, 128)                  0              
______________________________________________________________________________________________
block3_conv1 (Conv2D)                     (None, 56, 56, 256)                  295168         
______________________________________________________________________________________________
block3_conv2 (Conv2D)                     (None, 56, 56, 256)                  590080         
______________________________________________________________________________________________
block3_conv3 (Conv2D)                     (None, 56, 56, 256)                  590080         
______________________________________________________________________________________________
block3_pool (MaxPooling2D)                (None, 28, 28, 256)                  0              
______________________________________________________________________________________________
block4_conv1 (Conv2D)                     (None, 28, 28, 512)                  1180160        
______________________________________________________________________________________________
block4_conv2 (Conv2D)                     (None, 28, 28, 512)                  2359808        
______________________________________________________________________________________________
block4_conv3 (Conv2D)                     (None, 28, 28, 512)                  2359808        
______________________________________________________________________________________________
block4_pool (MaxPooling2D)                (None, 14, 14, 512)                  0              
______________________________________________________________________________________________
block5_conv1 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_conv2 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_conv3 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_pool (MaxPooling2D)                (None, 7, 7, 512)                    0              
______________________________________________________________________________________________
flatten (Flatten)                         (None, 25088)                        0              
______________________________________________________________________________________________
fc1 (Dense)                               (None, 4096)                         102764544      
______________________________________________________________________________________________
fc2 (Dense)                               (None, 4096)                         16781312       
______________________________________________________________________________________________
predictions (Dense)                       (None, 1000)                         4097000        
==============================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
______________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The vgg16 model is an image classification model that has been build as part of the ImageNet competition where the goal is to classify pictures into 1000 categories with the highest accuracy. As we can see it is fairly complicated.&lt;/p&gt;
&lt;p&gt;In order to create an explainer we will need to pass in the training data as well. For image data the training data is really only used to tell lime that we are dealing with an image model, so any image will suffice. The format for the training data is simply the path to the images, and because the internet runs on kitten pictures we’ll use one of these:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;img &amp;lt;- image_read(&amp;#39;https://www.data-imaginist.com/assets/images/kitten.jpg&amp;#39;)
img_path &amp;lt;- file.path(tempdir(), &amp;#39;kitten.jpg&amp;#39;)
image_write(img, img_path)
plot(as.raster(img))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-2-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As with text models the explainer will need to know how to prepare the input data for the model. For keras models this means formatting the image data as tensors. Thankfully keras comes with a lot of tools for reshaping image data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image_prep &amp;lt;- function(x) {
  arrays &amp;lt;- lapply(x, function(path) {
    img &amp;lt;- image_load(path, target_size = c(224,224))
    x &amp;lt;- image_to_array(img)
    x &amp;lt;- array_reshape(x, c(1, dim(x)))
    x &amp;lt;- imagenet_preprocess_input(x)
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}
explainer &amp;lt;- lime(img_path, model, image_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have an explainer model for understanding how the vgg16 neural network makes its predictions. Before we go along, lets see what the model think of our kitten:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;res &amp;lt;- predict(model, image_prep(img_path))
imagenet_decode_predictions(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
  class_name class_description      score
1  n02124075      Egyptian_cat 0.48913878
2  n02123045             tabby 0.15177219
3  n02123159         tiger_cat 0.10270492
4  n02127052              lynx 0.02638111
5  n03793489             mouse 0.00852214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, it is pretty sure about the whole cat thing. The reason we need to use &lt;code&gt;imagenet_decode_predictions()&lt;/code&gt; is that the output of a keras model is always just a nameless tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dim(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]    1 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;dimnames(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are used to classifiers knowing the class labels, but this is not the case for keras. Motivated by this, &lt;code&gt;lime&lt;/code&gt; now have a way to define/overwrite the class labels of a model, using the &lt;code&gt;as_classifier()&lt;/code&gt; function. Let’s redo our explainer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_labels &amp;lt;- readRDS(system.file(&amp;#39;extdata&amp;#39;, &amp;#39;imagenet_labels.rds&amp;#39;, package = &amp;#39;lime&amp;#39;))
explainer &amp;lt;- lime(img_path, as_classifier(model, model_labels), image_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;There is also an &lt;code&gt;as_regressor()&lt;/code&gt; function which tells &lt;code&gt;lime&lt;/code&gt;, without a doubt, that the model is a regression model. Most models can be introspected to see which type of model they are, but neural networks doesn’t really care. &lt;code&gt;lime&lt;/code&gt; guesses the model type from the activation used in the last layer (linear activation == regression), but if that heuristic fails then &lt;code&gt;as_regressor()&lt;/code&gt;/&lt;code&gt;as_classifier()&lt;/code&gt; can be used.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are now ready to poke into the model and find out what makes it think our image is of an Egyptian cat. But… first I’ll have to talk about yet another concept: superpixels (I promise I’ll get to the explanation part in a bit).&lt;/p&gt;
&lt;p&gt;In order to create meaningful permutations of our image (remember, this is the central idea in &lt;code&gt;lime&lt;/code&gt;), we have to define how to do so. The permutations needs to be substantial enough to have an impact on the image, but not so much that the model completely fails to recognise the content in every case - further, they should lead to an interpretable result. The concept of superpixels lends itself well to these constraints. In short, a superpixel is a patch of an area with high homogeneity, and superpixel segmentation is a clustering of image pixels into a number of superpixels. By segmenting the image to explain into superpixels we can turn area of contextual similarity on and off during the permutations and find out if that area is important. It is still necessary to experiment a bit as the optimal number of superpixels depend on the content of the image. Remember, we need them to be large enough to have an impact but not so large that the class probability becomes effectively binary. &lt;code&gt;lime&lt;/code&gt; comes with a function to assess the superpixel segmentation before beginning the explanation and it is recommended to play with it a bit — with time you’ll likely get a feel for the right values:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# default
plot_superpixels(img_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-7-1.png" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Changing some settings
plot_superpixels(img_path, n_superpixels = 200, weight = 40)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-7-2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The default is set to a pretty low number of superpixels — if the subject of interest is relatively small it may be necessary to increase the number of superpixels so that the full subject does not end up in one, or a few superpixels. The &lt;code&gt;weight&lt;/code&gt; parameter will allow you to make the segments more compact by weighting spatial distance higher than colour distance. For this example we’ll stick with the defaults.&lt;/p&gt;
&lt;p&gt;Be aware that explaining image models is much heavier than tabular or text data. In effect it will create 1000 new images per explanation (default permutation size for images) and run these through the model. As image classification models are often quite heavy, this will result in computation time measured in minutes. The permutation is batched (default to 10 permutations per batch), so you should not be afraid of running out of RAM or hard-drive space.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;explanation &amp;lt;- explain(img_path, explainer, n_labels = 2, n_features = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of an image explanation is a data frame of the same format as that from tabular and text data. Each feature will be a superpixel and the pixel range of the superpixel will be used as its description. Usually the explanation will only make sense in the context of the image itself, so the new version of &lt;code&gt;lime&lt;/code&gt; also comes with a &lt;code&gt;plot_image_explanation()&lt;/code&gt; function to do just that. Let’s see what our explanation have to tell us:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the model, for both the major predicted classes, focuses on the cat, which is nice since they are both different cat breeds. The plot function got a few different functions to help you tweak the visual, and it filters low scoring superpixels away by default. An alternative view that puts more focus on the relevant superpixels, but removes the context can be seen by using &lt;code&gt;display = 'block'&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation, display = &amp;#39;block&amp;#39;, threshold = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-9-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;While not as common with image explanations it is also possible to look at the areas of an image that contradicts the class:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation, threshold = 0, show_negative = TRUE, fill_alpha = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-10-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As each explanation takes longer time to create and needs to be tweaked on a per-image basis, image explanations are not something that you’ll create in large batches as you might do with tabular and text data. Still, a few explanations might allow you to understand your model better and be used for communicating the workings of your model. Further, as the time-limiting factor in image explanations are the image classifier and not lime itself, it is bound to improve as image classifiers becomes more performant.&lt;/p&gt;
&lt;h3 id="grab-back"&gt;Grab back&lt;/h3&gt;
&lt;p&gt;Apart from keras and image support, a slew of other features and improvements have been added. Here’s a quick overview:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All explanation plots now include the fit of the ridge regression used to make the explanation. This makes it easy to assess how good the assumptions about local linearity are kept.&lt;/li&gt;
&lt;li&gt;When explaining tabular data the default distance measure is now &lt;code&gt;'gower'&lt;/code&gt; from the &lt;code&gt;gower&lt;/code&gt; package. &lt;code&gt;gower&lt;/code&gt; makes it possible to measure distances between heterogeneous data without converting all features to numeric and experimenting with different exponential kernels.&lt;/li&gt;
&lt;li&gt;When explaining tabular data numerical features will no longer be sampled from a normal distribution during permutations, but from a kernel density defined by the training data. This should ensure that the permutations are more representative of the expected input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;This release represents an important milestone for &lt;code&gt;lime&lt;/code&gt; in R. With the addition of image explanations the &lt;code&gt;lime&lt;/code&gt; package is now on par or above its Python relative, feature-wise. Further development will focus on improving the performance of the model, e.g. by adding parallelisation or improving the local model definition, as well as exploring alternative explanation types such as &lt;a href="https://homes.cs.washington.edu/%7Emarcotcr/aaai18.pdf"&gt;anchor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy Explaining!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">6964f7338edb1b3c8129d1acdc1c2dba</distill:md5>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <category>Explainability</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</guid>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1344" height="672"/>
    </item>
    <item>
      <title>R Interface to Google CloudML</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</link>
      <description>


&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;We are excited to announce the availability of the &lt;a href="https://tensorflow.rstudio.com/tools/cloudml"&gt;cloudml&lt;/a&gt; package, which provides an R interface to &lt;a href="https://cloud.google.com/ml-engine"&gt;Google Cloud Machine Learning Engine&lt;/a&gt;. CloudML provides a number of services including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Scalable training of models built with the &lt;a href="https://keras.rstudio.com/"&gt;keras&lt;/a&gt;, &lt;a href="https://tensorflow.rstudio.com/tfestimators"&gt;tfestimators&lt;/a&gt;, and &lt;a href="https://tensorflow.rstudio.com/"&gt;tensorflow&lt;/a&gt; R packages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On-demand access to training on GPUs, including the new &lt;a href="http://www.nvidia.com/object/tesla-p100.html"&gt;Tesla P100 GPUs&lt;/a&gt; from NVIDIA®.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deployment of trained models to the Google global prediction platform that can support thousands of users and TBs of data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="training-with-cloudml"&gt;Training with CloudML&lt;/h2&gt;
&lt;p&gt;Once you’ve configured your system to publish to CloudML, training a model is as straightforward as calling the &lt;code&gt;cloudml_train()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(cloudml)
cloudml_train(&amp;quot;train.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;CloudML provides a variety of GPU configurations, which can be easily selected when calling &lt;code&gt;cloudml_train()&lt;/code&gt;. For example, the following would train the same model as above but with a &lt;a href="http://www.nvidia.com/object/tesla-k80.html"&gt;Tesla K80 GPU&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cloudml_train(&amp;quot;train.R&amp;quot;, master_type = &amp;quot;standard_gpu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To train using a &lt;a href="http://www.nvidia.com/object/tesla-p100.html"&gt;Tesla P100 GPU&lt;/a&gt; you would specify &lt;code&gt;"standard_p100"&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cloudml_train(&amp;quot;train.R&amp;quot;, master_type = &amp;quot;standard_p100&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When training completes the job is collected and a training run report is displayed:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-10-r-interface-to-cloudml/images/training-run.png" style="border: 1px solid rgba(0,0,0,0.1);" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id="learning-more"&gt;Learning More&lt;/h2&gt;
&lt;p&gt;Check out the &lt;a href="https://tensorflow.rstudio.com/tools/cloudml"&gt;cloudml package documentation&lt;/a&gt; to get started with training and deploying models on CloudML.&lt;/p&gt;
&lt;p&gt;You can also find out more about the various capabilities of CloudML in these articles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/training.html"&gt;Training with CloudML&lt;/a&gt; goes into additional depth on managing training jobs and their output.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html"&gt;Hyperparameter Tuning&lt;/a&gt; explores how you can improve the performance of your models by running many trials with distinct hyperparameters (e.g. number and size of layers) to determine their optimal values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/storage.html"&gt;Google Cloud Storage&lt;/a&gt; provides information on copying data between your local machine and Google Storage and also describes how to use data within Google Storage during training.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/deployment.html"&gt;Deploying Models&lt;/a&gt; describes how to deploy trained models and generate predictions from them.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">b2016385f01a59940596cbd4b0d7d63d</distill:md5>
      <category>Cloud</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</guid>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png" medium="image" type="image/png" width="394" height="211"/>
    </item>
    <item>
      <title>tfruns: Tools for TensorFlow Training Runs</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</link>
      <description>


&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html"&gt;tfruns package&lt;/a&gt; provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R. Use the tfruns package to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Track the hyperparameters, metrics, output, and source code of every training run.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare hyperparmaeters and metrics across runs to find the best performing model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Automatically generate reports to visualize individual training runs or comparisons between runs. &lt;br/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can install the &lt;strong&gt;tfruns&lt;/strong&gt; package from GitHub as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/tfruns&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Complete documentation for tfruns is available on the &lt;a href="https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html"&gt;TensorFlow for R website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tfruns&lt;/strong&gt; is intended to be used with the &lt;a href="https://tensorflow.rstudio.com/keras"&gt;keras&lt;/a&gt; and/or the &lt;a href="https://tensorflow.rstudio.com/keras"&gt;tfestimators&lt;/a&gt; packages, both of which provide higher level interfaces to TensorFlow from R. These packages can be installed with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# keras
install.packages(&amp;quot;keras&amp;quot;)

# tfestimators
devtools::install_github(&amp;quot;rstudio/tfestimators&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;In the following sections we’ll describe the various capabilities of &lt;strong&gt;tfruns&lt;/strong&gt;. Our example training script (&lt;a href="https://github.com/rstudio/tfruns/blob/master/inst/examples/mnist_mlp/mnist_mlp.R"&gt;mnist_mlp.R&lt;/a&gt;) trains a Keras model to recognize MNIST digits.&lt;/p&gt;
&lt;p&gt;To train a model with &lt;strong&gt;tfruns&lt;/strong&gt;, just use the &lt;code&gt;training_run()&lt;/code&gt; function in place of the &lt;code&gt;source()&lt;/code&gt; function to execute your R script. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfruns)
training_run(&amp;quot;mnist_mlp.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When training is completed, a summary of the run will automatically be displayed if you are within an interactive R session:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/view_run.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;The metrics and output of each run are automatically captured within a &lt;em&gt;run directory&lt;/em&gt; which is unique for each run that you initiate. Note that for Keras and TF Estimator models this data is captured automatically (no changes to your source code are required).&lt;/p&gt;
&lt;p&gt;You can call the &lt;code&gt;latest_run()&lt;/code&gt; function to view the results of the last run (including the path to the run directory which stores all of the run’s output):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latest_run()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$ run_dir           : chr &amp;quot;runs/2017-10-02T14-23-38Z&amp;quot;
$ eval_loss         : num 0.0956
$ eval_acc          : num 0.98
$ metric_loss       : num 0.0624
$ metric_acc        : num 0.984
$ metric_val_loss   : num 0.0962
$ metric_val_acc    : num 0.98
$ flag_dropout1     : num 0.4
$ flag_dropout2     : num 0.3
$ samples           : int 48000
$ validation_samples: int 12000
$ batch_size        : int 128
$ epochs            : int 20
$ epochs_completed  : int 20
$ metrics           : chr &amp;quot;(metrics data frame)&amp;quot;
$ model             : chr &amp;quot;(model summary)&amp;quot;
$ loss_function     : chr &amp;quot;categorical_crossentropy&amp;quot;
$ optimizer         : chr &amp;quot;RMSprop&amp;quot;
$ learning_rate     : num 0.001
$ script            : chr &amp;quot;mnist_mlp.R&amp;quot;
$ start             : POSIXct[1:1], format: &amp;quot;2017-10-02 14:23:38&amp;quot;
$ end               : POSIXct[1:1], format: &amp;quot;2017-10-02 14:24:24&amp;quot;
$ completed         : logi TRUE
$ output            : chr &amp;quot;(script ouptut)&amp;quot;
$ source_code       : chr &amp;quot;(source archive)&amp;quot;
$ context           : chr &amp;quot;local&amp;quot;
$ type              : chr &amp;quot;training&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The run directory used in the example above is “runs/2017-10-02T14-23-38Z”. Run directories are by default generated within the “runs” subdirectory of the current working directory, and use a timestamp as the name of the run directory. You can view the report for any given run using the &lt;code&gt;view_run()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;view_run(&amp;quot;runs/2017-10-02T14-23-38Z&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="comparing-runs"&gt;Comparing Runs&lt;/h2&gt;
&lt;p&gt;Let’s make a couple of changes to our training script to see if we can improve model performance. We’ll change the number of units in our first dense layer to 128, change the &lt;code&gt;learning_rate&lt;/code&gt; from 0.001 to 0.003 and run 30 rather than 20 &lt;code&gt;epochs&lt;/code&gt;. After making these changes to the source code we re-run the script using &lt;code&gt;training_run()&lt;/code&gt; as before:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_run(&amp;quot;mnist_mlp.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will also show us a report summarizing the results of the run, but what we are really interested in is a comparison between this run and the previous one. We can view a comparison via the &lt;code&gt;compare_runs()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compare_runs()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/compare_runs.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;The comparison report shows the model attributes and metrics side-by-side, as well as differences in the source code and output of the training script.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;compare_runs()&lt;/code&gt; will by default compare the last two runs, however you can pass any two run directories you like to be compared.&lt;/p&gt;
&lt;h2 id="using-flags"&gt;Using Flags&lt;/h2&gt;
&lt;p&gt;Tuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not by changing the source code of the training script as we did above, but instead by defining flags for key parameters you may want to vary. In the example script you can see that we have done this for the &lt;code&gt;dropout&lt;/code&gt; layers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;FLAGS &amp;lt;- flags(
  flag_numeric(&amp;quot;dropout1&amp;quot;, 0.4),
  flag_numeric(&amp;quot;dropout2&amp;quot;, 0.3)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These flags are then used in the definition of our model here:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;%
  layer_dense(units = 128, activation = &amp;#39;relu&amp;#39;, input_shape = c(784)) %&amp;gt;%
  layer_dropout(rate = FLAGS$dropout1) %&amp;gt;%
  layer_dense(units = 128, activation = &amp;#39;relu&amp;#39;) %&amp;gt;%
  layer_dropout(rate = FLAGS$dropout2) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;#39;softmax&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’ve defined flags, we can pass alternate flag values to &lt;code&gt;training_run()&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_run(&amp;#39;mnist_mlp.R&amp;#39;, flags = c(dropout1 = 0.2, dropout2 = 0.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You aren’t required to specify all of the flags (any flags excluded will simply use their default value).&lt;/p&gt;
&lt;p&gt;Flags make it very straightforward to systematically explore the impact of changes to hyperparameters on model performance, for example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (dropout1 in c(0.1, 0.2, 0.3))
  training_run(&amp;#39;mnist_mlp.R&amp;#39;, flags = c(dropout1 = dropout1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Flag values are automatically included in run data with a “flag_” prefix (e.g. &lt;code&gt;flag_dropout1&lt;/code&gt;, &lt;code&gt;flag_dropout2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;See the article on &lt;a href="https://tensorflow.rstudio.com/tools/training_flags.html"&gt;training flags&lt;/a&gt; for additional documentation on using flags.&lt;/p&gt;
&lt;h2 id="analyzing-runs"&gt;Analyzing Runs&lt;/h2&gt;
&lt;p&gt;We’ve demonstrated visualizing and comparing one or two runs, however as you accumulate more runs you’ll generally want to analyze and compare runs many runs. You can use the &lt;code&gt;ls_runs()&lt;/code&gt; function to yield a data frame with summary information on all of the runs you’ve conducted within a given directory:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ls_runs()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 27
                    run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss
                      &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
1 runs/2017-10-02T14-56-57Z    0.1263   0.9784      0.0773     0.9807          0.1283
2 runs/2017-10-02T14-56-04Z    0.1323   0.9783      0.0545     0.9860          0.1414
3 runs/2017-10-02T14-55-11Z    0.1407   0.9804      0.0348     0.9914          0.1542
4 runs/2017-10-02T14-51-44Z    0.1164   0.9801      0.0448     0.9882          0.1396
5 runs/2017-10-02T14-37-00Z    0.1338   0.9750      0.1097     0.9732          0.1328
6 runs/2017-10-02T14-23-38Z    0.0956   0.9796      0.0624     0.9835          0.0962
# ... with 21 more variables: metric_val_acc &amp;lt;dbl&amp;gt;, flag_dropout1 &amp;lt;dbl&amp;gt;,
#   flag_dropout2 &amp;lt;dbl&amp;gt;, samples &amp;lt;int&amp;gt;, validation_samples &amp;lt;int&amp;gt;, batch_size &amp;lt;int&amp;gt;,
#   epochs &amp;lt;int&amp;gt;, epochs_completed &amp;lt;int&amp;gt;, metrics &amp;lt;chr&amp;gt;, model &amp;lt;chr&amp;gt;, loss_function &amp;lt;chr&amp;gt;,
#   optimizer &amp;lt;chr&amp;gt;, learning_rate &amp;lt;dbl&amp;gt;, script &amp;lt;chr&amp;gt;, start &amp;lt;dttm&amp;gt;, end &amp;lt;dttm&amp;gt;,
#   completed &amp;lt;lgl&amp;gt;, output &amp;lt;chr&amp;gt;, source_code &amp;lt;chr&amp;gt;, context &amp;lt;chr&amp;gt;, type &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;ls_runs()&lt;/code&gt; returns a data frame you can also render a sortable, filterable version of it within RStudio using the &lt;code&gt;View()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;View(ls_runs())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/ls_runs_rstudio.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ls_runs()&lt;/code&gt; function also supports &lt;code&gt;subset&lt;/code&gt; and &lt;code&gt;order&lt;/code&gt; arguments. For example, the following will yield all runs with an eval accuracy better than 0.98:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ls_runs(eval_acc &amp;gt; 0.98, order = eval_acc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can pass the results of &lt;code&gt;ls_runs()&lt;/code&gt; to compare runs (which will always compare the first two runs passed). For example, this will compare the two runs that performed best in terms of evaluation accuracy:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compare_runs(ls_runs(eval_acc &amp;gt; 0.98, order = eval_acc))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/ls_runs_compare.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;h2 id="rstudio-ide"&gt;RStudio IDE&lt;/h2&gt;
&lt;p&gt;If you use RStudio with &lt;strong&gt;tfruns&lt;/strong&gt;, it’s strongly recommended that you update to the current &lt;a href="https://www.rstudio.com/products/rstudio/download/preview/"&gt;Preview Release&lt;/a&gt; of RStudio v1.1, as there are are a number of points of integration with the IDE that require this newer release.&lt;/p&gt;
&lt;h3 id="addin"&gt;Addin&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;tfruns&lt;/strong&gt; package installs an RStudio IDE addin which provides quick access to frequently used functions from the Addins menu:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/rstudio_addin.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;Note that you can use &lt;strong&gt;Tools&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Modify Keyboard Shortcuts&lt;/strong&gt; within RStudio to assign a keyboard shortcut to one or more of the addin commands.&lt;/p&gt;
&lt;h3 id="background-training"&gt;Background Training&lt;/h3&gt;
&lt;p&gt;RStudio v1.1 includes a Terminal pane alongside the Console pane. Since training runs can become quite lengthy, it’s often useful to run them in the background in order to keep the R console free for other work. You can do this from a Terminal as follows:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/rstudio_terminal.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;If you are not running within RStudio then you can of course use a system terminal window for background training.&lt;/p&gt;
&lt;h3 id="publishing-reports"&gt;Publishing Reports&lt;/h3&gt;
&lt;p&gt;Training run views and comparisons are HTML documents which can be saved and shared with others. When viewing a report within RStudio v1.1 you can save a copy of the report or publish it to RPubs or RStudio Connect:&lt;/p&gt;
&lt;p&gt;&lt;kbd&gt;&lt;img src="https://tensorflow.rstudio.com/tools/tfruns/articles/images/rstudio_publish.png" style="border: 1px solid #CCCCCC;" width=675/&gt;&lt;/kbd&gt;&lt;/p&gt;
&lt;p&gt;If you are not running within RStudio then you can use the &lt;code&gt;save_run_view()&lt;/code&gt; and &lt;code&gt;save_run_comparison()&lt;/code&gt; functions to create standalone HTML versions of run reports.&lt;/p&gt;
&lt;h2 id="managing-runs"&gt;Managing Runs&lt;/h2&gt;
&lt;p&gt;There are a variety of tools available for managing training run output, including:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Exporting run artifacts (e.g. saved models).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copying and purging run directories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using a custom run directory for an experiment or other set of related runs.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/tools/tfruns/articles/managing.html"&gt;Managing Runs&lt;/a&gt; article provides additional details on using these features.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">8773f7d6d470ea150396dea3b60603b2</distill:md5>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</guid>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns/preview.png" medium="image" type="image/png" width="2006" height="1116"/>
    </item>
    <item>
      <title>Keras for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</link>
      <description>


&lt;p&gt;We are excited to announce that the &lt;a href="https://tensorflow.rstudio.com/keras"&gt;keras package&lt;/a&gt; is now available on CRAN. The package provides an R interface to &lt;a href="https://keras.io"&gt;Keras&lt;/a&gt;, a high-level neural networks API developed with a focus on enabling fast experimentation. Keras has the following key features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Allows the same code to run on CPU or on GPU, seamlessly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;User-friendly API which makes it easy to quickly prototype deep learning models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is capable of running on top of multiple back-ends including &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://github.com/Microsoft/cntk"&gt;CNTK&lt;/a&gt;, or &lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are already familiar with Keras and want to jump right in, check out &lt;a href="https://tensorflow.rstudio.com/keras" class="uri"&gt;https://tensorflow.rstudio.com/keras&lt;/a&gt; which has everything you need to get started including over 20 complete examples to learn from.&lt;/p&gt;
&lt;p&gt;To learn a bit more about Keras and why we’re so excited to announce the Keras interface for R, read on!&lt;/p&gt;
&lt;h2 id="keras-and-deep-learning"&gt;Keras and Deep Learning&lt;/h2&gt;
&lt;p&gt;Interest in deep learning has been accelerating rapidly over the past few years, and several deep learning frameworks have emerged over the same time frame. Of all the available frameworks, Keras has stood out for its productivity, flexibility and user-friendly API. At the same time, TensorFlow has emerged as a next-generation machine learning platform that is both extremely flexible and well-suited to production deployment.&lt;/p&gt;
&lt;p&gt;Not surprisingly, Keras and TensorFlow have of late been pulling away from other deep learning frameworks:&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;
&lt;p lang="en" dir="ltr"&gt;
Google web search interest around deep learning frameworks over time. If you remember Q4 2015 and Q1-2 2016 as confusing, you weren't alone. &lt;a href="https://t.co/1f1VQVGr8n"&gt;pic.twitter.com/1f1VQVGr8n&lt;/a&gt;
&lt;/p&gt;
— François Chollet (&lt;span class="citation"&gt;@fchollet&lt;/span&gt;) &lt;a href="https://twitter.com/fchollet/status/871089784898310144"&gt;June 3, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;p&gt;The good news about Keras and TensorFlow is that you don’t need to choose between them! The default backend for Keras is TensorFlow and Keras can be &lt;a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"&gt;integrated seamlessly&lt;/a&gt; with TensorFlow workflows. There is also a pure-TensorFlow implementation of Keras with &lt;a href="https://www.youtube.com/watch?v=UeheTiBJ0Io&amp;amp;t=7s&amp;amp;index=8&amp;amp;list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv"&gt;deeper integration&lt;/a&gt; on the roadmap for later this year.&lt;/p&gt;
&lt;p&gt;Keras and TensorFlow are the state of the art in deep learning tools and with the keras package you can now access both with a fluent R interface.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting Started&lt;/h2&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;
&lt;p&gt;To begin, install the keras R package from CRAN as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Keras R interface uses the &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; backend engine by default. To install both the core Keras library as well as the TensorFlow backend use the &lt;code&gt;install_keras()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
install_keras()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for &lt;a href="https://tensorflow.rstudio.com/keras/reference/install_keras.html"&gt;&lt;code&gt;install_keras()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="mnist-example"&gt;MNIST Example&lt;/h3&gt;
&lt;p&gt;We can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the &lt;a href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:&lt;/p&gt;
&lt;p&gt;&lt;img style="width: 50%;" src="https://www.tensorflow.org/images/MNIST.png"&gt;&lt;/p&gt;
&lt;p&gt;The dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.&lt;/p&gt;
&lt;h4 id="preparing-the-data"&gt;Preparing the Data&lt;/h4&gt;
&lt;p&gt;The MNIST dataset is included with Keras and can be accessed using the &lt;code&gt;dataset_mnist()&lt;/code&gt; function. Here we load the dataset then create variables for our test and training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
mnist &amp;lt;- dataset_mnist()
x_train &amp;lt;- mnist$train$x
y_train &amp;lt;- mnist$train$y
x_test &amp;lt;- mnist$test$x
y_test &amp;lt;- mnist$test$y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; data is a 3-d array &lt;code&gt;(images,width,height)&lt;/code&gt; of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# reshape
dim(x_train) &amp;lt;- c(nrow(x_train), 784)
dim(x_test) &amp;lt;- c(nrow(x_test), 784)
# rescale
x_train &amp;lt;- x_train / 255
x_test &amp;lt;- x_test / 255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;y&lt;/code&gt; data is an integer vector with values ranging from 0 to 9. To prepare this data for training we &lt;a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"&gt;one-hot encode&lt;/a&gt; the vectors into binary class matrices using the Keras &lt;code&gt;to_categorical()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- to_categorical(y_train, 10)
y_test &amp;lt;- to_categorical(y_test, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="defining-the-model"&gt;Defining the Model&lt;/h4&gt;
&lt;p&gt;The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the &lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;sequential model&lt;/a&gt;, a linear stack of layers.&lt;/p&gt;
&lt;p&gt;We begin by creating a sequential model and then adding layers using the pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) operator:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() 
model %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;, input_shape = c(784)) %&amp;gt;% 
  layer_dropout(rate = 0.4) %&amp;gt;% 
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;input_shape&lt;/code&gt; argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax activation function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;summary()&lt;/code&gt; function to print the details of the model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre style="box-shadow: none;"&gt;&lt;code&gt;Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
dense_1 (Dense)                     (None, 256)                     200960      
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 256)                     0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 128)                     32896       
________________________________________________________________________________
dropout_2 (Dropout)                 (None, 128)                     0           
________________________________________________________________________________
dense_3 (Dense)                     (None, 10)                      1290        
================================================================================
Total params: 235,146
Trainable params: 235,146
Non-trainable params: 0
________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, compile the model with appropriate loss function, optimizer, and metrics:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = &amp;quot;categorical_crossentropy&amp;quot;,
  optimizer = optimizer_rmsprop(),
  metrics = c(&amp;quot;accuracy&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training-and-evaluation"&gt;Training and Evaluation&lt;/h4&gt;
&lt;p&gt;Use the &lt;code&gt;fit()&lt;/code&gt; function to train the model for 30 epochs using batches of 128 images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;history &amp;lt;- model %&amp;gt;% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;history&lt;/code&gt; object returned by &lt;code&gt;fit()&lt;/code&gt; includes loss and accuracy metrics which we can plot:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://keras.rstudio.com/images/training_history_ggplot2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Evaluate the model’s performance on the test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% evaluate(x_test, y_test,verbose = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$loss
[1] 0.1149

$acc
[1] 0.9807&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generate predictions on new data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% predict_classes(x_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2
 [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2
 [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9
 [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 9900 entries ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;Guide to the Sequential Model&lt;/a&gt; article describes the basics of Keras sequential models in more depth.&lt;/p&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;p&gt;Over 20 complete examples are available (special thanks to &lt;span class="citation"&gt;[@dfalbel]&lt;/span&gt;(&lt;a href="https://github.com/dfalbel" class="uri"&gt;https://github.com/dfalbel&lt;/a&gt;) for his work on these!). The examples cover image classification, text generation with stacked LSTMs, question-answering with memory networks, transfer learning, variational encoding, and more.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="46%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/addition_rnn.html"&gt;addition_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Implementation of sequence to sequence learning for performing addition of two numbers (as strings).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/babi_memnn.html"&gt;babi_memnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a memory network on the bAbI dataset for reading comprehension.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/babi_rnn.html"&gt;babi_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a two-branch recurrent network on the bAbI dataset for reading comprehension.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/cifar10_cnn.html"&gt;cifar10_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple deep CNN on the CIFAR10 small images dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/conv_lstm.html"&gt;conv_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates the use of a convolutional LSTM network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/deep_dream.html"&gt;deep_dream&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep Dreams in Keras.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_bidirectional_lstm.html"&gt;imdb_bidirectional_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a Bidirectional LSTM on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_cnn.html"&gt;imdb_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates the use of Convolution1D for text classification.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_cnn_lstm.html"&gt;imdb_cnn_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_fasttext.html"&gt;imdb_fasttext&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a FastText model on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_lstm.html"&gt;imdb_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a LSTM on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/lstm_text_generation.html"&gt;lstm_text_generation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Generates text from Nietzsche’s writings.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_acgan.html"&gt;mnist_acgan&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_antirectifier.html"&gt;mnist_antirectifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to write custom layers for Keras&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_cnn.html"&gt;mnist_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple convnet on the MNIST dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_irnn.html"&gt;mnist_irnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_mlp.html"&gt;mnist_mlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple deep multi-layer perceptron on the MNIST dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_hierarchical_rnn.html"&gt;mnist_hierarchical_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a Hierarchical RNN (HRNN) to classify MNIST digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_transfer_cnn.html"&gt;mnist_transfer_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transfer learning toy example.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/neural_style_transfer.html"&gt;neural_style_transfer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/reuters_mlp.html"&gt;reuters_mlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains and evaluates a simple MLP on the Reuters newswire topic classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/stateful_lstm.html"&gt;stateful_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to use stateful RNNs to model long sequences efficiently.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/variational_autoencoder.html"&gt;variational_autoencoder&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to build a variational autoencoder.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/variational_autoencoder_deconv.html"&gt;variational_autoencoder_deconv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to build a variational autoencoder with Keras using deconvolution layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="learning-more"&gt;Learning More&lt;/h2&gt;
&lt;p&gt;After you’ve become familiar with the basics, these articles are a good next step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;Guide to the Sequential Model&lt;/a&gt;. The sequential model is a linear stack of layers and is the API most users should start with.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/functional_api.html"&gt;Guide to the Functional API&lt;/a&gt;. The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/training_visualization.html"&gt;Training Visualization&lt;/a&gt;. There are a wide variety of tools available for visualizing training. These include plotting of training metrics, real time display of metrics within the RStudio IDE, and integration with the TensorBoard visualization tool included with TensorFlow.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/applications.html"&gt;Using Pre-Trained Models&lt;/a&gt;. Keras includes a number of deep learning models (Xception, VGG16, VGG19, ResNet50, InceptionVV3, and MobileNet) that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/faq.html"&gt;Frequently Asked Questions&lt;/a&gt;. Covers many additional topics including streaming training data, saving models, training on GPUs, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keras provides a productive, highly flexible framework for developing deep learning models. We can’t wait to see what the R community will do with these tools!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4005c12c2397234717b35c6a74ab1567</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</guid>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r/preview.png" medium="image" type="image/png" width="669" height="414"/>
    </item>
    <item>
      <title>TensorFlow Estimators</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yuan Tang</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</link>
      <description>


&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/tfestimators"&gt;tfestimators package&lt;/a&gt; is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.&lt;/p&gt;
&lt;p&gt;More models are coming soon such as state saving recurrent neural networks, dynamic recurrent neural networks, support vector machines, random forest, KMeans clustering, etc. TensorFlow estimators also provides a flexible framework for defining arbitrary new model types as custom estimators.&lt;/p&gt;
&lt;p&gt;The framework balances the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures.&lt;/p&gt;
&lt;p&gt;These abstractions guide developers to write models in ways conducive to productionization as well as making it possible to write downstream infrastructure for distributed training or parameter tuning independent of the model implementation.&lt;/p&gt;
&lt;p&gt;To make out of the box models flexible and usable across a wide range of problems, &lt;strong&gt;tfestimators&lt;/strong&gt; provides canned Estimators that are are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data.&lt;/p&gt;
&lt;p&gt;For more details on the architecture and design of TensorFlow Estimators, please check out the &lt;a href="http://www.kdd.org/kdd2017/"&gt;KDD’17&lt;/a&gt; paper: &lt;a href="http://terrytangyuan.github.io/data/papers/tf-estimators-kdd-paper.pdf"&gt;TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="quick-start"&gt;Quick Start&lt;/h2&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;
&lt;p&gt;To use &lt;strong&gt;tfestimators&lt;/strong&gt;, you need to install both the &lt;strong&gt;tfestimators&lt;/strong&gt; R package as well as &lt;a href="https://rstudio.github.io/tensorflow/"&gt;TensorFlow&lt;/a&gt; itself.&lt;/p&gt;
&lt;p&gt;First, install the tfestimators R package as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/tfestimators&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, use the &lt;code&gt;install_tensorflow()&lt;/code&gt; function to install TensorFlow (note that the current tfestimators package requires version 1.3.0 of TensorFlow so even if you already have TensorFlow installed you should update if you are running a previous version):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfestimators)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will provide you with a default installation of TensorFlow suitable for getting started. See the &lt;a href="https://tensorflow.rstudio.com/installation.html"&gt;article on installation&lt;/a&gt; to learn about more advanced options, including installing a version of TensorFlow that takes advantage of NVIDIA GPUs if you have the correct CUDA libraries installed.&lt;/p&gt;
&lt;h3 id="linear-regression"&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;Let’s create a simple linear regression model with the &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html"&gt;mtcars dataset&lt;/a&gt; to demonstrate the use of estimators. We’ll illustrate how &lt;strong&gt;input functions&lt;/strong&gt; can be constructed and used to feed data to an estimator, how &lt;strong&gt;feature columns&lt;/strong&gt; can be used to specify a set of transformations to apply to input data, and how these pieces come together in the Estimator interface.&lt;/p&gt;
&lt;h4 id="input-function"&gt;Input Function&lt;/h4&gt;
&lt;p&gt;Estimators can receive data through input functions. Input functions take an arbitrary data source (in-memory data sets, streaming data, custom data format, and so on) and generate Tensors that can be supplied to TensorFlow models. The &lt;strong&gt;tfestimators&lt;/strong&gt; package includes an &lt;code&gt;input_fn()&lt;/code&gt; function that can create TensorFlow input functions from common R data sources (e.g. data frames and matrices). It’s also possible to write a fully custom input function.&lt;/p&gt;
&lt;p&gt;Here, we define a helper function that will return an input function for a subset of our &lt;code&gt;mtcars&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfestimators)

# return an input_fn for a given subset of data
mtcars_input_fn &amp;lt;- function(data) {
  input_fn(data, 
           features = c(&amp;quot;disp&amp;quot;, &amp;quot;cyl&amp;quot;), 
           response = &amp;quot;mpg&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="feature-columns"&gt;Feature Columns&lt;/h4&gt;
&lt;p&gt;Next, we define the feature columns for our model. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model training, evaluation, and prediction steps. A feature column can be a plain mapping to some input column (e.g. &lt;code&gt;column_numeric()&lt;/code&gt; for a column of numerical data), or a transformation of other feature columns (e.g. &lt;code&gt;column_crossed()&lt;/code&gt; to define a new column as the cross of two other feature columns).&lt;/p&gt;
&lt;p&gt;Here, we create a list of feature columns containing two numeric variables - &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;cyl&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cols &amp;lt;- feature_columns(
  column_numeric(&amp;quot;disp&amp;quot;),
  column_numeric(&amp;quot;cyl&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also define multiple feature columns at once:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cols &amp;lt;- feature_columns( 
  column_numeric(&amp;quot;disp&amp;quot;, &amp;quot;cyl&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using the family of feature column functions we can define various transformations on the data before using it for modeling.&lt;/p&gt;
&lt;h4 id="estimator"&gt;Estimator&lt;/h4&gt;
&lt;p&gt;Next, we create the estimator by calling the &lt;code&gt;linear_regressor()&lt;/code&gt; function and passing it a set of feature columns:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- linear_regressor(feature_columns = cols)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training"&gt;Training&lt;/h4&gt;
&lt;p&gt;We’re now ready to train our model, using the &lt;code&gt;train()&lt;/code&gt; function. We’ll partition the &lt;code&gt;mtcars&lt;/code&gt; data set into separate training and validation data sets, and feed the training data set into &lt;code&gt;train()&lt;/code&gt;. We’ll hold 20% of the data aside for validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;indices &amp;lt;- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))
train &amp;lt;- mtcars[indices, ]
test  &amp;lt;- mtcars[-indices, ]

# train the model
model %&amp;gt;% train(mtcars_input_fn(train))&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="evaluation"&gt;Evaluation&lt;/h4&gt;
&lt;p&gt;We can evaluate the model’s accuracy using the &lt;code&gt;evaluate()&lt;/code&gt; function, using our ‘test’ data set for validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% evaluate(mtcars_input_fn(test))&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="prediction"&gt;Prediction&lt;/h4&gt;
&lt;p&gt;After we’ve finished training out model, we can use it to generate predictions from new data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;new_obs &amp;lt;- mtcars[1:3, ]
model %&amp;gt;% predict(mtcars_input_fn(new_obs))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="learning-more"&gt;Learning More&lt;/h2&gt;
&lt;p&gt;After you’ve become familiar with these concepts, these articles cover the basics of using TensorFlow Estimators and the main components in more detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/estimator_basics.html"&gt;Estimator Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/input_functions.html"&gt;Input Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/feature_columns.html"&gt;Feature Columns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These articles describe more advanced topics/usage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/run_hooks.html"&gt;Run Hooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/creating_estimators.html"&gt;Custom Estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/layers.html"&gt;TensorFlow Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/tensorboard.html"&gt;TensorBoard Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/parsing_spec.html"&gt;Parsing Utilities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the best ways to learn is from reviewing and experimenting with examples. See the &lt;a href="https://tensorflow.rstudio.com/tfestimators/articles/examples/index.html"&gt;Examples&lt;/a&gt; page for a variety of examples to help you get started.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4c5b41781d63d968451f6eb4e55b29f6</distill:md5>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</guid>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png" medium="image" type="image/png" width="1198" height="796"/>
    </item>
    <item>
      <title>TensorFlow v1.3 Released</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</link>
      <description>


&lt;p&gt;The final release of TensorFlow v1.3 is now available. This release of TensorFlow marks the initial availability of several canned estimators, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNNClassifier&lt;/li&gt;
&lt;li&gt;DNNRegressor&lt;/li&gt;
&lt;li&gt;LinearClassifier&lt;/li&gt;
&lt;li&gt;LinearRegressor&lt;/li&gt;
&lt;li&gt;DNNLinearCombinedClassifier&lt;/li&gt;
&lt;li&gt;DNNLinearCombinedRegressor.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/tfestimators"&gt;tfestimators package&lt;/a&gt; provides a high level R interface for these estimators.&lt;/p&gt;
&lt;p&gt;Full details on the release of TensorFlow v1.3 are available here: &lt;a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0" class="uri"&gt;https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can update your R installation of TensorFlow using the &lt;code&gt;install_tensorflow&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you should also provide any options used in your original installation (e.g. &lt;code&gt;method = "conda"&lt;/code&gt;, &lt;code&gt;version = "gpu"&lt;/code&gt;, etc. )&lt;/p&gt;
&lt;h3 id="cudnn-6.0"&gt;cuDNN 6.0&lt;/h3&gt;
&lt;p&gt;TensorFlow v1.3 is built against version 6.0 of the &lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN library&lt;/a&gt; from NVIDIA. Previous versions were built against cuDNN v5.1, so for installations running the GPU version of TensorFlow this means that you will need to install an updated version of cuDNN along with TensorFlow v1.3.&lt;/p&gt;
&lt;p&gt;Updated installation instructions are available here: &lt;a href="https://tensorflow.rstudio.com/tensorflow/articles/installation_gpu.html"&gt;https://tensorflow.rstudio.com/tensorflow/installation_gpu.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Version 1.4 of TensorFlow is expected to migrate again to version 7.0 of cuDNN.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">f23324c7afd091fd4f6a58709c2b7be9</distill:md5>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</guid>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png" medium="image" type="image/png" width="3876" height="741"/>
    </item>
  </channel>
</rss>
