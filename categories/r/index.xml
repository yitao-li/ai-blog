<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 03 Nov 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>torch for tabular data</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular</link>
      <description>


&lt;p&gt;Machine learning on image-like data can be many things: fun (dogs vs. cats), societally useful (medical imaging), or societally harmful (surveillance). In comparison, tabular data – the bread and butter of data science – may seem more mundane.&lt;/p&gt;
&lt;p&gt;What’s more, if you’re particularly interested in deep learning (DL), and looking for the extra benefits to be gained from big data, big architectures, and big compute, you’re much more likely to build an impressive showcase on the former instead of the latter.&lt;/p&gt;
&lt;p&gt;So for tabular data, why not just go with random forests, or gradient boosting, or other classical methods? I can think of at least a few reasons to learn about DL for tabular data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Even if all your features are interval-scale or ordinal, thus requiring “just” some form of (not necessarily linear) regression, applying DL may result in performance benefits due to sophisticated optimization algorithms, activation functions, layer depth, and more (plus interactions of all of these).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If, in addition, there are categorical features, DL models may profit from &lt;em&gt;embedding&lt;/em&gt; those in continuous space, discovering similarities and relationships that go unnoticed in one-hot encoded representations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What if most features are numeric or categorical, but there’s also text in column F and an image in column G? With DL, different modalities can be worked on by different modules that feed their outputs into a common module, to take over from there.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="agenda"&gt;Agenda&lt;/h2&gt;
&lt;p&gt;In this introductory post, we keep the architecture straightforward. We don’t experiment with fancy optimizers or nonlinearities. Nor do we add in text or image processing. However, we do make use of embeddings, and pretty prominently at that. Thus from the above bullet list, we’ll shed a light on the second, while leaving the other two for future posts.&lt;/p&gt;
&lt;p&gt;In a nutshell, what we’ll see is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to create a custom &lt;em&gt;dataset&lt;/em&gt;, tailored to the specific data you have.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to handle a mix of numeric and categorical data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to extract continuous-space representations from the embedding modules.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset, &lt;a href="https://archive.ics.uci.edu/ml/datasets/Mushroom"&gt;Mushrooms&lt;/a&gt;, was chosen for its abundance of categorical columns. It is an unusual dataset to use in DL: It was designed for machine learning models to infer logical rules, as in: IF &lt;em&gt;a&lt;/em&gt; AND NOT &lt;em&gt;b&lt;/em&gt; OR &lt;em&gt;c&lt;/em&gt; […], then it’s an &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Mushrooms are classified into two groups: edible and non-edible. The dataset description lists five possible rules with their resulting accuracies. While the least we want to go into here is the hotly debated topic of whether DL is suited to, or how it could be made more suited to rule learning, we’ll allow ourselves some curiosity and check out what happens if we successively remove all columns used to construct those five rules.&lt;/p&gt;
&lt;p&gt;Oh, and before you start copy-pasting: Here is the example in a &lt;a href="https://colab.research.google.com/drive/1bfyoD13YaLPLVcQVOLimv6vq9y4CKMM2?usp=sharing"&gt;Google Colaboratory notebook&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(purrr)
library(readr)
library(dplyr)
library(ggplot2)
library(ggrepel)

download.file(
  &amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data&amp;quot;,
  destfile = &amp;quot;agaricus-lepiota.data&amp;quot;
)

mushroom_data &amp;lt;- read_csv(
  &amp;quot;agaricus-lepiota.data&amp;quot;,
  col_names = c(
    &amp;quot;poisonous&amp;quot;,
    &amp;quot;cap-shape&amp;quot;,
    &amp;quot;cap-surface&amp;quot;,
    &amp;quot;cap-color&amp;quot;,
    &amp;quot;bruises&amp;quot;,
    &amp;quot;odor&amp;quot;,
    &amp;quot;gill-attachment&amp;quot;,
    &amp;quot;gill-spacing&amp;quot;,
    &amp;quot;gill-size&amp;quot;,
    &amp;quot;gill-color&amp;quot;,
    &amp;quot;stalk-shape&amp;quot;,
    &amp;quot;stalk-root&amp;quot;,
    &amp;quot;stalk-surface-above-ring&amp;quot;,
    &amp;quot;stalk-surface-below-ring&amp;quot;,
    &amp;quot;stalk-color-above-ring&amp;quot;,
    &amp;quot;stalk-color-below-ring&amp;quot;,
    &amp;quot;veil-type&amp;quot;,
    &amp;quot;veil-color&amp;quot;,
    &amp;quot;ring-type&amp;quot;,
    &amp;quot;ring-number&amp;quot;,
    &amp;quot;spore-print-color&amp;quot;,
    &amp;quot;population&amp;quot;,
    &amp;quot;habitat&amp;quot;
  ),
  col_types = rep(&amp;quot;c&amp;quot;, 23) %&amp;gt;% paste(collapse = &amp;quot;&amp;quot;)
) %&amp;gt;%
  # can as well remove because there&amp;#39;s just 1 unique value
  select(-`veil-type`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;dataset()&lt;/code&gt; creates an R6 class. As with most R6 classes, there will usually be a need for an &lt;code&gt;initialize()&lt;/code&gt; method. Below, we use &lt;code&gt;initialize()&lt;/code&gt; to preprocess the data and store it in convenient pieces. More on that in a minute. Prior to that, please note the two other methods a &lt;code&gt;dataset&lt;/code&gt; has to implement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;.getitem(i)&lt;/code&gt; . This is the whole purpose of a &lt;code&gt;dataset&lt;/code&gt;: Retrieve and return the observation located at some index it is asked for. Which index? That’s to be decided by the caller, a &lt;code&gt;dataloader&lt;/code&gt;. During training, usually we want to permute the order in which observations are used, while not caring about order in case of validation or test data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;.length()&lt;/code&gt;. This method, again for use of a &lt;code&gt;dataloader&lt;/code&gt;, indicates how many observations there are.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, both methods are straightforward to implement. &lt;code&gt;.getitem(i)&lt;/code&gt; directly uses its argument to index into the data, and &lt;code&gt;.length()&lt;/code&gt; returns the number of observations:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mushroom_dataset &amp;lt;- dataset(
  name = &amp;quot;mushroom_dataset&amp;quot;,

  initialize = function(indices) {
    data &amp;lt;- self$prepare_mushroom_data(mushroom_data[indices, ])
    self$xcat &amp;lt;- data[[1]][[1]]
    self$xnum &amp;lt;- data[[1]][[2]]
    self$y &amp;lt;- data[[2]]
  },

  .getitem = function(i) {
    xcat &amp;lt;- self$xcat[i, ]
    xnum &amp;lt;- self$xnum[i, ]
    y &amp;lt;- self$y[i, ]
    
    list(x = list(xcat, xnum), y = y)
  },
  
  .length = function() {
    dim(self$y)[1]
  },
  
  prepare_mushroom_data = function(input) {
    
    input &amp;lt;- input %&amp;gt;%
      mutate(across(.fns = as.factor)) 
    
    target_col &amp;lt;- input$poisonous %&amp;gt;% 
      as.integer() %&amp;gt;%
      `-`(1) %&amp;gt;%
      as.matrix()
    
    categorical_cols &amp;lt;- input %&amp;gt;% 
      select(-poisonous) %&amp;gt;%
      select(where(function(x) nlevels(x) != 2)) %&amp;gt;%
      mutate(across(.fns = as.integer)) %&amp;gt;%
      as.matrix()

    numerical_cols &amp;lt;- input %&amp;gt;%
      select(-poisonous) %&amp;gt;%
      select(where(function(x) nlevels(x) == 2)) %&amp;gt;%
      mutate(across(.fns = as.integer)) %&amp;gt;%
      as.matrix()
    
    list(list(torch_tensor(categorical_cols), torch_tensor(numerical_cols)),
         torch_tensor(target_col))
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for data storage, there is a field for the target, &lt;code&gt;self$y&lt;/code&gt;, but instead of the expected &lt;code&gt;self$x&lt;/code&gt; we see separate fields for numerical features (&lt;code&gt;self$xnum&lt;/code&gt;) and categorical ones (&lt;code&gt;self$xcat&lt;/code&gt;). This is just for convenience: The latter will be passed into embedding modules, which require its inputs to be of type &lt;code&gt;torch_long()&lt;/code&gt;, as opposed to most other modules that, by default, work with &lt;code&gt;torch_float()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Accordingly, then, all &lt;code&gt;prepare_mushroom_data()&lt;/code&gt; does is break apart the data into those three parts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Indispensable aside:&lt;/em&gt; In this dataset, really &lt;em&gt;all&lt;/em&gt; features happen to be categorical – it’s just that for some, there are but two types. Technically, we could just have treated them the same as the non-binary features. But since normally in DL, we just leave binary features the way they are, we use this as an occasion to show how to handle a mix of various data types.&lt;/p&gt;
&lt;p&gt;Our custom &lt;code&gt;dataset&lt;/code&gt; defined, we create instances for training and validation; each gets its companion &lt;code&gt;dataloader&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:nrow(mushroom_data), size = floor(0.8 * nrow(mushroom_data)))
valid_indices &amp;lt;- setdiff(1:nrow(mushroom_data), train_indices)

train_ds &amp;lt;- mushroom_dataset(train_indices)
train_dl &amp;lt;- train_ds %&amp;gt;% dataloader(batch_size = 256, shuffle = TRUE)

valid_ds &amp;lt;- mushroom_dataset(valid_indices)
valid_dl &amp;lt;- valid_ds %&amp;gt;% dataloader(batch_size = 256, shuffle = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model"&gt;Model&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, how much you &lt;em&gt;modularize&lt;/em&gt; your models is up to you. Often, high degrees of modularization enhance readability and help with troubleshooting.&lt;/p&gt;
&lt;p&gt;Here we factor out the embedding functionality. An &lt;code&gt;embedding_module&lt;/code&gt;, to be passed the categorical features only, will call &lt;code&gt;torch&lt;/code&gt;’s &lt;code&gt;nn_embedding()&lt;/code&gt; on each of them:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_module &amp;lt;- nn_module(
  
  initialize = function(cardinalities) {
    self$embeddings = nn_module_list(lapply(cardinalities, function(x) nn_embedding(num_embeddings = x, embedding_dim = ceiling(x/2))))
  },
  
  forward = function(x) {
    embedded &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(self$embeddings))
    for (i in 1:length(self$embeddings)) {
      embedded[[i]] &amp;lt;- self$embeddings[[i]](x[ , i])
    }
    torch_cat(embedded, dim = 2)
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main model, when called, starts by embedding the categorical features, then appends the numerical input and continues processing:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;net &amp;lt;- nn_module(
  &amp;quot;mushroom_net&amp;quot;,

  initialize = function(cardinalities,
                        num_numerical,
                        fc1_dim,
                        fc2_dim) {
    self$embedder &amp;lt;- embedding_module(cardinalities)
    self$fc1 &amp;lt;- nn_linear(sum(map(cardinalities, function(x) ceiling(x/2)) %&amp;gt;% unlist()) + num_numerical, fc1_dim)
    self$fc2 &amp;lt;- nn_linear(fc1_dim, fc2_dim)
    self$output &amp;lt;- nn_linear(fc2_dim, 1)
  },

  forward = function(xcat, xnum) {
    embedded &amp;lt;- self$embedder(xcat)
    all &amp;lt;- torch_cat(list(embedded, xnum$to(dtype = torch_float())), dim = 2)
    all %&amp;gt;% self$fc1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$fc2() %&amp;gt;%
      self$output() %&amp;gt;%
      nnf_sigmoid()
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now instantiate this model, passing in, on the one hand, output sizes for the linear layers, and on the other, feature cardinalities. The latter will be used by the embedding modules to determine their output sizes, following a simple rule “embed into a space of size half the number of input values”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cardinalities &amp;lt;- map(
  mushroom_data[ , 2:ncol(mushroom_data)], compose(nlevels, as.factor)) %&amp;gt;%
  keep(function(x) x &amp;gt; 2) %&amp;gt;%
  unlist() %&amp;gt;%
  unname()

num_numerical &amp;lt;- ncol(mushroom_data) - length(cardinalities) - 1

fc1_dim &amp;lt;- 16
fc2_dim &amp;lt;- 16

model &amp;lt;- net(
  cardinalities,
  num_numerical,
  fc1_dim,
  fc2_dim
)

device &amp;lt;- if (cuda_is_available()) torch_device(&amp;quot;cuda:0&amp;quot;) else &amp;quot;cpu&amp;quot;

model &amp;lt;- model$to(device = device)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;The training loop now is “business as usual”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters, lr = 0.1)

for (epoch in 1:20) {

  model$train()
  train_losses &amp;lt;- c()  

  for (b in enumerate(train_dl)) {
    optimizer$zero_grad()
    output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
    loss &amp;lt;- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))
    loss$backward()
    optimizer$step()
    train_losses &amp;lt;- c(train_losses, loss$item())
  }

  model$eval()
  valid_losses &amp;lt;- c()

  for (b in enumerate(valid_dl)) {
    output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
    loss &amp;lt;- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))
    valid_losses &amp;lt;- c(valid_losses, loss$item())
  }

  cat(sprintf(&amp;quot;Loss at epoch %d: training: %3f, validation: %3f\n&amp;quot;, epoch, mean(train_losses), mean(valid_losses)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: training: 0.274634, validation: 0.111689
Loss at epoch 2: training: 0.057177, validation: 0.036074
Loss at epoch 3: training: 0.025018, validation: 0.016698
Loss at epoch 4: training: 0.010819, validation: 0.010996
Loss at epoch 5: training: 0.005467, validation: 0.002849
Loss at epoch 6: training: 0.002026, validation: 0.000959
Loss at epoch 7: training: 0.000458, validation: 0.000282
Loss at epoch 8: training: 0.000231, validation: 0.000190
Loss at epoch 9: training: 0.000172, validation: 0.000144
Loss at epoch 10: training: 0.000120, validation: 0.000110
Loss at epoch 11: training: 0.000098, validation: 0.000090
Loss at epoch 12: training: 0.000079, validation: 0.000074
Loss at epoch 13: training: 0.000066, validation: 0.000064
Loss at epoch 14: training: 0.000058, validation: 0.000055
Loss at epoch 15: training: 0.000052, validation: 0.000048
Loss at epoch 16: training: 0.000043, validation: 0.000042
Loss at epoch 17: training: 0.000038, validation: 0.000038
Loss at epoch 18: training: 0.000034, validation: 0.000034
Loss at epoch 19: training: 0.000032, validation: 0.000031
Loss at epoch 20: training: 0.000028, validation: 0.000027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While loss on the validation set is still decreasing, we’ll soon see that the network has learned enough to obtain an accuracy of 100%.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To check classification accuracy, we re-use the validation set, seeing how we haven’t employed it for tuning anyway.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()

test_dl &amp;lt;- valid_ds %&amp;gt;% dataloader(batch_size = valid_ds$.length(), shuffle = FALSE)
iter &amp;lt;- test_dl$.iter()
b &amp;lt;- iter$.next()

output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
preds &amp;lt;- output$to(device = &amp;quot;cpu&amp;quot;) %&amp;gt;% as.array()
preds &amp;lt;- ifelse(preds &amp;gt; 0.5, 1, 0)

comp_df &amp;lt;- data.frame(preds = preds, y = b[[2]] %&amp;gt;% as_array())
num_correct &amp;lt;- sum(comp_df$preds == comp_df$y)
num_total &amp;lt;- nrow(comp_df)
accuracy &amp;lt;- num_correct/num_total
accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Phew. No embarrassing failure for the DL approach on a task where straightforward rules are sufficient. Plus, we’ve really been parsimonious as to network size.&lt;/p&gt;
&lt;p&gt;Before concluding with an inspection of the learned embeddings, let’s have some fun obscuring things.&lt;/p&gt;
&lt;h2 id="making-the-task-harder"&gt;Making the task harder&lt;/h2&gt;
&lt;p&gt;The following rules (with accompanying accuracies) are reported in the dataset description.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Disjunctive rules for poisonous mushrooms, from most general
    to most specific:

    P_1) odor=NOT(almond.OR.anise.OR.none)
         120 poisonous cases missed, 98.52% accuracy

    P_2) spore-print-color=green
         48 cases missed, 99.41% accuracy
         
    P_3) odor=none.AND.stalk-surface-below-ring=scaly.AND.
              (stalk-color-above-ring=NOT.brown) 
         8 cases missed, 99.90% accuracy
         
    P_4) habitat=leaves.AND.cap-color=white
             100% accuracy     

    Rule P_4) may also be

    P_4&amp;#39;) population=clustered.AND.cap_color=white

    These rule involve 6 attributes (out of 22). &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, there’s no distinction being made between training and test sets; but we’ll stay with our 80:20 split anyway. We’ll successively remove all mentioned attributes, starting with the three that enabled 100% accuracy, and continuing our way up. Here are the results I obtained seeding the random number generator like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;torch_manual_seed(777)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width="90%" /&gt;
&lt;col width="9%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;without&lt;/th&gt;
&lt;th align="right"&gt;accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color, odor&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9526&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Still 95% correct … While experiments like this are fun, it looks like they can also tell us something serious: Imagine the case of so-called “debiasing” by removing features like race, gender, or income. How many proxy variables may still be left that allow for inferring the masked attributes?&lt;/p&gt;
&lt;h2 id="a-look-at-the-hidden-representations"&gt;A look at the hidden representations&lt;/h2&gt;
&lt;p&gt;Looking at the weight matrix of an embedding module, what we see are the learned representations of a feature’s values. The first categorical column was &lt;code&gt;cap-shape&lt;/code&gt;; let’s extract its corresponding embeddings:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_weights &amp;lt;- vector(mode = &amp;quot;list&amp;quot;)
for (i in 1: length(model$embedder$embeddings)) {
  embedding_weights[[i]] &amp;lt;- model$embedder$embeddings[[i]]$parameters$weight$to(device = &amp;quot;cpu&amp;quot;)
}

cap_shape_repr &amp;lt;- embedding_weights[[1]]
cap_shape_repr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor
-0.0025 -0.1271  1.8077
-0.2367 -2.6165 -0.3363
-0.5264 -0.9455 -0.6702
 0.3057 -1.8139  0.3762
-0.8583 -0.7752  1.0954
 0.2740 -0.7513  0.4879
[ CPUFloatType{6,3} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of columns is three, since that’s what we chose when creating the embedding layer. The number of rows is six, matching the number of available categories. We may look up per-feature categories in the dataset description (&lt;em&gt;agaricus-lepiota.names&lt;/em&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cap_shapes &amp;lt;- c(&amp;quot;bell&amp;quot;, &amp;quot;conical&amp;quot;, &amp;quot;convex&amp;quot;, &amp;quot;flat&amp;quot;, &amp;quot;knobbed&amp;quot;, &amp;quot;sunken&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For visualization, it’s convenient to do principal components analysis (but there are other options, like t-SNE). Here are the six cap shapes in two-dimensional space:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pca &amp;lt;- prcomp(cap_shape_repr, center = TRUE, scale. = TRUE, rank = 2)$x[, c(&amp;quot;PC1&amp;quot;, &amp;quot;PC2&amp;quot;)]

pca %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(class = cap_shapes) %&amp;gt;%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_label_repel(aes(label = class)) + 
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +
  theme(aspect.ratio = 1) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-11-03-torch-tabular/images/cap-shape.png" width="414" /&gt;&lt;/p&gt;
&lt;p&gt;Naturally, how interesting you find the results depends on how much you care about the hidden representation of a variable. Analyses like these may quickly turn into an activity where extreme caution is to be applied, as any biases in the data will immediately translate into biased representations. Moreover, reduction to two-dimensional space may or may not be adequate.&lt;/p&gt;
&lt;p&gt;This concludes our introduction to &lt;code&gt;torch&lt;/code&gt; for tabular data. While the conceptual focus was on categorical features, and how to make use of them in combination with numerical ones, we’ve taken care to also provide background on something that will come up time and again: defining a &lt;code&gt;dataset&lt;/code&gt; tailored to the task at hand.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">d4a37159c1510a8e5f7c4a3f544d4995</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular</guid>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular/images/preview.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classifying images with torch</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification</link>
      <description>


&lt;p&gt;In recent posts, we’ve been exploring essential &lt;code&gt;torch&lt;/code&gt; functionality: &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch/"&gt;tensors&lt;/a&gt;, the sine qua non of every deep learning framework; &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;autograd&lt;/a&gt;, &lt;code&gt;torch&lt;/code&gt;’s implementation of reverse-mode automatic differentiation; &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-07-torch-modules"&gt;modules&lt;/a&gt;, composable building blocks of neural networks; and &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-09-torch-optim/"&gt;optimizers&lt;/a&gt;, the – well – optimization algorithms that &lt;code&gt;torch&lt;/code&gt; provides.&lt;/p&gt;
&lt;p&gt;But we haven’t really had our “hello world” moment yet, at least not if by “hello world” you mean the inevitable &lt;em&gt;deep learning experience of classifying pets&lt;/em&gt;. Cat or dog? Beagle or boxer? Chinook or Chihuahua? We’ll distinguish ourselves by asking a (slightly) different question: What kind of bird?&lt;/p&gt;
&lt;p&gt;Topics we’ll address on our way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The core roles of &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;datasets&lt;/em&gt; and &lt;em&gt;data loaders&lt;/em&gt;, respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to apply &lt;code&gt;transform&lt;/code&gt;s, both for image preprocessing and data augmentation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to use Resnet &lt;span class="citation"&gt;(He et al. 2015)&lt;/span&gt;, a pre-trained model that comes with &lt;code&gt;torchvision&lt;/code&gt;, for transfer learning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to use learning rate schedulers, and in particular, the one-cycle learning rate algorithm [@abs-1708-07120].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to find a good initial learning rate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For convenience, the code is available on &lt;a href="https://colab.research.google.com/drive/1OJzzqiQVbh3ZdLB2L2t_DhBGInlh9o-k?usp=sharing"&gt;Google Colaboratory&lt;/a&gt; – no copy-pasting required.&lt;/p&gt;
&lt;h2 id="data-loading-and-preprocessing"&gt;Data loading and preprocessing&lt;/h2&gt;
&lt;p&gt;The example dataset used here is available on &lt;a href="https://www.kaggle.com/gpiosenka/100-bird-species/data" class="uri"&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Conveniently, it may be obtained using &lt;a href="https://github.com/mlverse/torchdatasets"&gt;&lt;code&gt;torchdatasets&lt;/code&gt;&lt;/a&gt;, which uses &lt;a href="https://github.com/rstudio/pins"&gt;&lt;code&gt;pins&lt;/code&gt;&lt;/a&gt; for authentication, retrieval and storage. To enable &lt;code&gt;pins&lt;/code&gt; to manage your Kaggle downloads, please follow the instructions &lt;a href="https://pins.rstudio.com/articles/boards-kaggle.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This dataset is very “clean”, unlike the images we may be used to from, e.g., &lt;a href="http://image-net.org/"&gt;ImageNet&lt;/a&gt;. To help with generalization, we introduce noise during training – in other words, we perform &lt;em&gt;data augmentation&lt;/em&gt;. In &lt;code&gt;torchvision&lt;/code&gt;, data augmentation is part of an &lt;em&gt;image processing pipeline&lt;/em&gt; that first converts an image to a tensor, and then applies any transformations such as resizing, cropping, normalization, or various forms of distorsion.&lt;/p&gt;
&lt;p&gt;Below are the transformations performed on the training set. Note how most of them are for data augmentation, while normalization is done to comply with what’s expected by ResNet.&lt;/p&gt;
&lt;h4 id="image-preprocessing-pipeline"&gt;Image preprocessing pipeline&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(torchvision)
library(torchdatasets)

library(dplyr)
library(pins)
library(ggplot2)

device &amp;lt;- if (cuda_is_available()) torch_device(&amp;quot;cuda:0&amp;quot;) else &amp;quot;cpu&amp;quot;

train_transforms &amp;lt;- function(img) {
  img %&amp;gt;%
    # first convert image to tensor
    transform_to_tensor() %&amp;gt;%
    # then move to the GPU (if available)
    (function(x) x$to(device = device)) %&amp;gt;%
    # data augmentation
    transform_random_resized_crop(size = c(224, 224)) %&amp;gt;%
    # data augmentation
    transform_color_jitter() %&amp;gt;%
    # data augmentation
    transform_random_horizontal_flip() %&amp;gt;%
    # normalize according to what is expected by resnet
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the validation set, we don’t want to introduce noise, but still need to resize, crop, and normalize the images. The test set should be treated identically.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;valid_transforms &amp;lt;- function(img) {
  img %&amp;gt;%
    transform_to_tensor() %&amp;gt;%
    (function(x) x$to(device = device)) %&amp;gt;%
    transform_resize(256) %&amp;gt;%
    transform_center_crop(224) %&amp;gt;%
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}

test_transforms &amp;lt;- valid_transforms&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, let’s get the data, nicely divided into training, validation and test sets. Additionally, we tell the corresponding R objects what transformations they’re expected to apply:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, download = TRUE, transform = train_transforms)

valid_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, split = &amp;quot;valid&amp;quot;, transform = valid_transforms)

test_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, split = &amp;quot;test&amp;quot;, transform = test_transforms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things to note. First, transformations are part of the &lt;em&gt;dataset&lt;/em&gt; concept, as opposed to the &lt;em&gt;data loader&lt;/em&gt; we’ll encounter shortly. Second, let’s take a look at how the images have been stored on disk. The overall directory structure (starting from &lt;code&gt;data&lt;/code&gt;, which we specified as the root directory to be used) is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/bird_species/train
data/bird_species/valid
data/bird_species/test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;valid&lt;/code&gt;, and &lt;code&gt;test&lt;/code&gt; directories, different classes of images reside in their own folders. For example, here is the directory layout for the first three classes in the test set:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/bird_species/test/ALBATROSS/
 - data/bird_species/test/ALBATROSS/1.jpg
 - data/bird_species/test/ALBATROSS/2.jpg
 - data/bird_species/test/ALBATROSS/3.jpg
 - data/bird_species/test/ALBATROSS/4.jpg
 - data/bird_species/test/ALBATROSS/5.jpg
 
data/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/1.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/2.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/3.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/4.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/5.jpg
 
 data/test/&amp;#39;AMERICAN BITTERN&amp;#39;/
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/1.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/2.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/3.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/4.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/5.jpg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is exactly the kind of layout expected by &lt;code&gt;torch&lt;/code&gt;s &lt;code&gt;image_folder_dataset()&lt;/code&gt; – and really &lt;code&gt;bird_species_dataset()&lt;/code&gt; instantiates a subtype of this class. Had we downloaded the data manually, respecting the required directory structure, we could have created the datasets like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# e.g.
train_ds &amp;lt;- image_folder_dataset(
  file.path(data_dir, &amp;quot;train&amp;quot;),
  transform = train_transforms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we got the data, let’s see how many items there are in each set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds$.length()
valid_ds$.length()
test_ds$.length()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;31316
1125
1125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That training set is really big! It’s thus recommended to run this on GPU, or just play around with the provided Colab notebook.&lt;/p&gt;
&lt;p&gt;With so many samples, we’re curious how many classes there are.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_names &amp;lt;- test_ds$classes
length(class_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we &lt;em&gt;do&lt;/em&gt; have a substantial training set, but the task is formidable as well: We’re going to tell apart no less than 225 different bird species.&lt;/p&gt;
&lt;h4 id="data-loaders"&gt;Data loaders&lt;/h4&gt;
&lt;p&gt;While &lt;em&gt;datasets&lt;/em&gt; know what to do with each single item, &lt;em&gt;data loaders&lt;/em&gt; know how to treat them collectively. How many samples make up a batch? Do we want to feed them in the same order always, or instead, have a different order chosen for every epoch?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 64

train_dl &amp;lt;- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)
valid_dl &amp;lt;- dataloader(valid_ds, batch_size = batch_size)
test_dl &amp;lt;- dataloader(test_ds, batch_size = batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data loaders, too, may be queried for their length. Now length means: How many batches?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dl$.length() 
valid_dl$.length() 
test_dl$.length()  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;490
18
18&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="some-birds"&gt;Some birds&lt;/h4&gt;
&lt;p&gt;Next, let’s view a few images from the test set. We can retrieve the first batch – images and corresponding classes – by creating an iterator from the &lt;code&gt;dataloader&lt;/code&gt; and calling &lt;code&gt;next()&lt;/code&gt; on it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# for display purposes, here we are actually using a batch_size of 24
batch &amp;lt;- train_dl$.iter()$.next()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;batch&lt;/code&gt; is a list, the first item being the image tensors:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch[[1]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  24   3 224 224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the second, the classes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch[[2]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Classes are coded as integers, to be used as indices in a vector of class names. We’ll use those for labeling the images.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classes &amp;lt;- batch[[2]]
classes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
 1
 1
 1
 1
 2
 2
 2
 2
 2
 3
 3
 3
 3
 3
 4
 4
 4
 4
 4
 5
 5
 5
 5
[ GPULongType{24} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The image tensors have shape &lt;code&gt;batch_size x num_channels x height x width&lt;/code&gt;. For plotting using &lt;code&gt;as.raster()&lt;/code&gt;, we need to reshape the images such that channels come last. We also undo the normalization applied by the &lt;code&gt;dataloader&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are the first twenty-four images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

images &amp;lt;- as_array(batch[[1]]) %&amp;gt;% aperm(perm = c(1, 3, 4, 2))
mean &amp;lt;- c(0.485, 0.456, 0.406)
std &amp;lt;- c(0.229, 0.224, 0.225)
images &amp;lt;- std * images + mean
images &amp;lt;- images * 255
images[images &amp;gt; 255] &amp;lt;- 255
images[images &amp;lt; 0] &amp;lt;- 0

par(mfcol = c(4,6), mar = rep(1, 4))

images %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::set_names(class_names[as_array(classes)]) %&amp;gt;%
  purrr::map(as.raster, max = 255) %&amp;gt;%
  purrr::iwalk(~{plot(.x); title(.y)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/image_classif_birds.png" width="250" /&gt;&lt;/p&gt;
&lt;h2 id="model"&gt;Model&lt;/h2&gt;
&lt;p&gt;The backbone of our model is a pre-trained instance of ResNet.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model_resnet18(pretrained = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we want to distinguish among our 225 bird species, while ResNet was trained on 1000 different classes. What can we do? We simply replace the output layer.&lt;/p&gt;
&lt;p&gt;The new output layer is also the only one whose weights we are going to train – leaving all other ResNet parameters the way they are. Technically, we &lt;em&gt;could&lt;/em&gt; perform backpropagation through the complete model, striving to fine-tune ResNet’s weights as well. However, this would slow down training significantly. In fact, the choice is not all-or-none: It is up to us how many of the original parameters to keep fixed, and how many to “set free” for fine tuning. For the task at hand, we’ll be content to just train the newly added output layer: With the abundance of animals, including birds, in ImageNet, we expect the trained ResNet to know a lot about them!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$parameters %&amp;gt;% purrr::walk(function(param) param$requires_grad_(FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To replace the output layer, the model is modified in-place:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_features &amp;lt;- model$fc$in_features

model$fc &amp;lt;- nn_linear(in_features = num_features, out_features = length(class_names))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now put the modified model on the GPU (if available):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model$to(device = device)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;For optimization, we use cross entropy loss and stochastic gradient descent.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;criterion &amp;lt;- nn_cross_entropy_loss()

optimizer &amp;lt;- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="finding-an-optimally-efficient-learning-rate"&gt;Finding an optimally efficient learning rate&lt;/h4&gt;
&lt;p&gt;We set the learning rate to &lt;code&gt;0.1&lt;/code&gt;, but that is just a formality. As has become widely known due to the excellent lectures by &lt;a href="http://fast.ai"&gt;fast.ai&lt;/a&gt;, it makes sense to spend some time upfront to determine an efficient learning rate. While out-of-the-box, &lt;code&gt;torch&lt;/code&gt; does not provide a tool like fast.ai’s learning rate finder, the logic is straightforward to implement. Here’s how to find a good learning rate, as translated to R from &lt;a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"&gt;Sylvain Gugger’s post&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# ported from: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html

losses &amp;lt;- c()
log_lrs &amp;lt;- c()

find_lr &amp;lt;- function(init_value = 1e-8, final_value = 10, beta = 0.98) {

  num &amp;lt;- train_dl$.length()
  mult = (final_value/init_value)^(1/num)
  lr &amp;lt;- init_value
  optimizer$param_groups[[1]]$lr &amp;lt;- lr
  avg_loss &amp;lt;- 0
  best_loss &amp;lt;- 0
  batch_num &amp;lt;- 0

  for (b in enumerate(train_dl)) {

    batch_num &amp;lt;- batch_num + 1
    optimizer$zero_grad()
    output &amp;lt;- model(b[[1]]$to(device = device))
    loss &amp;lt;- criterion(output, b[[2]]$to(device = device))

    #Compute the smoothed loss
    avg_loss &amp;lt;- beta * avg_loss + (1-beta) * loss$item()
    smoothed_loss &amp;lt;- avg_loss / (1 - beta^batch_num)
    #Stop if the loss is exploding
    if (batch_num &amp;gt; 1 &amp;amp;&amp;amp; smoothed_loss &amp;gt; 4 * best_loss) break
    #Record the best loss
    if (smoothed_loss &amp;lt; best_loss || batch_num == 1) best_loss &amp;lt;- smoothed_loss

    #Store the values
    losses &amp;lt;&amp;lt;- c(losses, smoothed_loss)
    log_lrs &amp;lt;&amp;lt;- c(log_lrs, (log(lr, 10)))

    loss$backward()
    optimizer$step()

    #Update the lr for the next step
    lr &amp;lt;- lr * mult
    optimizer$param_groups[[1]]$lr &amp;lt;- lr
  }
}

find_lr()

df &amp;lt;- data.frame(log_lrs = log_lrs, losses = losses)
ggplot(df, aes(log_lrs, losses)) + geom_point(size = 1) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/lr_finder.png" width="372" /&gt;&lt;/p&gt;
&lt;p&gt;The best learning rate is not the exact one where loss is at a minimum. Instead, it should be picked somewhat earlier on the curve, while loss is still decreasing. &lt;code&gt;0.05&lt;/code&gt; looks like a sensible choice.&lt;/p&gt;
&lt;p&gt;This value is nothing but an anchor, however. &lt;em&gt;Learning rate schedulers&lt;/em&gt; allow learning rates to evolve according to some proven algorithm. Among others, &lt;code&gt;torch&lt;/code&gt; implements one-cycle learning [@abs-1708-07120], cyclical learning rates &lt;span class="citation"&gt;(Smith 2015)&lt;/span&gt;, and cosine annealing with warm restarts &lt;span class="citation"&gt;(Loshchilov and Hutter 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we use &lt;code&gt;lr_one_cycle()&lt;/code&gt;, passing in our newly found, optimally efficient, hopefully, value &lt;code&gt;0.05&lt;/code&gt; as a maximum learning rate. &lt;code&gt;lr_one_cycle()&lt;/code&gt; will start with a low rate, then gradually ramp up until it reaches the allowed maximum. After that, the learning rate will slowly, continuously decrease, until it falls slightly below its initial value.&lt;/p&gt;
&lt;p&gt;All this happens not per epoch, but exactly once, which is why the name has &lt;code&gt;one_cycle&lt;/code&gt; in it. Here’s how the evolution of learning rates looks in our example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/one_cycle_lr.png" width="315" /&gt;&lt;/p&gt;
&lt;p&gt;Before we start training, let’s quickly re-initialize the model, so as to start from a clean slate:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model_resnet18(pretrained = TRUE)
model$parameters %&amp;gt;% purrr::walk(function(param) param$requires_grad_(FALSE))

num_features &amp;lt;- model$fc$in_features

model$fc &amp;lt;- nn_linear(in_features = num_features, out_features = length(class_names))

model &amp;lt;- model$to(device = device)

criterion &amp;lt;- nn_cross_entropy_loss()

optimizer &amp;lt;- optim_sgd(model$parameters, lr = 0.05, momentum = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And instantiate the scheduler:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs = 10

scheduler &amp;lt;- optimizer %&amp;gt;% 
  lr_one_cycle(max_lr = 0.05, epochs = num_epochs, steps_per_epoch = train_dl$.length())&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training-loop"&gt;Training loop&lt;/h4&gt;
&lt;p&gt;Now we train for ten epochs. For every training batch, we call &lt;code&gt;scheduler$step()&lt;/code&gt; to adjust the learning rate. Notably, this has to be done &lt;em&gt;after&lt;/em&gt; &lt;code&gt;optimizer$step()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_batch &amp;lt;- function(b) {

  optimizer$zero_grad()
  output &amp;lt;- model(b[[1]])
  loss &amp;lt;- criterion(output, b[[2]]$to(device = device))
  loss$backward()
  optimizer$step()
  scheduler$step()
  loss$item()

}

valid_batch &amp;lt;- function(b) {

  output &amp;lt;- model(b[[1]])
  loss &amp;lt;- criterion(output, b[[2]]$to(device = device))
  loss$item()
}

for (epoch in 1:num_epochs) {

  model$train()
  train_losses &amp;lt;- c()

  for (b in enumerate(train_dl)) {
    loss &amp;lt;- train_batch(b)
    train_losses &amp;lt;- c(train_losses, loss)
  }

  model$eval()
  valid_losses &amp;lt;- c()

  for (b in enumerate(valid_dl)) {
    loss &amp;lt;- valid_batch(b)
    valid_losses &amp;lt;- c(valid_losses, loss)
  }

  cat(sprintf(&amp;quot;\nLoss at epoch %d: training: %3f, validation: %3f\n&amp;quot;, epoch, mean(train_losses), mean(valid_losses)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: training: 2.662901, validation: 0.790769

Loss at epoch 2: training: 1.543315, validation: 1.014409

Loss at epoch 3: training: 1.376392, validation: 0.565186

Loss at epoch 4: training: 1.127091, validation: 0.575583

Loss at epoch 5: training: 0.916446, validation: 0.281600

Loss at epoch 6: training: 0.775241, validation: 0.215212

Loss at epoch 7: training: 0.639521, validation: 0.151283

Loss at epoch 8: training: 0.538825, validation: 0.106301

Loss at epoch 9: training: 0.407440, validation: 0.083270

Loss at epoch 10: training: 0.354659, validation: 0.080389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the model made good progress, but we don’t yet know anything about classification accuracy in absolute terms. We’ll check that out on the test set.&lt;/p&gt;
&lt;h2 id="test-set-accuracy"&gt;Test set accuracy&lt;/h2&gt;
&lt;p&gt;Finally, we calculate accuracy on the test set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()

test_batch &amp;lt;- function(b) {

  output &amp;lt;- model(b[[1]])
  labels &amp;lt;- b[[2]]$to(device = device)
  loss &amp;lt;- criterion(output, labels)
  
  test_losses &amp;lt;&amp;lt;- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values
  # and position 2 containing the respective indices
  predicted &amp;lt;- torch_max(output$data(), dim = 2)[[2]]
  total &amp;lt;&amp;lt;- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct &amp;lt;&amp;lt;- correct + (predicted == labels)$sum()$item()

}

test_losses &amp;lt;- c()
total &amp;lt;- 0
correct &amp;lt;- 0

for (b in enumerate(test_dl)) {
  test_batch(b)
}

mean(test_losses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.03719&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_accuracy &amp;lt;-  correct/total
test_accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.98756&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An impressive result, given how many different species there are!&lt;/p&gt;
&lt;h2 id="wrapup"&gt;Wrapup&lt;/h2&gt;
&lt;p&gt;Hopefully, this has been a useful introduction to classifying images with &lt;code&gt;torch&lt;/code&gt;, as well as to its non-domain-specific architectural elements, like datasets, data loaders, and learning-rate schedulers. Future posts will explore other domains, as well as move on beyond “hello world” in image recognition. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-HeZRS15"&gt;
&lt;p&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” &lt;em&gt;CoRR&lt;/em&gt; abs/1512.03385. &lt;a href="http://arxiv.org/abs/1512.03385"&gt;http://arxiv.org/abs/1512.03385&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-LoshchilovH16a"&gt;
&lt;p&gt;Loshchilov, Ilya, and Frank Hutter. 2016. “SGDR: Stochastic Gradient Descent with Restarts.” &lt;em&gt;CoRR&lt;/em&gt; abs/1608.03983. &lt;a href="http://arxiv.org/abs/1608.03983"&gt;http://arxiv.org/abs/1608.03983&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Smith15a"&gt;
&lt;p&gt;Smith, Leslie N. 2015. “No More Pesky Learning Rate Guessing Games.” &lt;em&gt;CoRR&lt;/em&gt; abs/1506.01186. &lt;a href="http://arxiv.org/abs/1506.01186"&gt;http://arxiv.org/abs/1506.01186&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Physically, the dataset consists of a single &lt;code&gt;zip&lt;/code&gt; file; so it is really the first instruction that downloads all the data. The remaining two function calls perform semantic mappings only.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">b3e0f8f45fd6819a8f7bf3bc2a833824</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification</guid>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification/images/image_classif_birds.png" medium="image" type="image/png" width="500" height="333"/>
    </item>
    <item>
      <title>sparklyr.flint 0.2: ASOF Joins, OLS Regression, and additional summarizers</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</link>
      <description>


&lt;p&gt;Since &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/index.html"&gt;&lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/a&gt;, a &lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; extension for leveraging &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; time series functionalities through &lt;code&gt;sparklyr&lt;/code&gt;, was &lt;a href="https://blogs.rstudio.com/ai/posts/2020-09-07-sparklyr-flint"&gt;introduced&lt;/a&gt; in September, we have made a number of enhancements to it, and have successfully submitted &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 to CRAN.&lt;/p&gt;
&lt;p&gt;In this blog post, we highlight the following new features and improvements from &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#asof-joins"&gt;ASOF Joins&lt;/a&gt; of Timeseries RDDs&lt;/li&gt;
&lt;li&gt;&lt;a href="#ols-regression"&gt;OLS Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#additional-summarizers"&gt;Additional Summarizers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="asof-joins"&gt;ASOF Joins&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the term, ASOF joins are temporal join operations based on inexact matching of timestamps. Within the context of &lt;a href="https://spark.apache.org"&gt;Apache Spark&lt;/a&gt;, a join operation, loosely speaking, matches records from two data frames (let’s call them &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;) based on some criteria. A temporal join implies matching records in &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; based on timestamps, and with inexact matching of timestamps permitted, it is typically useful to join &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; along one of the following temporal directions:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Looking behind: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t&lt;/code&gt;, then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the most recent timestamp less than or equal to &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Looking ahead: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t,&lt;/code&gt; then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the smallest timestamp greater than or equal to (or alternatively, strictly greater than) &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, oftentimes it is not useful to consider two timestamps as “matching” if they are too far apart. Therefore, an additional constraint on the maximum amount of time to look behind or look ahead is usually also part of an ASOF join operation.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, all ASOF join functionalities of Flint are accessible via the &lt;code&gt;asof_join()&lt;/code&gt; method. For example, given 2 timeseries RDDs &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(sparklyr)
library(sparklyr.flint)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
left &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10), u = seq(10))) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)
right &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10) + 1, v = seq(10) + 1L)) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following prints the result of matching each record from &lt;code&gt;left&lt;/code&gt; with the most recent record(s) from &lt;code&gt;right&lt;/code&gt; that are at most 1 second behind.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;gt;=&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1    NA
##  2 1970-01-01 00:00:02     2     2
##  3 1970-01-01 00:00:03     3     3
##  4 1970-01-01 00:00:04     4     4
##  5 1970-01-01 00:00:05     5     5
##  6 1970-01-01 00:00:06     6     6
##  7 1970-01-01 00:00:07     7     7
##  8 1970-01-01 00:00:08     8     8
##  9 1970-01-01 00:00:09     9     9
## 10 1970-01-01 00:00:10    10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas if we change the temporal direction to “&amp;lt;”, then each record from &lt;code&gt;left&lt;/code&gt; will be matched with any record(s) from &lt;code&gt;right&lt;/code&gt; that is strictly in the future and is at most 1 second ahead of the current record from &lt;code&gt;left&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;lt;&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1     2
##  2 1970-01-01 00:00:02     2     3
##  3 1970-01-01 00:00:03     3     4
##  4 1970-01-01 00:00:04     4     5
##  5 1970-01-01 00:00:05     5     6
##  6 1970-01-01 00:00:06     6     7
##  7 1970-01-01 00:00:07     7     8
##  8 1970-01-01 00:00:08     8     9
##  9 1970-01-01 00:00:09     9    10
## 10 1970-01-01 00:00:10    10    11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice regardless of which temporal direction is selected, an outer-left join is always performed (i.e., all timestamp values and &lt;code&gt;u&lt;/code&gt; values of &lt;code&gt;left&lt;/code&gt; from above will always be present in the output, and the &lt;code&gt;v&lt;/code&gt; column in the output will contain &lt;code&gt;NA&lt;/code&gt; whenever there is no record from &lt;code&gt;right&lt;/code&gt; that meets the matching criteria).&lt;/p&gt;
&lt;h2 id="ols-regression"&gt;OLS Regression&lt;/h2&gt;
&lt;p&gt;You might be wondering whether the version of this functionality in Flint is more or less identical to &lt;code&gt;lm()&lt;/code&gt; in R. Turns out it has much more to offer than &lt;code&gt;lm()&lt;/code&gt; does. An OLS regression in Flint will compute useful metrics such as &lt;a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"&gt;Akaike information criterion&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion"&gt;Bayesian information criterion&lt;/a&gt;, both of which are useful for model selection purposes, and the calculations of both are parallelized by Flint to fully utilize computational power available in a Spark cluster. In addition, Flint supports ignoring regressors that are constant or nearly constant, which becomes useful when an intercept term is included. To see why this is the case, we need to briefly examine the goal of the OLS regression, which is to find some column vector of coefficients &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt; that minimizes &lt;span class="math inline"&gt;\(\|\mathbf{y} - \mathbf{X} \mathbf{\beta}\|^2\)&lt;/span&gt;, where &lt;span class="math inline"&gt;\(\mathbf{y}\)&lt;/span&gt; is the column vector of response variables, and &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is a matrix consisting of columns of regressors plus an entire column of &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s representing the intercept terms. The solution to this problem is &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt;, assuming the Gram matrix &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; is non-singular. However, if &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; contains a column of all &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s of intercept terms, and another column formed by a regressor that is constant (or nearly so), then columns of &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; will be linearly dependent (or nearly so) and &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; will be singular (or nearly so), which presents an issue computation-wise. However, if a regressor is constant, then it essentially plays the same role as the intercept terms do. So simply excluding such a constant regressor in &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; solves the problem. Also, speaking of inverting the Gram matrix, readers remembering the concept of “condition number” from numerical analysis must be thinking to themselves how computing &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt; could be numerically unstable if &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; has a large condition number. This is why Flint also outputs the condition number of the Gram matrix in the OLS regression result, so that one can sanity-check the underlying quadratic minimization problem being solved is well-conditioned.&lt;/p&gt;
&lt;p&gt;So, to summarize, the OLS regression functionality implemented in Flint not only outputs the solution to the problem, but also calculates useful metrics that help data scientists assess the sanity and predictive quality of the resulting model.&lt;/p&gt;
&lt;p&gt;To see OLS regression in action with &lt;code&gt;sparklyr.flint&lt;/code&gt;, one can run the following example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mtcars_sdf &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE) %&amp;gt;%
  dplyr::mutate(time = 0L)
mtcars_ts &amp;lt;- from_sdf(mtcars_sdf, is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;)
model &amp;lt;- ols_regression(mtcars_ts, mpg ~ hp + wt) %&amp;gt;% to_sdf()

print(model %&amp;gt;% dplyr::select(akaikeIC, bayesIC, cond))

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##   akaikeIC bayesIC    cond
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     155.    159. 345403.

# ^ output says condition number of the Gram matrix was within reason&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and obtain &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt;, the vector of optimal coefficients, with the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(model %&amp;gt;% dplyr::pull(beta))

## [[1]]
## [1] -0.03177295 -3.87783074&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="additional-summarizers"&gt;Additional Summarizers&lt;/h2&gt;
&lt;p&gt;The EWMA (Exponential Weighted Moving Average), EMA half-life, and the standardized moment summarizers (namely, skewness and kurtosis) along with a few others which were missing in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 are now fully supported in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2.&lt;/p&gt;
&lt;h2 id="better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;While &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 included a &lt;code&gt;collect()&lt;/code&gt; method for exporting data from a Flint time-series RDD to an R data frame, it did not have a similar method for extracting the underlying Spark data frame from a Flint time-series RDD. This was clearly an oversight. In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, one can call &lt;code&gt;to_sdf()&lt;/code&gt; on a timeseries RDD to get back a Spark data frame that is usable in &lt;code&gt;sparklyr&lt;/code&gt; (e.g., as shown by &lt;code&gt;model %&amp;gt;% to_sdf() %&amp;gt;% dplyr::select(...)&lt;/code&gt; examples from above). One can also get to the underlying Spark data frame JVM object reference by calling &lt;code&gt;spark_dataframe()&lt;/code&gt; on a Flint time-series RDD (this is usually unnecessary in vast majority of &lt;code&gt;sparklyr&lt;/code&gt; use cases though).&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have presented a number of new features and improvements introduced in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 and deep-dived into some of them in this blog post. We hope you are as excited about them as we are.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;The author would like to thank Mara (&lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;), Sigrid (&lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;), and Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) for their fantastic editorial inputs on this blog post!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4341b8d32399655443d0be1f44b15976</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</guid>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released/images/sparklyr-flint-0.2.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Optimizers in torch</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim</link>
      <description>


&lt;p&gt;This is the fourth and last installment in a series introducing &lt;code&gt;torch&lt;/code&gt; basics. Initially, we &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch/"&gt;focused on &lt;em&gt;tensors&lt;/em&gt;&lt;/a&gt;. To illustrate their power, we coded a complete (if toy-size) neural network from scratch. We didn’t make use of any of &lt;code&gt;torch&lt;/code&gt;’s higher-level capabilities – not even &lt;em&gt;autograd&lt;/em&gt;, its automatic-differentiation feature.&lt;/p&gt;
&lt;p&gt;This changed in the &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;follow-up post&lt;/a&gt;. No more thinking about derivatives and the chain rule; a single call to &lt;code&gt;backward()&lt;/code&gt; did it all.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-07-torch-modules"&gt;In the third post&lt;/a&gt;, the code again saw a major simplification. Instead of tediously assembling a DAG&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; by hand, we let &lt;em&gt;modules&lt;/em&gt; take care of the logic.&lt;/p&gt;
&lt;p&gt;Based on that last state, there are just two more things to do. For one, we still compute the loss by hand. And secondly, even though we get the gradients all nicely computed from &lt;em&gt;autograd&lt;/em&gt;, we still loop over the model’s parameters, updating them all ourselves. You won’t be surprised to hear that none of this is necessary.&lt;/p&gt;
&lt;h2 id="losses-and-loss-functions"&gt;Losses and loss functions&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;torch&lt;/code&gt; comes with all the usual loss functions, such as mean squared error, cross entropy, Kullback-Leibler divergence, and the like. In general, there are two usage modes.&lt;/p&gt;
&lt;p&gt;Take the example of calculating mean squared error. One way is to call &lt;code&gt;nnf_mse_loss()&lt;/code&gt; directly on the prediction and ground truth tensors. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- torch_randn(c(3, 2, 3))
y &amp;lt;- torch_zeros(c(3, 2, 3))

nnf_mse_loss(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
0.682362
[ CPUFloatType{} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other loss functions designed to be called directly start with &lt;code&gt;nnf_&lt;/code&gt; as well: &lt;code&gt;nnf_binary_cross_entropy()&lt;/code&gt;, &lt;code&gt;nnf_nll_loss()&lt;/code&gt;, &lt;code&gt;nnf_kl_div()&lt;/code&gt; … and so on.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The second way is to define the algorithm in advance and call it at some later time. Here, respective constructors all start with &lt;code&gt;nn_&lt;/code&gt; and end in &lt;code&gt;_loss&lt;/code&gt;. For example: &lt;code&gt;nn_bce_loss()&lt;/code&gt;, &lt;code&gt;nn_nll_loss(),&lt;/code&gt; &lt;code&gt;nn_kl_div_loss()&lt;/code&gt; …&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- nn_mse_loss()

loss(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
0.682362
[ CPUFloatType{} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method may be preferable when one and the same algorithm should be applied to more than one pair of tensors.&lt;/p&gt;
&lt;h2 id="optimizers"&gt;Optimizers&lt;/h2&gt;
&lt;p&gt;So far, we’ve been updating model parameters following a simple strategy: The gradients told us which direction on the loss curve was downward; the learning rate told us how big of a step to take. What we did was a straightforward implementation of &lt;em&gt;gradient descent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, optimization algorithms used in deep learning get a lot more sophisticated than that. Below, we’ll see how to replace our manual updates using &lt;code&gt;optim_adam()&lt;/code&gt;, &lt;code&gt;torch&lt;/code&gt;’s implementation of the Adam algorithm &lt;span class="citation"&gt;(Kingma and Ba 2017)&lt;/span&gt;. First though, let’s take a quick look at how &lt;code&gt;torch&lt;/code&gt; optimizers work.&lt;/p&gt;
&lt;p&gt;Here is a very simple network, consisting of just one linear layer, to be called on a single data point.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- torch_randn(1, 3)

model &amp;lt;- nn_linear(3, 1)
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we create an optimizer, we tell it what parameters it is supposed to work on.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters, lr = 0.01)
optimizer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;optim_adam&amp;gt;
  Inherits from: &amp;lt;torch_Optimizer&amp;gt;
  Public:
    add_param_group: function (param_group) 
    clone: function (deep = FALSE) 
    defaults: list
    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, 
    param_groups: list
    state: list
    step: function (closure = NULL) 
    zero_grad: function () &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At any time, we can inspect those parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$param_groups[[1]]$params&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we perform the forward and backward passes. The backward pass calculates the gradients, but does &lt;em&gt;not&lt;/em&gt; update the parameters, as we can see both from the model &lt;em&gt;and&lt;/em&gt; the optimizer objects:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out &amp;lt;- model(data)
out$backward()

optimizer$param_groups[[1]]$params
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling &lt;code&gt;step()&lt;/code&gt; on the optimizer actually &lt;em&gt;performs&lt;/em&gt; the updates. Again, let’s check that both model and optimizer now hold the updated values:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$step()

optimizer$param_groups[[1]]$params
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL
$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we perform optimization in a loop, we need to make sure to call &lt;code&gt;optimizer$zero_grad()&lt;/code&gt; on every step, as otherwise gradients would be accumulated. You can see this in our final version of the network.&lt;/p&gt;
&lt;h2 id="simple-network-final-version"&gt;Simple network: final version&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)



### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32

model &amp;lt;- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

### network parameters ---------------------------------------------------------

# for adam, need to choose a much higher learning rate in this problem
learning_rate &amp;lt;- 0.08

optimizer &amp;lt;- optim_adam(model$parameters, lr = learning_rate)

### training loop --------------------------------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  
  y_pred &amp;lt;- model(x)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- nnf_mse_loss(y_pred, y, reduction = &amp;quot;sum&amp;quot;)
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation -------- 
  
  # Still need to zero out the gradients before the backward pass, only this time,
  # on the optimizer object
  optimizer$zero_grad()
  
  # gradients are still computed on the loss tensor (no change here)
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # use the optimizer to update model parameters
  optimizer$step()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! We’ve seen all the major actors on stage: tensors, &lt;em&gt;autograd&lt;/em&gt;, modules, loss functions, and optimizers. In future posts, we’ll explore how to use &lt;em&gt;torch&lt;/em&gt; for standard deep learning tasks involving images, text, tabular data, and more. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-kingma2017adam"&gt;
&lt;p&gt;Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” &lt;a href="http://arxiv.org/abs/1412.6980"&gt;http://arxiv.org/abs/1412.6980&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;directed acyclic graph&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;The prefix &lt;code&gt;nnf_&lt;/code&gt; was chosen because in PyTorch, the corresponding functions live in &lt;a href="https://pytorch.org/docs/stable/nn.functional.html"&gt;torch.nn.functional&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;This time, the corresponding PyTorch module is &lt;a href="https://pytorch.org/docs/stable/nn.html"&gt;torch.nn&lt;/a&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">bd93587b8614d54f43fb671d90ab8661</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim</guid>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Using torch modules</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules</link>
      <description>


&lt;p&gt;&lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch"&gt;Initially&lt;/a&gt;, we started learning about &lt;code&gt;torch&lt;/code&gt; basics by coding a simple neural network from scratch, making use of just a single of &lt;code&gt;torch&lt;/code&gt;’s features: &lt;em&gt;tensors&lt;/em&gt;. &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;Then&lt;/a&gt;, we immensely simplified the task, replacing manual backpropagation with &lt;em&gt;autograd&lt;/em&gt;. Today, we &lt;em&gt;modularize&lt;/em&gt; the network - in both the habitual and a very literal sense: Low-level matrix operations are swapped out for &lt;code&gt;torch&lt;/code&gt; &lt;code&gt;module&lt;/code&gt;s.&lt;/p&gt;
&lt;h2 id="modules"&gt;Modules&lt;/h2&gt;
&lt;p&gt;From other frameworks (Keras, say), you may be used to distinguishing between &lt;em&gt;models&lt;/em&gt; and &lt;em&gt;layers&lt;/em&gt;. In &lt;code&gt;torch&lt;/code&gt;, both are instances of &lt;code&gt;nn_Module()&lt;/code&gt;, and thus, have some methods in common. For those thinking in terms of “models” and “layers”, I’m artificially splitting up this section into two parts. In reality though, there is no dichotomy: New modules may be composed of existing ones up to arbitrary levels of recursion.&lt;/p&gt;
&lt;h3 id="base-modules-layers"&gt;Base modules (“layers”)&lt;/h3&gt;
&lt;p&gt;Instead of writing out an affine operation by hand – &lt;code&gt;x$mm(w1) + b1&lt;/code&gt;, say –, as we’ve been doing so far, we can create a linear module. The following snippet instantiates a linear layer that expects three-feature inputs and returns a single output per observation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
l &amp;lt;- nn_linear(3, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The module has two parameters, “weight” and “bias”. Both now come pre-initialized:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Modules are callable; calling a module executes its &lt;code&gt;forward()&lt;/code&gt; method, which, for a linear layer, matrix-multiplies input and weights, and adds the bias.&lt;/p&gt;
&lt;p&gt;Let’s try this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data  &amp;lt;- torch_randn(10, 3)
out &amp;lt;- l(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, &lt;code&gt;out&lt;/code&gt; now holds some data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$data()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2711
-1.8151
-0.0073
 0.1876
-0.0930
 0.7498
-0.2332
-0.0428
 0.3849
-0.2618
[ CPUFloatType{10,1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition though, this tensor knows what will need to be done, should ever it be asked to calculate gradients:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AddmmBackward&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the difference between tensors returned by modules and self-created ones. When creating tensors ourselves, we need to pass &lt;code&gt;requires_grad = TRUE&lt;/code&gt; to trigger gradient calculation. With modules, &lt;code&gt;torch&lt;/code&gt; correctly assumes that we’ll want to perform backpropagation at some point.&lt;/p&gt;
&lt;p&gt;By now though, we haven’t called &lt;code&gt;backward()&lt;/code&gt; yet. Thus, no gradients have yet been computed:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$weight$grad
l$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
[ Tensor (undefined) ]
torch_tensor 
[ Tensor (undefined) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s change this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in (function (self, gradient, keep_graph, create_graph)  : 
  grad can be implicitly created only for scalar outputs (_make_grads at ../torch/csrc/autograd/autograd.cpp:47)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why the error? &lt;em&gt;Autograd&lt;/em&gt; expects the output tensor to be a scalar, while in our example, we have a tensor of size &lt;code&gt;(10, 1)&lt;/code&gt;. This error won’t often occur in practice, where we work with &lt;em&gt;batches&lt;/em&gt; of inputs (sometimes, just a single batch). But still, it’s interesting to see how to resolve this.&lt;/p&gt;
&lt;p&gt;To make the example work, we introduce a – virtual – final aggregation step – taking the mean, say. Let’s call it &lt;code&gt;avg&lt;/code&gt;. If such a mean were taken, its gradient with respect to &lt;code&gt;l$weight&lt;/code&gt; would be obtained via the chain rule:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{equation*} 
 \frac{\partial \ avg}{\partial w} = \frac{\partial \ avg}{\partial \ out}  \ \frac{\partial \ out}{\partial w}
\end{equation*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Of the quantities on the right side, we’re interested in the second. We need to provide the first one, the way it would look &lt;em&gt;if really we were taking the mean&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d_avg_d_out &amp;lt;- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()
out$backward(gradient = d_avg_d_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;l$weight$grad&lt;/code&gt; and &lt;code&gt;l$bias$grad&lt;/code&gt; &lt;em&gt;do&lt;/em&gt; contain gradients:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$weight$grad
l$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1.3410  6.4343 -30.7135
[ CPUFloatType{1,3} ]
torch_tensor 
 100
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to &lt;code&gt;nn_linear()&lt;/code&gt; , &lt;code&gt;torch&lt;/code&gt; provides pretty much all the common layers you might hope for. But few tasks are solved by a single layer. How do you combine them? Or, in the usual lingo: How do you build &lt;em&gt;models&lt;/em&gt;?&lt;/p&gt;
&lt;h3 id="container-modules-models"&gt;Container modules (“models”)&lt;/h3&gt;
&lt;p&gt;Now, &lt;em&gt;models&lt;/em&gt; are just modules that contain other modules. For example, if all inputs are supposed to flow through the same nodes and along the same edges, then &lt;code&gt;nn_sequential()&lt;/code&gt; can be used to build a simple graph.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- nn_sequential(
    nn_linear(3, 16),
    nn_relu(),
    nn_linear(16, 1)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same technique as above to get an overview of all model parameters (two weight matrices and two bias vectors):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$`0.weight`
torch_tensor 
-0.1968 -0.1127 -0.0504
 0.0083  0.3125  0.0013
 0.4784 -0.2757  0.2535
-0.0898 -0.4706 -0.0733
-0.0654  0.5016  0.0242
 0.4855 -0.3980 -0.3434
-0.3609  0.1859 -0.4039
 0.2851  0.2809 -0.3114
-0.0542 -0.0754 -0.2252
-0.3175  0.2107 -0.2954
-0.3733  0.3931  0.3466
 0.5616 -0.3793 -0.4872
 0.0062  0.4168 -0.5580
 0.3174 -0.4867  0.0904
-0.0981 -0.0084  0.3580
 0.3187 -0.2954 -0.5181
[ CPUFloatType{16,3} ]

$`0.bias`
torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]

$`2.weight`
torch_tensor 
Columns 1 to 10-0.0908 -0.1786  0.0812 -0.0414 -0.0251 -0.1961  0.2326  0.0943 -0.0246  0.0748

Columns 11 to 16 0.2111 -0.1801 -0.0102 -0.0244  0.1223 -0.1958
[ CPUFloatType{1,16} ]

$`2.bias`
torch_tensor 
 0.2470
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To inspect an individual parameter, make use of its position in the sequential model. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model[[1]]$bias&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just like &lt;code&gt;nn_linear()&lt;/code&gt; above, this module can be called directly on data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out &amp;lt;- model(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On a composite module like this one, calling &lt;code&gt;backward()&lt;/code&gt; will backpropagate through all the layers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())

# e.g.
model[[1]]$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CPUFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And placing the composite module on the GPU will move all tensors there:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$cuda()
model[[1]]$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CUDAFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see how using &lt;code&gt;nn_sequential()&lt;/code&gt; can simplify our example network.&lt;/p&gt;
&lt;h2 id="simple-network-using-modules"&gt;Simple network using modules&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)


### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32

model &amp;lt;- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

### network parameters ---------------------------------------------------------

learning_rate &amp;lt;- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  
  y_pred &amp;lt;- model(x)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- (y_pred - y)$pow(2)$sum()
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation -------- 
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()
  
  # compute gradient of the loss w.r.t. all learnable parameters of the model
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON&amp;#39;T want to record
  # for automatic gradient computation
  # Update each parameter by its `grad`
  
  with_no_grad({
    model$parameters %&amp;gt;% purrr::walk(function(param) param$sub_(learning_rate * param$grad))
  })
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The forward pass looks a lot better now; however, we still loop through the model’s parameters and update each one by hand. Furthermore, you may be already be suspecting that &lt;code&gt;torch&lt;/code&gt; provides abstractions for common loss functions. In the next and last installment of this series, we’ll address both points, making use of &lt;code&gt;torch&lt;/code&gt; losses and optimizers. See you then!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">1c75e56e973fa95261f20a34e9b0ed79</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules</guid>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Introducing torch autograd</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd</link>
      <description>


&lt;p&gt;Last week, we saw how to code &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch"&gt;a simple network from scratch&lt;/a&gt;, using nothing but &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;tensors&lt;/em&gt;. Predictions, loss, gradients, weight updates – all these things we’ve been computing ourselves. Today, we make a significant change: Namely, we spare ourselves the cumbersome calculation of gradients, and have &lt;code&gt;torch&lt;/code&gt; do it for us.&lt;/p&gt;
&lt;p&gt;Prior to that though, let’s get some background.&lt;/p&gt;
&lt;h2 id="automatic-differentiation-with-autograd"&gt;Automatic differentiation with &lt;em&gt;autograd&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;torch&lt;/code&gt; uses a module called &lt;em&gt;autograd&lt;/em&gt; to&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;record operations performed on tensors, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;store what will have to be done to obtain the corresponding gradients, once we’re entering the backward pass.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These prospective actions are stored internally as functions, and when it’s time to compute the gradients, these functions are applied in order: Application starts from the output node, and calculated gradients are successively &lt;em&gt;propagated&lt;/em&gt; &lt;em&gt;back&lt;/em&gt; through the network. This is a form of &lt;em&gt;reverse mode automatic differentiation&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id="autograd-basics"&gt;&lt;em&gt;Autograd&lt;/em&gt; basics&lt;/h4&gt;
&lt;p&gt;As users, we can see a bit of the implementation. As a prerequisite for this “recording” to happen, tensors have to be created with &lt;code&gt;requires_grad = TRUE&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

x &amp;lt;- torch_ones(2, 2, requires_grad = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be clear, &lt;code&gt;x&lt;/code&gt; now is a tensor &lt;em&gt;with respect to which&lt;/em&gt; gradients have to be calculated – normally, a tensor representing a weight or a bias, not the input data &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. If we subsequently perform some operation on that tensor, assigning the result to &lt;code&gt;y&lt;/code&gt;,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y &amp;lt;- x$mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we find that &lt;code&gt;y&lt;/code&gt; now has a non-empty &lt;code&gt;grad_fn&lt;/code&gt; that tells &lt;code&gt;torch&lt;/code&gt; how to compute the gradient of &lt;code&gt;y&lt;/code&gt; with respect to &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MeanBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Actual &lt;em&gt;computation&lt;/em&gt; of gradients is triggered by calling &lt;code&gt;backward()&lt;/code&gt; on the output tensor.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After &lt;code&gt;backward()&lt;/code&gt; has been called, &lt;code&gt;x&lt;/code&gt; has a non-null field termed &lt;code&gt;grad&lt;/code&gt; that stores the gradient of &lt;code&gt;y&lt;/code&gt; with respect to &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With longer chains of computations, we can take a glance at how &lt;code&gt;torch&lt;/code&gt; builds up a graph of backward operations. Here is a slightly more complex example – feel free to skip if you’re not the type who just &lt;em&gt;has&lt;/em&gt; to peek into things for them to make sense.&lt;/p&gt;
&lt;h4 id="digging-deeper"&gt;Digging deeper&lt;/h4&gt;
&lt;p&gt;We build up a simple graph of tensors, with inputs &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; being connected to output &lt;code&gt;out&lt;/code&gt; by intermediaries &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x1 &amp;lt;- torch_ones(2, 2, requires_grad = TRUE)
x2 &amp;lt;- torch_tensor(1.1, requires_grad = TRUE)

y &amp;lt;- x1 * (x2 + 2)

z &amp;lt;- y$pow(2) * 3

out &amp;lt;- z$mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save memory, intermediate gradients are normally not being stored. Calling &lt;code&gt;retain_grad()&lt;/code&gt; on a tensor allows one to deviate from this default. Let’s do this here, for the sake of demonstration:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$retain_grad()

z$retain_grad()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can go backwards through the graph and inspect &lt;code&gt;torch&lt;/code&gt;’s action plan for backprop, starting from &lt;code&gt;out$grad_fn&lt;/code&gt;, like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for mean, the last operation executed
out$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MeanBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3
out$grad_fn$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
MulBackward1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for pow in z = y.pow(2) * 3
out$grad_fn$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
PowBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the multiplication in y = x * (x + 2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
MulBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the two branches of y = x * (x + 2),
# where the left branch is a leaf node (AccumulateGrad for x1)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
torch::autograd::AccumulateGrad
[[2]]
AddBackward1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# here we arrive at the other leaf node (AccumulateGrad for x2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
torch::autograd::AccumulateGrad&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now call &lt;code&gt;out$backward()&lt;/code&gt;, all tensors in the graph will have their respective gradients calculated.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward()

z$grad
y$grad
x2$grad
x1$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]
torch_tensor 
 4.6500  4.6500
 4.6500  4.6500
[ CPUFloatType{2,2} ]
torch_tensor 
 18.6000
[ CPUFloatType{1} ]
torch_tensor 
 14.4150  14.4150
 14.4150  14.4150
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this nerdy excursion, let’s see how &lt;em&gt;autograd&lt;/em&gt; makes our network simpler.&lt;/p&gt;
&lt;h2 id="the-simple-network-now-using-autograd"&gt;The simple network, now using &lt;em&gt;autograd&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;em&gt;autograd&lt;/em&gt;, we say good-bye to the tedious, error-prone process of coding backpropagation ourselves. A single method call does it all: &lt;code&gt;loss$backward()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;torch&lt;/code&gt; keeping track of operations as required, we don’t even have to explicitly name the intermediate tensors any more. We can code forward pass, loss calculation, and backward pass in just three lines:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_pred &amp;lt;- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)
  
loss &amp;lt;- (y_pred - y)$pow(2)$sum()

loss$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the complete code. We’re at an intermediate stage: We still manually compute the forward pass and the loss, and we still manually update the weights. Due to the latter, there is something I need to explain. But I’ll let you check out the new version first:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)


### initialize weights ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32
# weights connecting input to hidden layer
w1 &amp;lt;- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 &amp;lt;- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 &amp;lt;- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 &amp;lt;- torch_zeros(1, d_out, requires_grad = TRUE)

### network parameters ---------------------------------------------------------

learning_rate &amp;lt;- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  ### -------- Forward pass --------
  
  y_pred &amp;lt;- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- (y_pred - y)$pow(2)$sum()
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON&amp;#39;T 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 &amp;lt;- w1$sub_(learning_rate * w1$grad)
     w2 &amp;lt;- w2$sub_(learning_rate * w2$grad)
     b1 &amp;lt;- b1$sub_(learning_rate * b1$grad)
     b2 &amp;lt;- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they&amp;#39;d accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As explained above, after &lt;code&gt;some_tensor$backward()&lt;/code&gt;, all tensors preceding it in the graph&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; will have their &lt;code&gt;grad&lt;/code&gt; fields populated. We make use of these fields to update the weights. But now that &lt;em&gt;autograd&lt;/em&gt; is “on”, whenever we execute an operation we &lt;em&gt;don’t&lt;/em&gt; want recorded for backprop, we need to explicitly exempt it: This is why we wrap the weight updates in a call to &lt;code&gt;with_no_grad()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While this is something you may file under “nice to know” – after all, once we arrive at the last post in the series, this manual updating of weights will be gone – the idiom of &lt;em&gt;zeroing gradients&lt;/em&gt; is here to stay: Values stored in &lt;code&gt;grad&lt;/code&gt; fields accumulate; whenever we’re done using them, we need to zero them out before reuse.&lt;/p&gt;
&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;
&lt;p&gt;So where do we stand? We started out coding a network completely from scratch, making use of nothing but &lt;code&gt;torch&lt;/code&gt; tensors. Today, we got significant help from &lt;em&gt;autograd&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But we’re still manually updating the weights, – and aren’t deep learning frameworks known to provide abstractions (“layers”, or: “modules”) on top of tensor computations …?&lt;/p&gt;
&lt;p&gt;We address both issues in the follow-up installments. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Unless we &lt;em&gt;want&lt;/em&gt; to change the data, as when generating adversarial examples.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;All that have &lt;code&gt;requires_grad&lt;/code&gt; set to &lt;code&gt;TRUE&lt;/code&gt;, to be precise.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">19aac2b24610807272cc3c1ccd6b0e8d</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd</guid>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Getting familiar with torch tensors</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch</link>
      <description>


&lt;p&gt;Two days ago, I introduced &lt;a href="https://github.com/mlverse/torch"&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/a&gt;, an R package that provides the native functionality that is brought to Python users by &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;. In that post, I assumed basic familiarity with TensorFlow/Keras. Consequently, I portrayed &lt;code&gt;torch&lt;/code&gt; in a way I figured would be helpful to someone who “grew up” with the Keras way of training a model: Aiming to focus on differences, yet not lose sight of the overall process.&lt;/p&gt;
&lt;p&gt;This post now changes perspective. We code a simple neural network “from scratch”, making use of just one of &lt;code&gt;torch&lt;/code&gt;’s building blocks: &lt;em&gt;tensors&lt;/em&gt;. This network will be as “raw” (low-level) as can be. (For the less math-inclined people among us, it may serve as a refresher of what’s actually going on beneath all those convenience tools they built for us. But the real purpose is to illustrate what can be done with tensors alone.)&lt;/p&gt;
&lt;p&gt;Subsequently, three posts will progressively show how to reduce the effort – noticeably right from the start, enormously once we finish. At the end of this mini-series, you will have seen how automatic differentiation works in &lt;code&gt;torch&lt;/code&gt;, how to use &lt;code&gt;module&lt;/code&gt;s (layers, in &lt;code&gt;keras&lt;/code&gt; speak, and compositions thereof), and optimizers. By then, you’ll have a lot of the background desirable when applying &lt;code&gt;torch&lt;/code&gt; to real-world tasks.&lt;/p&gt;
&lt;p&gt;This post will be the longest, since there is a lot to learn about tensors: How to create them; how to manipulate their contents and/or modify their shapes; how to convert them to R arrays, matrices or vectors; and of course, given the omnipresent need for speed: how to get all those operations executed on the GPU. Once we’ve cleared that agenda, we code the aforementioned little network, seeing all those aspects in action.&lt;/p&gt;
&lt;h2 id="tensors"&gt;Tensors&lt;/h2&gt;
&lt;h3 id="creation"&gt;Creation&lt;/h3&gt;
&lt;p&gt;Tensors may be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of types &lt;code&gt;float&lt;/code&gt; and &lt;code&gt;bool&lt;/code&gt;, respectively:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
# a 1d vector of length 2
t &amp;lt;- torch_tensor(c(1, 2))
t

# also 1d, but of type boolean
t &amp;lt;- torch_tensor(c(TRUE, FALSE))
t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
 2
[ CPUFloatType{2} ]

torch_tensor 
 1
 0
[ CPUBoolType{2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here are two ways to create two-dimensional tensors (matrices). Note how in the second approach, you need to specify &lt;code&gt;byrow = TRUE&lt;/code&gt; in the call to &lt;code&gt;matrix()&lt;/code&gt; to get values arranged in row-major order.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# a 3x3 tensor (matrix)
t &amp;lt;- torch_tensor(rbind(c(1,2,0), c(3,0,0), c(4,5,6)))
t

# also 3x3
t &amp;lt;- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1  2  0
 3  0  0
 4  5  6
[ CPUFloatType{3,3} ]

torch_tensor 
 1  2  3
 4  5  6
 7  8  9
[ CPULongType{3,3} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In higher dimensions especially, it can be easier to specify the type of tensor abstractly, as in: “give me a tensor of &amp;lt;…&amp;gt; of shape n1 x n2”, where &amp;lt;…&amp;gt; could be “zeros”; or “ones”; or, say, “values drawn from a standard normal distribution”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# a 3x3 tensor of standard-normally distributed values
t &amp;lt;- torch_randn(3, 3)
t

# a 4x2x2 (3d) tensor of zeroes
t &amp;lt;- torch_zeros(4, 2, 2)
t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
-2.1563  1.7085  0.5245
 0.8955 -0.6854  0.2418
 0.4193 -0.7742 -1.0399
[ CPUFloatType{3,3} ]

torch_tensor 
(1,.,.) = 
  0  0
  0  0

(2,.,.) = 
  0  0
  0  0

(3,.,.) = 
  0  0
  0  0

(4,.,.) = 
  0  0
  0  0
[ CPUFloatType{4,2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many similar functions exist, including, e.g., &lt;code&gt;torch_arange()&lt;/code&gt; to create a tensor holding a sequence of evenly spaced values, &lt;code&gt;torch_eye()&lt;/code&gt; which returns an identity matrix, and &lt;code&gt;torch_logspace()&lt;/code&gt; which fills a specified range with a list of values spaced logarithmically.&lt;/p&gt;
&lt;p&gt;If no &lt;code&gt;dtype&lt;/code&gt; argument is specified, &lt;code&gt;torch&lt;/code&gt; will infer the data type from the passed-in value(s). For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(c(3, 5, 7))
t$dtype

t &amp;lt;- torch_tensor(1L)
t$dtype&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_Float
torch_Long&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we can explicitly request a different &lt;code&gt;dtype&lt;/code&gt; if we want:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(2, dtype = torch_double())
t$dtype&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_Double&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;torch&lt;/code&gt; tensors live on a &lt;em&gt;device&lt;/em&gt;. By default, this will be the CPU:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t$device&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_device(type=&amp;#39;cpu&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we could also define a tensor to live on the GPU:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(2, device = &amp;quot;cuda&amp;quot;)
t$device&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_device(type=&amp;#39;cuda&amp;#39;, index=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll talk more about devices below.&lt;/p&gt;
&lt;p&gt;There is another very important parameter to the tensor-creation functions: &lt;code&gt;requires_grad&lt;/code&gt;. Here though, I need to ask for your patience: This one will prominently figure in the follow-up post.&lt;/p&gt;
&lt;h3 id="conversion-to-built-in-r-data-types"&gt;Conversion to built-in R data types&lt;/h3&gt;
&lt;p&gt;To convert &lt;code&gt;torch&lt;/code&gt; tensors to R, use &lt;code&gt;as_array()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
as_array(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on whether the tensor is one-, two-, or three-dimensional, the resulting R object will be a vector, a matrix, or an array:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(c(1, 2, 3))
as_array(t) %&amp;gt;% class()

t &amp;lt;- torch_ones(c(2, 2))
as_array(t) %&amp;gt;% class()

t &amp;lt;- torch_ones(c(2, 2, 2))
as_array(t) %&amp;gt;% class()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;numeric&amp;quot;

[1] &amp;quot;matrix&amp;quot; &amp;quot;array&amp;quot; 

[1] &amp;quot;array&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For one-dimensional and two-dimensional tensors, it is also possible to use &lt;code&gt;as.integer()&lt;/code&gt; / &lt;code&gt;as.matrix()&lt;/code&gt;. (One reason you might want to do this is to have more self-documenting code.)&lt;/p&gt;
&lt;p&gt;If a tensor currently lives on the GPU, you need to move it to the CPU first:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(2, device = &amp;quot;cuda&amp;quot;)
as.integer(t$cpu())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="indexing-and-slicing-tensors"&gt;Indexing and slicing tensors&lt;/h3&gt;
&lt;p&gt;Often, we want to retrieve not a complete tensor, but only some of the values it holds, or even just a single value. In these cases, we talk about &lt;em&gt;slicing&lt;/em&gt; and &lt;em&gt;indexing&lt;/em&gt;, respectively.&lt;/p&gt;
&lt;p&gt;In R, these operations are 1-based, meaning that when we specify offsets, we assume for the very first element in an array to reside at offset &lt;code&gt;1&lt;/code&gt;. The same behavior was implemented for &lt;code&gt;torch&lt;/code&gt;. Thus, a lot of the functionality described in this section should feel intuitive.&lt;/p&gt;
&lt;p&gt;The way I’m organizing this section is the following. We’ll inspect the intuitive parts first, where by intuitive I mean: intuitive to the R user who has not yet worked with Python’s &lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt;. Then come things which, to this user, may look more surprising, but will turn out to be pretty useful.&lt;/p&gt;
&lt;h4 id="indexing-and-slicing-the-r-like-part"&gt;Indexing and slicing: the R-like part&lt;/h4&gt;
&lt;p&gt;None of these should be overly surprising:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(rbind(c(1,2,3), c(4,5,6)))
t

# a single value
t[1, 1]

# first row, all columns
t[1, ]

# first row, a subset of columns
t[1, 1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]

torch_tensor 
1
[ CPUFloatType{} ]

torch_tensor 
 1
 2
 3
[ CPUFloatType{3} ]

torch_tensor 
 1
 2
[ CPUFloatType{2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how, just as in R, singleton dimensions are dropped:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(rbind(c(1,2,3), c(4,5,6)))

# 2x3
t$size() 

# just a single row: will be returned as a vector
t[1, 1:2]$size() 

# a single element
t[1, 1]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 3

[1] 2

integer(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just like in R, you can specify &lt;code&gt;drop = FALSE&lt;/code&gt; to keep those dimensions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t[1, 1:2, drop = FALSE]$size()

t[1, 1, drop = FALSE]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 2

[1] 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="indexing-and-slicing-what-to-look-out-for"&gt;Indexing and slicing: What to look out for&lt;/h4&gt;
&lt;p&gt;Whereas R uses negative numbers to remove elements at specified positions, in &lt;code&gt;torch&lt;/code&gt; negative values indicate that we start counting from the end of a tensor – with &lt;code&gt;-1&lt;/code&gt; pointing to its last element:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(rbind(c(1,2,3), c(4,5,6)))

t[1, -1]

t[ , -2:-1] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
3
[ CPUFloatType{} ]

torch_tensor 
 2  3
 5  6
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a feature you might know from NumPy. Same with the following.&lt;/p&gt;
&lt;p&gt;When the slicing expression &lt;code&gt;m:n&lt;/code&gt; is augmented by another colon and a third number – &lt;code&gt;m:n:o&lt;/code&gt; –, we will take every &lt;code&gt;o&lt;/code&gt;th item from the range specified by &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_tensor(1:10)
t[2:10:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  2
  4
  6
  8
 10
[ CPULongType{5} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes we don’t know how many dimensions a tensor has, but we do know what to do with the final dimension, or the first one. To subsume all others, we can use &lt;code&gt;..&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t &amp;lt;- torch_randint(-7, 7, size = c(2, 2, 2))
t

t[.., 1]

t[2, ..]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
(1,.,.) = 
  2 -2
 -5  4

(2,.,.) = 
  0  4
 -3 -1
[ CPUFloatType{2,2,2} ]

torch_tensor 
 2 -5
 0 -3
[ CPUFloatType{2,2} ]

torch_tensor 
 0  4
-3 -1
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we move on to a topic that, in practice, is just as indispensable as slicing: changing tensor &lt;em&gt;shapes&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="reshaping-tensors"&gt;Reshaping tensors&lt;/h3&gt;
&lt;p&gt;Changes in shape can occur in two fundamentally different ways. Seeing how “reshape” really means: &lt;em&gt;keep the values but modify their layout&lt;/em&gt;, we could either alter how they’re arranged physically, or keep the physical structure as-is and just change the “mapping” (a semantic change, as it were).&lt;/p&gt;
&lt;p&gt;In the first case, storage will have to be allocated for two tensors, source and target, and elements will be copied from the latter to the former. In the second, physically there will be just a single tensor, referenced by two logical entities with distinct metadata.&lt;/p&gt;
&lt;p&gt;Not surprisingly, for performance reasons, the second operation is preferred.&lt;/p&gt;
&lt;h4 id="zero-copy-reshaping"&gt;Zero-copy reshaping&lt;/h4&gt;
&lt;p&gt;We start with zero-copy methods, as we’ll want to use them whenever we can.&lt;/p&gt;
&lt;p&gt;A special case often seen in practice is adding or removing a singleton dimension.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;unsqueeze()&lt;/code&gt; adds a dimension of size &lt;code&gt;1&lt;/code&gt; at a position specified by &lt;code&gt;dim&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randint(low = 3, high = 7, size = c(3, 3, 3))
t1$size()

t2 &amp;lt;- t1$unsqueeze(dim = 1)
t2$size()

t3 &amp;lt;- t1$unsqueeze(dim = 2)
t3$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 3 3

[1] 1 3 3 3

[1] 3 1 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conversely, &lt;code&gt;squeeze()&lt;/code&gt; removes singleton dimensions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t4 &amp;lt;- t3$squeeze()
t4$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same could be accomplished with &lt;code&gt;view()&lt;/code&gt;. &lt;code&gt;view()&lt;/code&gt;, however, is much more general, in that it allows you to reshape the data to any valid dimensionality. (Valid meaning: The number of elements stays the same.)&lt;/p&gt;
&lt;p&gt;Here we have a &lt;code&gt;3x2&lt;/code&gt; tensor that is reshaped to size &lt;code&gt;2x3&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))
t1

t2 &amp;lt;- t1$view(c(2, 3))
t2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note how this is different from matrix transposition.)&lt;/p&gt;
&lt;p&gt;Instead of going from two to three dimensions, we can flatten the matrix to a vector.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t4 &amp;lt;- t1$view(c(-1, 6))

t4$size()

t4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 6

torch_tensor 
 1  2  3  4  5  6
[ CPUFloatType{1,6} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In contrast to indexing operations, this does not drop dimensions.&lt;/p&gt;
&lt;p&gt;Like we said above, operations like &lt;code&gt;squeeze()&lt;/code&gt; or &lt;code&gt;view()&lt;/code&gt; do not make copies. Or, put differently: The output tensor shares storage with the input tensor. We can in fact verify this ourselves:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1$storage()$data_ptr()

t2$storage()$data_ptr()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0x5648d02ac800&amp;quot;

[1] &amp;quot;0x5648d02ac800&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What’s different is the storage &lt;em&gt;metadata&lt;/em&gt; &lt;code&gt;torch&lt;/code&gt; keeps about both tensors. Here, the relevant information is the &lt;em&gt;stride&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;A tensor’s &lt;code&gt;stride()&lt;/code&gt; method tracks, &lt;em&gt;for every dimension&lt;/em&gt;, how many elements have to be traversed to arrive at its next element (row or column, in two dimensions). For &lt;code&gt;t1&lt;/code&gt; above, of shape &lt;code&gt;3x2&lt;/code&gt;, we have to skip over 2 items to arrive at the next row. To arrive at the next column though, in every row we just have to skip a single entry:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1$stride()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;code&gt;t2&lt;/code&gt;, of shape &lt;code&gt;3x2&lt;/code&gt;, the distance between column elements is the same, but the distance between rows is now 3:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t2$stride()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While zero-copy operations are optimal, there are cases where they won’t work.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;view()&lt;/code&gt;, this can happen when a tensor was obtained via an operation – other than &lt;code&gt;view()&lt;/code&gt; itself – that itself has already modified the &lt;em&gt;stride&lt;/em&gt;. One example would be &lt;code&gt;transpose()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))
t1
t1$stride()

t2 &amp;lt;- t1$t()
t2
t2$stride()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

[1] 2 1

torch_tensor 
 1  3  5
 2  4  6
[ CPUFloatType{2,3} ]

[1] 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt; lingo, tensors – like &lt;code&gt;t2&lt;/code&gt; – that re-use existing storage (and just read it differently), are said not to be “contiguous”&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. One way to reshape them is to use &lt;code&gt;contiguous()&lt;/code&gt; on them before. We’ll see this in the next subsection.&lt;/p&gt;
&lt;h4 id="reshape-with-copy"&gt;Reshape with copy&lt;/h4&gt;
&lt;p&gt;In the following snippet, trying to reshape &lt;code&gt;t2&lt;/code&gt; using &lt;code&gt;view()&lt;/code&gt; fails, as it already carries information indicating that the underlying data should not be read in physical order.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))

t2 &amp;lt;- t1$t()

t2$view(6) # error!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in (function (self, size)  : 
  view size is not compatible with input tensor&amp;#39;s size and stride (at least one dimension spans across two contiguous subspaces).
  Use .reshape(...) instead. (view at ../aten/src/ATen/native/TensorShape.cpp:1364)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we first call &lt;code&gt;contiguous()&lt;/code&gt; on it, a &lt;em&gt;new tensor&lt;/em&gt; is created, which may then be (virtually) reshaped using &lt;code&gt;view()&lt;/code&gt;.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t3 &amp;lt;- t2$contiguous()

t3$view(6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
 3
 5
 2
 4
 6
[ CPUFloatType{6} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we can use &lt;code&gt;reshape()&lt;/code&gt;. &lt;code&gt;reshape()&lt;/code&gt; defaults to &lt;code&gt;view()&lt;/code&gt;-like behavior if possible; otherwise it will create a physical copy.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t2$storage()$data_ptr()

t4 &amp;lt;- t2$reshape(6)

t4$storage()$data_ptr()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0x5648d49b4f40&amp;quot;

[1] &amp;quot;0x5648d2752980&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="operations-on-tensors"&gt;Operations on tensors&lt;/h3&gt;
&lt;p&gt;Unsurprisingly, &lt;code&gt;torch&lt;/code&gt; provides a bunch of mathematical operations on tensors; we’ll see some of them in the network code below, and you’ll encounter lots more when you continue your &lt;code&gt;torch&lt;/code&gt; journey. Here, we quickly take a look at the overall tensor method semantics.&lt;/p&gt;
&lt;p&gt;Tensor methods normally return references to new objects. Here, we add to &lt;code&gt;t1&lt;/code&gt; a clone of itself:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))
t2 &amp;lt;- t1$clone()

t1$add(t2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  2   4
  6   8
 10  12
[ CPUFloatType{3,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this process, &lt;code&gt;t1&lt;/code&gt; has not been modified:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many tensor methods have variants for mutating operations. These all carry a trailing underscore:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1$add_(t1)

# now t1 has been modified
t1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]

torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you can of course assign the new object to a new reference variable:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t3 &amp;lt;- t1$add(t1)

t3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  8  16
 24  32
 40  48
[ CPUFloatType{3,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one thing we need to discuss before we wrap up our introduction to tensors: How can we have all those operations executed on the GPU?&lt;/p&gt;
&lt;h2 id="running-on-gpu"&gt;Running on GPU&lt;/h2&gt;
&lt;p&gt;To check if your GPU(s) is/are visible to torch, run&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cuda_is_available()

cuda_device_count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] TRUE

[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tensors may be requested to live on the GPU right at creation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;device &amp;lt;- torch_device(&amp;quot;cuda&amp;quot;)

t &amp;lt;- torch_ones(c(2, 2), device = device) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, they can be moved between devices at any time:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t2 &amp;lt;- t$cuda()
t2$device&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_device(type=&amp;#39;cuda&amp;#39;, index=0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;t3 &amp;lt;- t2$cpu()
t3$device&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_device(type=&amp;#39;cpu&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for our discussion on tensors — almost. There is one &lt;code&gt;torch&lt;/code&gt; feature that, although related to tensor operations, deserves special mention. It is called broadcasting, and “bilingual” (R + Python) users will know it from NumPy.&lt;/p&gt;
&lt;h2 id="broadcasting"&gt;Broadcasting&lt;/h2&gt;
&lt;p&gt;We often have to perform operations on tensors with shapes that don’t match exactly.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, we can add a scalar to a tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))

t1 + 22&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 23.1097  21.4425  22.7732  22.2973  21.4128
 22.6936  21.8829  21.1463  21.6781  21.0827
 22.5672  21.2210  21.2344  23.1154  20.5004
[ CPUFloatType{3,5} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same will work if we add tensor of size &lt;code&gt;1&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))

t1 + torch_tensor(c(22))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding tensors of different sizes normally won’t work:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))
t2 &amp;lt;- torch_randn(c(5,5))

t1$add(t2) # error&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in (function (self, other, alpha)  : 
  The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1 (infer_size at ../aten/src/ATen/ExpandUtils.cpp:24)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is what is meant by &lt;em&gt;broadcasting&lt;/em&gt;. The way it works in &lt;code&gt;torch&lt;/code&gt; is not just inspired by, but actually identical to that of NumPy.&lt;/p&gt;
&lt;p&gt;The rules are:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;We align array shapes, &lt;em&gt;starting from the right&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Say we have two tensors, one of size &lt;code&gt;8x1x6x1&lt;/code&gt;, the other of size &lt;code&gt;7x1x5&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here they are, right-aligned:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;# t1, shape:     8  1  6  1
# t2, shape:        7  1  5&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Starting to look from the right&lt;/em&gt;, the sizes along aligned axes either have to match exactly, or one of them has to be equal to &lt;code&gt;1&lt;/code&gt;: in which case the latter is &lt;em&gt;broadcast&lt;/em&gt; to the larger one.&lt;/p&gt;
&lt;p&gt;In the above example, this is the case for the second-from-last dimension. This now gives&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;# t1, shape:     8  1  6  1
# t2, shape:        7  6  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;, with broadcasting happening in &lt;code&gt;t2&lt;/code&gt;.&lt;/p&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;If on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a size of &lt;code&gt;1&lt;/code&gt; in that place, in which case broadcasting will happen as stated in (2).&lt;/p&gt;
&lt;p&gt;This is the case with &lt;code&gt;t1&lt;/code&gt;’s leftmost dimension. First, there is a virtual expansion&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;# t1, shape:     8  1  6  1
# t2, shape:     1  7  1  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then, broadcasting happens:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# t1, shape:     8  1  6  1
# t2, shape:     8  7  1  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to these rules, our above example&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))
t2 &amp;lt;- torch_randn(c(5,5))

t1$add(t2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;could be modified in various ways that would allow for adding two tensors.&lt;/p&gt;
&lt;p&gt;For example, if &lt;code&gt;t2&lt;/code&gt; were &lt;code&gt;1x5&lt;/code&gt;, it would only need to get broadcast to size &lt;code&gt;3x5&lt;/code&gt; before the addition operation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))
t2 &amp;lt;- torch_randn(c(1,5))

t1$add(t2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
-1.0505  1.5811  1.1956 -0.0445  0.5373
 0.0779  2.4273  2.1518 -0.6136  2.6295
 0.1386 -0.6107 -1.2527 -1.3256 -0.1009
[ CPUFloatType{3,5} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If it were of size &lt;code&gt;5&lt;/code&gt;, a virtual leading dimension would be added, and then, the same broadcasting would take place as in the previous case.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(3,5))
t2 &amp;lt;- torch_randn(c(5))

t1$add(t2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
-1.4123  2.1392 -0.9891  1.1636 -1.4960
 0.8147  1.0368 -2.6144  0.6075 -2.0776
-2.3502  1.4165  0.4651 -0.8816 -1.0685
[ CPUFloatType{3,5} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a more complex example. Broadcasting how happens both in &lt;code&gt;t1&lt;/code&gt; and in &lt;code&gt;t2&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_randn(c(1,5))
t2 &amp;lt;- torch_randn(c(3,1))

t1$add(t2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1.2274  1.1880  0.8531  1.8511 -0.0627
 0.2639  0.2246 -0.1103  0.8877 -1.0262
-1.5951 -1.6344 -1.9693 -0.9713 -2.8852
[ CPUFloatType{3,5} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a nice concluding example, through broadcasting an outer product can be computed like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;t1 &amp;lt;- torch_tensor(c(0, 10, 20, 30))

t2 &amp;lt;- torch_tensor(c(1, 2, 3))

t1$view(c(4,1)) * t2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  0   0   0
 10  20  30
 20  40  60
 30  60  90
[ CPUFloatType{4,3} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, we really get to implementing that neural network!&lt;/p&gt;
&lt;h2 id="a-simple-neural-network-using-torch-tensors"&gt;A simple neural network using &lt;code&gt;torch&lt;/code&gt; tensors&lt;/h2&gt;
&lt;p&gt;Our task, which we approach in a low-level way today but considerably simplify in upcoming installments, consists of regressing a single target datum based on three input variables.&lt;/p&gt;
&lt;p&gt;We directly use &lt;code&gt;torch&lt;/code&gt; to simulate some data.&lt;/p&gt;
&lt;h4 id="toy-data"&gt;Toy data&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
# input
x &amp;lt;- torch_randn(n, d_in)
# target
y &amp;lt;- x[, 1, drop = FALSE] * 0.2 -
  x[, 2, drop = FALSE] * 1.3 -
  x[, 3, drop = FALSE] * 0.5 +
  torch_randn(n, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to initialize the network’s weights. We’ll have one hidden layer, with &lt;code&gt;32&lt;/code&gt; units. The output layer’s size, being determined by the task, is equal to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="initialize-weights"&gt;Initialize weights&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;# dimensionality of hidden layer
d_hidden &amp;lt;- 32

# weights connecting input to hidden layer
w1 &amp;lt;- torch_randn(d_in, d_hidden)
# weights connecting hidden to output layer
w2 &amp;lt;- torch_randn(d_hidden, d_out)

# hidden layer bias
b1 &amp;lt;- torch_zeros(1, d_hidden)
# output layer bias
b2 &amp;lt;- torch_zeros(1, d_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the training loop proper. The training loop here really &lt;em&gt;is&lt;/em&gt; the network.&lt;/p&gt;
&lt;h4 id="training-loop"&gt;Training loop&lt;/h4&gt;
&lt;p&gt;In each iteration (“epoch”), the training loop does four things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;runs through the network, computing predictions (&lt;em&gt;forward pass)&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;compares those predictions to the ground truth and quantify the loss&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;runs backwards through the network, computing the gradients that indicate how the weights should be changed&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;updates the weights, making use of the requested learning rate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the template we’re going to fill:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (t in 1:200) {
    
    ### -------- Forward pass -------- 
    
    # here we&amp;#39;ll compute the prediction
    
    
    ### -------- compute loss -------- 
    
    # here we&amp;#39;ll compute the sum of squared errors
    

    ### -------- Backpropagation -------- 
    
    # here we&amp;#39;ll pass through the network, calculating the required gradients
    

    ### -------- Update weights -------- 
    
    # here we&amp;#39;ll update the weights, subtracting portion of the gradients 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The forward pass effectuates two affine transformations, one each for the hidden and output layers. In-between, ReLU activation is applied:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;  # compute pre-activations of hidden layers (dim: 100 x 32)
  # torch_mm does matrix multiplication
  h &amp;lt;- x$mm(w1) + b1
  
  # apply activation function (dim: 100 x 32)
  # torch_clamp cuts off values below/above given thresholds
  h_relu &amp;lt;- h$clamp(min = 0)
  
  # compute output (dim: 100 x 1)
  y_pred &amp;lt;- h_relu$mm(w2) + b2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our loss here is mean squared error:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;  loss &amp;lt;- as.numeric((y_pred - y)$pow(2)$sum())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculating gradients the manual way is a bit tedious&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, but it can be done:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;  # gradient of loss w.r.t. prediction (dim: 100 x 1)
  grad_y_pred &amp;lt;- 2 * (y_pred - y)
  # gradient of loss w.r.t. w2 (dim: 32 x 1)
  grad_w2 &amp;lt;- h_relu$t()$mm(grad_y_pred)
  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)
  grad_h_relu &amp;lt;- grad_y_pred$mm(w2$t())
  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)
  grad_h &amp;lt;- grad_h_relu$clone()
  
  grad_h[h &amp;lt; 0] &amp;lt;- 0
  
  # gradient of loss w.r.t. b2 (shape: ())
  grad_b2 &amp;lt;- grad_y_pred$sum()
  
  # gradient of loss w.r.t. w1 (dim: 3 x 32)
  grad_w1 &amp;lt;- x$t()$mm(grad_h)
  # gradient of loss w.r.t. b1 (shape: (32, ))
  grad_b1 &amp;lt;- grad_h$sum(dim = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step then uses the calculated gradients to update the weights:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;  learning_rate &amp;lt;- 1e-4
  
  w2 &amp;lt;- w2 - learning_rate * grad_w2
  b2 &amp;lt;- b2 - learning_rate * grad_b2
  w1 &amp;lt;- w1 - learning_rate * grad_w1
  b1 &amp;lt;- b1 - learning_rate * grad_b1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use these snippets to fill in the gaps in the above template, and give it a try!&lt;/p&gt;
&lt;h4 id="putting-it-all-together"&gt;Putting it all together&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;-
  x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)


### initialize weights ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32
# weights connecting input to hidden layer
w1 &amp;lt;- torch_randn(d_in, d_hidden)
# weights connecting hidden to output layer
w2 &amp;lt;- torch_randn(d_hidden, d_out)

# hidden layer bias
b1 &amp;lt;- torch_zeros(1, d_hidden)
# output layer bias
b2 &amp;lt;- torch_zeros(1, d_out)

### network parameters ---------------------------------------------------------

learning_rate &amp;lt;- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  ### -------- Forward pass --------
  
  # compute pre-activations of hidden layers (dim: 100 x 32)
  h &amp;lt;- x$mm(w1) + b1
  # apply activation function (dim: 100 x 32)
  h_relu &amp;lt;- h$clamp(min = 0)
  # compute output (dim: 100 x 1)
  y_pred &amp;lt;- h_relu$mm(w2) + b2
  
  ### -------- compute loss --------

  loss &amp;lt;- as.numeric((y_pred - y)$pow(2)$sum())
  
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss, &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation --------
  
  # gradient of loss w.r.t. prediction (dim: 100 x 1)
  grad_y_pred &amp;lt;- 2 * (y_pred - y)
  # gradient of loss w.r.t. w2 (dim: 32 x 1)
  grad_w2 &amp;lt;- h_relu$t()$mm(grad_y_pred)
  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)
  grad_h_relu &amp;lt;- grad_y_pred$mm(
    w2$t())
  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)
  grad_h &amp;lt;- grad_h_relu$clone()
  
  grad_h[h &amp;lt; 0] &amp;lt;- 0
  
  # gradient of loss w.r.t. b2 (shape: ())
  grad_b2 &amp;lt;- grad_y_pred$sum()
  
  # gradient of loss w.r.t. w1 (dim: 3 x 32)
  grad_w1 &amp;lt;- x$t()$mm(grad_h)
  # gradient of loss w.r.t. b1 (shape: (32, ))
  grad_b1 &amp;lt;- grad_h$sum(dim = 1)
  
  ### -------- Update weights --------
  
  w2 &amp;lt;- w2 - learning_rate * grad_w2
  b2 &amp;lt;- b2 - learning_rate * grad_b2
  w1 &amp;lt;- w1 - learning_rate * grad_w1
  b1 &amp;lt;- b1 - learning_rate * grad_b1
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:  10     Loss:  352.3585 
Epoch:  20     Loss:  219.3624 
Epoch:  30     Loss:  155.2307 
Epoch:  40     Loss:  124.5716 
Epoch:  50     Loss:  109.2687 
Epoch:  60     Loss:  100.1543 
Epoch:  70     Loss:  94.77817 
Epoch:  80     Loss:  91.57003 
Epoch:  90     Loss:  89.37974 
Epoch:  100    Loss:  87.64617 
Epoch:  110    Loss:  86.3077 
Epoch:  120    Loss:  85.25118 
Epoch:  130    Loss:  84.37959 
Epoch:  140    Loss:  83.44133 
Epoch:  150    Loss:  82.60386 
Epoch:  160    Loss:  81.85324 
Epoch:  170    Loss:  81.23454 
Epoch:  180    Loss:  80.68679 
Epoch:  190    Loss:  80.16555 
Epoch:  200    Loss:  79.67953 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks like it worked pretty well! It also should have fulfilled its purpose: Showing what you can achieve using &lt;code&gt;torch&lt;/code&gt; tensors alone. In case you didn’t feel like going through the backprop logic with too much enthusiasm, don’t worry: In the next installment, this will get significantly less cumbersome. See you then!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Although the assumption may be tempting, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual language.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;For correctness’ sake, &lt;code&gt;contiguous()&lt;/code&gt; will only make a copy if the tensor it is called on is &lt;em&gt;not contiguous already.&lt;/em&gt;&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Just to avoid any misunderstandings: In the next installment, this will be very first thing rendered obsolete by &lt;code&gt;torch&lt;/code&gt;’s automatic differentiation capabilities.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">a756f0ee33da7707c6a78ec212dd0d30</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch</guid>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/images/pic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.4: Weighted Sampling, Tidyr Verbs, Robust Scaler, RAPIDS, and more</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</link>
      <description>


&lt;p&gt;&lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; 1.4 is now available on &lt;a href="https://cran.r-project.org/web/packages/sparklyr/index.html"&gt;CRAN&lt;/a&gt;! To install &lt;code&gt;sparklyr&lt;/code&gt; 1.4 from CRAN, run&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this blog post, we will showcase the following much-anticipated new functionalities from the &lt;code&gt;sparklyr&lt;/code&gt; 1.4 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#parallelized-weighted-sampling"&gt;Parallelized Weighted Sampling&lt;/a&gt; with Spark&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="#tidyr-verbs"&gt;Tidyr Verbs&lt;/a&gt; on Spark Dataframes&lt;/li&gt;
&lt;li&gt;&lt;a href="#robust-scaler"&gt;&lt;code&gt;ft_robust_scaler&lt;/code&gt;&lt;/a&gt; as the R interface for &lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; from Spark 3.0&lt;/li&gt;
&lt;li&gt;Option for enabling &lt;a href="#rapids"&gt;&lt;code&gt;RAPIDS&lt;/code&gt;&lt;/a&gt; GPU acceleration plugin in &lt;code&gt;spark_connect()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#higher-order-functions-and-dplyr-related-improvements"&gt;Higher-order functions and &lt;code&gt;dplyr&lt;/code&gt;-related improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="parallelized-weighted-sampling"&gt;Parallelized Weighted Sampling&lt;/h2&gt;
&lt;p&gt;Readers familiar with &lt;code&gt;dplyr::sample_n()&lt;/code&gt; and &lt;code&gt;dplyr::sample_frac()&lt;/code&gt; functions may have noticed that both of them support weighted-sampling use cases on R dataframes, e.g.,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dplyr::sample_n(mtcars, size = 3, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;               mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Fiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
Merc 280C     17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dplyr::sample_frac(mtcars, size = 0.1, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Honda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Merc 450SE  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
Fiat X1-9   27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will select some random subset of &lt;code&gt;mtcars&lt;/code&gt; using the &lt;code&gt;mpg&lt;/code&gt; attribute as the sampling weight for each row. If &lt;code&gt;replace = FALSE&lt;/code&gt; is set, then a row is removed from the sampling population once it gets selected, whereas when setting &lt;code&gt;replace = TRUE&lt;/code&gt;, each row will always stay in the sampling population and can be selected multiple times.&lt;/p&gt;
&lt;p&gt;Now the exact same use cases are supported for Spark dataframes in &lt;code&gt;sparklyr&lt;/code&gt; 1.4! For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
mtcars_sdf &amp;lt;- copy_to(sc, mtcars, repartition = 4L)

dplyr::sample_n(mtcars_sdf, size = 5, weight = mpg, replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will return a random subset of size 5 from the Spark dataframe &lt;code&gt;mtcars_sdf&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More importantly, the sampling algorithm implemented in &lt;code&gt;sparklyr&lt;/code&gt; 1.4 is something that fits perfectly into the MapReduce paradigm: as we have split our &lt;code&gt;mtcars&lt;/code&gt; data into 4 partitions of &lt;code&gt;mtcars_sdf&lt;/code&gt; by specifying &lt;code&gt;repartition = 4L&lt;/code&gt;, the algorithm will first process each partition independently and in parallel, selecting a sample set of size up to 5 from each, and then reduce all 4 sample sets into a final sample set of size 5 by choosing records having the top 5 highest sampling priorities among all.&lt;/p&gt;
&lt;p&gt;How is such parallelization possible, especially for the sampling without replacement scenario, where the desired result is defined as the outcome of a sequential process? A detailed answer to this question is in &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-29-parallelized-sampling/"&gt;this blog post&lt;/a&gt;, which includes a definition of the problem (in particular, the exact meaning of sampling weights in term of probabilities), a high-level explanation of the current solution and the motivation behind it, and also, some mathematical details all hidden in one link to a PDF file, so that non-math-oriented readers can get the gist of everything else without getting scared away, while math-oriented readers can enjoy working out all the integrals themselves before peeking at the answer.&lt;/p&gt;
&lt;h2 id="tidyr-verbs"&gt;Tidyr Verbs&lt;/h2&gt;
&lt;p&gt;The specialized implementations of the following &lt;a href="https://tidyr.tidyverse.org/"&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt; verbs that work efficiently with Spark dataframes were included as part of &lt;code&gt;sparklyr&lt;/code&gt; 1.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/fill.html"&gt;&lt;code&gt;tidyr::fill&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/nest.html"&gt;&lt;code&gt;tidyr::nest&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/nest.html"&gt;&lt;code&gt;tidyr::unnest&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/pivot_wider.html"&gt;&lt;code&gt;tidyr::pivot_wider&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/pivot_longer.html"&gt;&lt;code&gt;tidyr::pivot_longer&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/separate.html"&gt;&lt;code&gt;tidyr::separate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tidyr.tidyverse.org/reference/unite.html"&gt;&lt;code&gt;tidyr::unite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can demonstrate how those verbs are useful for tidying data through some examples.&lt;/p&gt;
&lt;p&gt;Let’s say we are given &lt;code&gt;mtcars_sdf&lt;/code&gt;, a Spark dataframe containing all rows from &lt;code&gt;mtcars&lt;/code&gt; plus the name of each row:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
mtcars_sdf &amp;lt;- cbind(
  data.frame(model = rownames(mtcars)),
  data.frame(mtcars, row.names = NULL)
) %&amp;gt;%
  copy_to(sc, ., repartition = 4L)

print(mtcars_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model          mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4
2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4
3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1
4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1
5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we would like to turn all numeric attributes in &lt;code&gt;mtcar_sdf&lt;/code&gt; (in other words, all columns other than the &lt;code&gt;model&lt;/code&gt; column) into key-value pairs stored in 2 columns, with the &lt;code&gt;key&lt;/code&gt; column storing the name of each attribute, and the &lt;code&gt;value&lt;/code&gt; column storing each attribute’s numeric value. One way to accomplish that with &lt;code&gt;tidyr&lt;/code&gt; is by utilizing the &lt;code&gt;tidyr::pivot_longer&lt;/code&gt; functionality:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_kv_sdf &amp;lt;- mtcars_sdf %&amp;gt;%
  tidyr::pivot_longer(cols = -model, names_to = &amp;quot;key&amp;quot;, values_to = &amp;quot;value&amp;quot;)
print(mtcars_kv_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 3]
  model     key   value
  &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4 am      1
2 Mazda RX4 carb    4
3 Mazda RX4 cyl     6
4 Mazda RX4 disp  160
5 Mazda RX4 drat    3.9
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To undo the effect of &lt;code&gt;tidyr::pivot_longer&lt;/code&gt;, we can apply &lt;code&gt;tidyr::pivot_wider&lt;/code&gt; to our &lt;code&gt;mtcars_kv_sdf&lt;/code&gt; Spark dataframe, and get back the original data that was present in &lt;code&gt;mtcars_sdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tbl &amp;lt;- mtcars_kv_sdf %&amp;gt;%
  tidyr::pivot_wider(names_from = key, values_from = value)
print(tbl, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model         carb   cyl  drat    hp   mpg    vs    wt    am  disp  gear  qsec
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4        4     6  3.9    110  21       0  2.62     1  160      4  16.5
2 Hornet 4 Dr…     1     6  3.08   110  21.4     1  3.22     0  258      3  19.4
3 Hornet Spor…     2     8  3.15   175  18.7     0  3.44     0  360      3  17.0
4 Merc 280C        4     6  3.92   123  17.8     1  3.44     0  168.     4  18.9
5 Merc 450SLC      3     8  3.07   180  15.2     0  3.78     0  276.     3  18
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to reduce many columns into fewer ones is by using &lt;code&gt;tidyr::nest&lt;/code&gt; to move some columns into nested tables. For instance, we can create a nested table &lt;code&gt;perf&lt;/code&gt; encapsulating all performance-related attributes from &lt;code&gt;mtcars&lt;/code&gt; (namely, &lt;code&gt;hp&lt;/code&gt;, &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;qsec&lt;/code&gt;). However, unlike R dataframes, Spark Dataframes do not have the concept of nested tables, and the closest to nested tables we can get is a &lt;code&gt;perf&lt;/code&gt; column containing named structs with &lt;code&gt;hp&lt;/code&gt;, &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;qsec&lt;/code&gt; attributes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_nested_sdf &amp;lt;- mtcars_sdf %&amp;gt;%
  tidyr::nest(perf = c(hp, mpg, disp, qsec))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then inspect the type of &lt;code&gt;perf&lt;/code&gt; column in &lt;code&gt;mtcars_nested_sdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sdf_schema(mtcars_nested_sdf)$perf$type&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;ArrayType(StructType(StructField(hp,DoubleType,true), StructField(mpg,DoubleType,true), StructField(disp,DoubleType,true), StructField(qsec,DoubleType,true)),true)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and inspect individual struct elements within &lt;code&gt;perf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;perf &amp;lt;- mtcars_nested_sdf %&amp;gt;% dplyr::pull(perf)
unlist(perf[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    hp    mpg   disp   qsec
110.00  21.00 160.00  16.46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can also use &lt;code&gt;tidyr::unnest&lt;/code&gt; to undo the effects of &lt;code&gt;tidyr::nest&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mtcars_unnested_sdf &amp;lt;- mtcars_nested_sdf %&amp;gt;%
  tidyr::unnest(col = perf)
print(mtcars_unnested_sdf, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 12]
  model          cyl  drat    wt    vs    am  gear  carb    hp   mpg  disp  qsec
  &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 Mazda RX4        6  3.9   2.62     0     1     4     4   110  21    160   16.5
2 Hornet 4 Dr…     6  3.08  3.22     1     0     3     1   110  21.4  258   19.4
3 Duster 360       8  3.21  3.57     0     0     3     4   245  14.3  360   15.8
4 Merc 280         6  3.92  3.44     1     0     4     4   123  19.2  168.  18.3
5 Lincoln Con…     8  3     5.42     0     0     3     4   215  10.4  460   17.8
# … with more rows&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="robust-scaler"&gt;Robust Scaler&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; is a new functionality introduced in Spark 3.0 (&lt;a href="https://issues.apache.org/jira/browse/SPARK-28399"&gt;SPARK-28399&lt;/a&gt;). Thanks to a &lt;a href="https://github.com/sparklyr/sparklyr/pull/2254"&gt;pull request&lt;/a&gt; by &lt;a href="https://github.com/zero323"&gt;@zero323&lt;/a&gt;, an R interface for &lt;code&gt;RobustScaler&lt;/code&gt;, namely, the &lt;code&gt;ft_robust_scaler()&lt;/code&gt; function, is now part of &lt;code&gt;sparklyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is often observed that many machine learning algorithms perform better on numeric inputs that are standardized. Many of us have learned in stats 101 that given a random variable &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, we can compute its mean &lt;span class="math inline"&gt;\(\mu = E[X]\)&lt;/span&gt;, standard deviation &lt;span class="math inline"&gt;\(\sigma = \sqrt{E[X^2] - (E[X])^2}\)&lt;/span&gt;, and then obtain a standard score &lt;span class="math inline"&gt;\(z = \frac{X - \mu}{\sigma}\)&lt;/span&gt; which has mean of 0 and standard deviation of 1.&lt;/p&gt;
&lt;p&gt;However, notice both &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(E[X^2]\)&lt;/span&gt; from above are quantities that can be easily skewed by extreme outliers in &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, causing distortions in &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;. A particular bad case of it would be if all non-outliers among &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; are very close to &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, hence making &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; close to &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, while extreme outliers are all far in the negative direction, hence dragging down &lt;span class="math inline"&gt;\(E[X]\)&lt;/span&gt; while skewing &lt;span class="math inline"&gt;\(E[X^2]\)&lt;/span&gt; upwards.&lt;/p&gt;
&lt;p&gt;An alternative way of standardizing &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; based on its median, 1st quartile, and 3rd quartile values, all of which are robust against outliers, would be the following:&lt;/p&gt;
&lt;p&gt;&lt;span class="math inline"&gt;\(\displaystyle z = \frac{X - \text{Median}(X)}{\text{P75}(X) - \text{P25}(X)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and this is precisely what &lt;a href="https://spark.apache.org/docs/3.0.0/api/java/org/apache/spark/ml/feature/RobustScaler.html"&gt;RobustScaler&lt;/a&gt; offers.&lt;/p&gt;
&lt;p&gt;To see &lt;code&gt;ft_robust_scaler()&lt;/code&gt; in action and demonstrate its usefulness, we can go through a contrived example consisting of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw 500 random samples from the standard normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;sample_values &amp;lt;- rnorm(500)
print(sample_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772
  [6] -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Inspect the minimal and maximal values among the &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; random samples:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;print(min(sample_values))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] -3.008049&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;print(max(sample_values))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] 3.810277&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now create &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; other values that are extreme outliers compared to the &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; random samples above. Given that we know all &lt;span class="math inline"&gt;\(500\)&lt;/span&gt; samples are within the range of &lt;span class="math inline"&gt;\((-4, 4)\)&lt;/span&gt;, we can choose &lt;span class="math inline"&gt;\(-501, -502, \ldots, -509, -510\)&lt;/span&gt; as our &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; outliers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;outliers &amp;lt;- -500L - seq(10)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Copy all &lt;span class="math inline"&gt;\(510\)&lt;/span&gt; values into a Spark dataframe named &lt;code&gt;sdf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;)
sdf &amp;lt;- copy_to(sc, data.frame(value = c(sample_values, outliers)))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can then apply &lt;code&gt;ft_robust_scaler()&lt;/code&gt; to obtain the standardized value for each input:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;scaled &amp;lt;- sdf %&amp;gt;%
  ft_vector_assembler(&amp;quot;value&amp;quot;, &amp;quot;input&amp;quot;) %&amp;gt;%
  ft_robust_scaler(&amp;quot;input&amp;quot;, &amp;quot;scaled&amp;quot;) %&amp;gt;%
  dplyr::pull(scaled) %&amp;gt;%
  unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the result shows the non-outlier data points being scaled to values that still more or less form a bell-shaped distribution centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, as expected, so the scaling is robust against influence of the outliers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggplot2)

ggplot(data.frame(scaled = scaled), aes(x = scaled)) +
  xlim(-7, 7) +
  geom_histogram(binwidth = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-30-sparklyr-1.4.0-released/images/scaled.png" id="id" class="class" style="width:60.0%;height:60.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can compare the distribution of the scaled values above with the distribution of z-scores of all input values, and notice how scaling the input with only mean and standard deviation would have caused noticeable skewness – which the robust scaler has successfully avoided:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_values &amp;lt;- c(sample_values, outliers)
z_scores &amp;lt;- (all_values - mean(all_values)) / sd(all_values)
ggplot(data.frame(scaled = z_scores), aes(x = scaled)) +
  xlim(-0.05, 0.2) +
  geom_histogram(binwidth = 0.005)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-30-sparklyr-1.4.0-released/images/skewed.png" id="id" class="class" style="width:60.0%;height:60.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the 2 plots above, one can observe while both standardization processes produced some distributions that were still bell-shaped, the one produced by &lt;code&gt;ft_robust_scaler()&lt;/code&gt; is centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, correctly indicating the average among all non-outlier values, while the z-score distribution is clearly not centered around &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; as its center has been noticeably shifted by the &lt;span class="math inline"&gt;\(10\)&lt;/span&gt; outlier values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rapids"&gt;RAPIDS&lt;/h2&gt;
&lt;p&gt;Readers following Apache Spark releases closely probably have noticed the recent addition of &lt;a href="https://rapids.ai/"&gt;RAPIDS&lt;/a&gt; GPU acceleration support in Spark 3.0. Catching up with this recent development, an option to enable RAPIDS in Spark connections was also created in &lt;code&gt;sparklyr&lt;/code&gt; and shipped in &lt;code&gt;sparklyr&lt;/code&gt; 1.4. On a host with RAPIDS-capable hardware (e.g., an Amazon EC2 instance of type ‘p3.2xlarge’), one can install &lt;code&gt;sparklyr&lt;/code&gt; 1.4 and observe RAPIDS hardware acceleration being reflected in Spark SQL physical query plans:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;, packages = &amp;quot;rapids&amp;quot;)
dplyr::db_explain(sc, &amp;quot;SELECT 4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Physical Plan ==
*(2) GpuColumnarToRow false
+- GpuProject [4 AS 4#45]
   +- GpuRowToColumnar TargetSize(2147483647)
      +- *(1) Scan OneRowRelation[]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="higher-order-functions-and-dplyr-related-improvements"&gt;Higher-Order Functions and &lt;code&gt;dplyr&lt;/code&gt;-Related Improvements&lt;/h2&gt;
&lt;p&gt;All newly introduced higher-order functions from Spark 3.0, such as &lt;code&gt;array_sort()&lt;/code&gt; with custom comparator, &lt;code&gt;transform_keys()&lt;/code&gt;, &lt;code&gt;transform_values()&lt;/code&gt;, and &lt;code&gt;map_zip_with()&lt;/code&gt;, are supported by &lt;code&gt;sparklyr&lt;/code&gt; 1.4.&lt;/p&gt;
&lt;p&gt;In addition, all higher-order functions can now be accessed directly through &lt;code&gt;dplyr&lt;/code&gt; rather than their &lt;code&gt;hof_*&lt;/code&gt; counterparts in &lt;code&gt;sparklyr&lt;/code&gt;. This means, for example, that we can run the following &lt;code&gt;dplyr&lt;/code&gt; queries to calculate the square of all array elements in column &lt;code&gt;x&lt;/code&gt; of &lt;code&gt;sdf&lt;/code&gt;, and then sort them in descending order:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;3.0.0&amp;quot;)
sdf &amp;lt;- copy_to(sc, tibble::tibble(x = list(c(-3, -2, 1, 5), c(6, -7, 5, 8))))

sq_desc &amp;lt;- sdf %&amp;gt;%
  dplyr::mutate(x = transform(x, ~ .x * .x)) %&amp;gt;%
  dplyr::mutate(x = array_sort(x, ~ as.integer(sign(.y - .x)))) %&amp;gt;%
  dplyr::pull(x)

print(sq_desc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 25  9  4  1

[[2]]
[1] 64 49 36 25&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;In chronological order, we would like to thank the following individuals for their contributions to &lt;code&gt;sparklyr&lt;/code&gt; 1.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https:://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/nealrichardson"&gt;@nealrichardson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/yitao-li"&gt;@yitao-li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/wkdavis"&gt;@wkdavis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/Loquats"&gt;@Loquats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https:://github.com/zero323"&gt;@zero323&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also appreciate bug reports, feature requests, and valuable other feedback about &lt;code&gt;sparklyr&lt;/code&gt; from our awesome open-source community (e.g., the weighted sampling feature in &lt;code&gt;sparklyr&lt;/code&gt; 1.4 was largely motivated by this &lt;a href="https://github.com/sparklyr/sparklyr/issues/2592"&gt;Github issue&lt;/a&gt; filed by &lt;a href="https://github.com/ajing"&gt;@ajing&lt;/a&gt;, and some &lt;code&gt;dplyr&lt;/code&gt;-related bug fixes in this release were initiated in &lt;a href="https://github.com/sparklyr/sparklyr/issues/2648"&gt;#2648&lt;/a&gt; and completed with this &lt;a href="https://github.com/sparklyr/sparklyr/pull/2651"&gt;pull request&lt;/a&gt; by &lt;a href="https:://github.com/wkdavis"&gt;@wkdavis&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Last but not least, the author of this blog post is extremely grateful for fantastic editorial suggestions from &lt;a href="https:://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;, &lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;, and &lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you wish to learn more about &lt;code&gt;sparklyr&lt;/code&gt;, we recommend checking out &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, and also some of the previous release posts such as &lt;a href="https://blog.rstudio.com/2020/07/16/sparklyr-1-3/"&gt;sparklyr 1.3&lt;/a&gt; and &lt;a href="https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/"&gt;sparklyr 1.2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">19cbb456871dc547ae3c7f20c08fb027</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</guid>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released/images/sparklyr-1.4.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Please allow me to introduce myself: Torch for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</link>
      <description>


&lt;p&gt;Last January at &lt;a href="https://rstudio.com/conference/"&gt;rstudio::conf&lt;/a&gt;, in that distant past when conferences still used to take place at some physical location, my colleague &lt;a href="https://twitter.com/dfalbel"&gt;Daniel&lt;/a&gt; gave a talk introducing new features and ongoing development in the &lt;code&gt;tensorflow&lt;/code&gt; ecosystem. In the Q&amp;amp;A part, he was asked something unexpected: Were we going to build support for &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;? He hesitated; that was in fact the plan, and he had already played around with natively implementing &lt;code&gt;torch&lt;/code&gt; tensors at a prior time, but he was not completely certain how well “it” would work.&lt;/p&gt;
&lt;p&gt;“It”, that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via &lt;code&gt;reticulate&lt;/code&gt;. Instead, we delegate to the underlying C++ library &lt;code&gt;libtorch&lt;/code&gt; for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, &lt;code&gt;torch&lt;/code&gt; does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.&lt;/p&gt;
&lt;p&gt;So why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against &lt;code&gt;libtorch&lt;/code&gt; would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that &lt;code&gt;torch&lt;/code&gt; can be useful to the R community. Thus, without further ado, let’s train a neural network.&lt;/p&gt;
&lt;p&gt;You’re not at your laptop now? Just follow along in the &lt;a href="https://colab.research.google.com/drive/1NdiN9n_a7NEvFpvjPDvxKTshrSWgxZK5?usp=sharing"&gt;companion notebook on Colaboratory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;h4 id="torch"&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Installing &lt;code&gt;torch&lt;/code&gt; is as straightforward as typing&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will detect whether you have CUDA installed, and either download the CPU or the GPU version of &lt;code&gt;libtorch&lt;/code&gt;. Then, it will install the R package from CRAN. To make use of the very newest features, you can install the development version from GitHub:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;mlverse/torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To quickly check the installation, and whether GPU support works fine (assuming that there &lt;em&gt;is&lt;/em&gt; a CUDA-capable NVidia GPU), create a tensor &lt;em&gt;on the CUDA device&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;torch_tensor(1, device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
[ CUDAFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all our &lt;em&gt;hello torch&lt;/em&gt; example did was run a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="torchvision"&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Whereas &lt;code&gt;torch&lt;/code&gt; is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.&lt;/p&gt;
&lt;p&gt;As of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan”, because &lt;code&gt;torchtext&lt;/code&gt; and &lt;code&gt;torchaudio&lt;/code&gt; are yet to be created. Right now, &lt;code&gt;torchvision&lt;/code&gt; is all we need:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;mlverse/torchvision&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to load the data.&lt;/p&gt;
&lt;h2 id="data-loading-and-pre-processing"&gt;Data loading and pre-processing&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(torchvision)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The list of vision datasets bundled with PyTorch is long, and they’re continually being added to &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin”, &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt; &lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt;. Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution &lt;code&gt;28x28&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are the first 32 characters:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-29-introducing-torch-for-r/images/kmnist.png" alt="Kuzushiji MNIST." width="768" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Kuzushiji MNIST.
&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="dataset"&gt;Dataset&lt;/h4&gt;
&lt;p&gt;The following code will download the data separately for training and test sets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds &amp;lt;- kmnist_dataset(
  &amp;quot;.&amp;quot;,
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)

test_ds &amp;lt;- kmnist_dataset(
  &amp;quot;.&amp;quot;,
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;transform&lt;/code&gt; argument. &lt;code&gt;transform_to_tensor&lt;/code&gt; takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?&lt;/p&gt;
&lt;p&gt;Contrary to what you might expect – if until now, you’ve been using &lt;code&gt;keras&lt;/code&gt; – the additional dimension is &lt;em&gt;not&lt;/em&gt; the batch dimension. Batching will be taken care of by the &lt;code&gt;dataloader&lt;/code&gt;, to be introduced next. Instead, this is the &lt;em&gt;channels&lt;/em&gt; dimension that in &lt;code&gt;torch&lt;/code&gt;, is found &lt;em&gt;before&lt;/em&gt; the width and height dimensions by default.&lt;/p&gt;
&lt;p&gt;One thing I’ve found to be extremely useful about &lt;code&gt;torch&lt;/code&gt; is how easy it is to inspect objects. Even though we’re dealing with a &lt;code&gt;dataset&lt;/code&gt;, a custom object, and not an R array or even a &lt;code&gt;torch&lt;/code&gt; tensor, we can easily peek at what’s inside. Indexing in &lt;code&gt;torch&lt;/code&gt; is 1-based, conforming to the R user’s intuitions. Consequently,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives us the first element in the dataset, an R &lt;em&gt;list&lt;/em&gt; of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)&lt;/p&gt;
&lt;p&gt;Let’s inspect the shape of the input tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1][[1]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 28 28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In &lt;code&gt;torch&lt;/code&gt;, this is the task of data loaders.&lt;/p&gt;
&lt;h4 id="data-loader"&gt;Data loader&lt;/h4&gt;
&lt;p&gt;Each of the training and test sets gets their own data loader:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dl &amp;lt;- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl &amp;lt;- dataloader(test_ds, batch_size = 32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, &lt;code&gt;torch&lt;/code&gt; makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_iter &amp;lt;- train_dl$.iter()
train_iter$.next()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functionality like this may not seem indispensable when working with a well-known dataset, but it will turn out to be very useful when a lot of domain-specific pre-processing is required.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(4,8), mar = rep(0, 4))
images &amp;lt;- train_dl$.iter()$.next()[[1]][1:32, 1, , ] 
images %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re ready to define our network – a simple convnet.&lt;/p&gt;
&lt;h2 id="network"&gt;Network&lt;/h2&gt;
&lt;p&gt;If you’ve been using &lt;code&gt;keras&lt;/code&gt; &lt;em&gt;custom models&lt;/em&gt; (or have some experience with &lt;em&gt;Py&lt;/em&gt;Torch), the following way of defining a network may not look too surprising.&lt;/p&gt;
&lt;p&gt;You use &lt;code&gt;nn_module()&lt;/code&gt; to define an R6 class that will hold the network’s components. Its layers are created in &lt;code&gt;initialize()&lt;/code&gt;; &lt;code&gt;forward()&lt;/code&gt; describes what happens during the network’s forward pass. One thing on terminology: In &lt;code&gt;torch&lt;/code&gt;, layers are called &lt;em&gt;modules&lt;/em&gt;, as are networks. This makes sense: The design is truly &lt;em&gt;modular&lt;/em&gt; in that any module can be used as a component in a larger one.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;net &amp;lt;- nn_module(
  
  &amp;quot;KMNIST-CNN&amp;quot;,
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    self$conv1 &amp;lt;- nn_conv2d(1, 32, 3)
    self$conv2 &amp;lt;- nn_conv2d(32, 64, 3)
    self$dropout1 &amp;lt;- nn_dropout2d(0.25)
    self$dropout2 &amp;lt;- nn_dropout2d(0.5)
    self$fc1 &amp;lt;- nn_linear(9216, 128)
    self$fc2 &amp;lt;- nn_linear(128, 10)
  },
  
  forward = function(x) {
    x %&amp;gt;% 
      self$conv1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$conv2() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      nnf_max_pool2d(2) %&amp;gt;%
      self$dropout1() %&amp;gt;%
      torch_flatten(start_dim = 2) %&amp;gt;%
      self$fc1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$dropout2() %&amp;gt;%
      self$fc2()
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The layers – apologies: modules – themselves may look familiar. Unsurprisingly, &lt;code&gt;nn_conv2d()&lt;/code&gt; performs two-dimensional convolution; &lt;code&gt;nn_linear()&lt;/code&gt; multiplies by a weight matrix and adds a vector of biases. But what are those numbers: &lt;code&gt;nn_linear(128, 10)&lt;/code&gt;, say?&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, &lt;code&gt;nn_linear(128, 10)&lt;/code&gt; has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about the previous module? How do we arrive at &lt;code&gt;9216&lt;/code&gt; input connections?&lt;/p&gt;
&lt;p&gt;Here, a bit of calculation is necessary. We go through all actions that happen in &lt;code&gt;forward()&lt;/code&gt; – if they affect shapes, we keep track of the transformation; if they don’t, we ignore them.&lt;/p&gt;
&lt;p&gt;So, we start with input tensors of shape &lt;code&gt;batch_size x 1 x 28 x 28&lt;/code&gt;. Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(1, 32, 3)&lt;/code&gt; , or equivalently, &lt;code&gt;nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),&lt;/code&gt;applies a convolution with kernel size 3, stride 1 (the default), and no padding (the default). We can consult the &lt;a href="https://mlverse.github.io/torch/reference/nn_conv2d.html"&gt;documentation&lt;/a&gt; to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of &lt;code&gt;26 x 26&lt;/code&gt;. &lt;em&gt;Per channel&lt;/em&gt;, that is. Thus, the actual output shape is &lt;code&gt;batch_size x 32 x 26 x 26&lt;/code&gt; . Next,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; applies ReLU activation, in no way touching the shape. Next is&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(32, 64, 3)&lt;/code&gt;, another convolution with zero padding and kernel size 3. Output size now is &lt;code&gt;batch_size x 64 x 24 x 24&lt;/code&gt; . Now, the second&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; again does nothing to the output shape, but&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_max_pool2d(2)&lt;/code&gt; (equivalently: &lt;code&gt;nnf_max_pool2d(kernel_size = 2)&lt;/code&gt;) does: It applies max pooling over regions of extension &lt;code&gt;2 x 2&lt;/code&gt;, thus downsizing the output to a format of &lt;code&gt;batch_size x 64 x 12 x 12&lt;/code&gt; . Now,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_dropout2d(0.25)&lt;/code&gt; is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the &lt;em&gt;channels&lt;/em&gt;, &lt;em&gt;height&lt;/em&gt; and &lt;em&gt;width&lt;/em&gt; axes into a single dimension. This is done in&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;torch_flatten(start_dim = 2)&lt;/code&gt;. Output shape is now &lt;code&gt;batch_size * 9216&lt;/code&gt; , since &lt;code&gt;64 * 12 * 12 = 9216&lt;/code&gt; . Thus here we have the &lt;code&gt;9216&lt;/code&gt; input connections fed into the&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(9216, 128)&lt;/code&gt; discussed above. Again,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; and &lt;code&gt;nn_dropout2d(0.5)&lt;/code&gt; leave dimensions as they are, and finally,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(128, 10)&lt;/code&gt; gives us the desired output scores, one for each of the ten classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with &lt;code&gt;torch&lt;/code&gt;’s flexibility, there is another way. Since every layer is callable &lt;em&gt;in isolation&lt;/em&gt;, we can just … create some sample data and see what happens!&lt;/p&gt;
&lt;p&gt;Here is a sample “image” – or more precisely, a one-item batch containing it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- torch_randn(c(1, 1, 28, 28))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we call the first &lt;em&gt;conv2d&lt;/em&gt; module on it?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv1 &amp;lt;- nn_conv2d(1, 32, 3)
conv1(x)$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 32 26 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or both &lt;em&gt;conv2d&lt;/em&gt; modules?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv2 &amp;lt;- nn_conv2d(32, 64, 3)
(conv1(x) %&amp;gt;% conv2())$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 64 24 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And so on. This is just one example illustrating how &lt;code&gt;torch&lt;/code&gt;s flexibility makes developing neural nets easier.&lt;/p&gt;
&lt;p&gt;Back to the main thread. We instantiate the model, and we ask &lt;code&gt;torch&lt;/code&gt; to allocate its weights (parameters) on the GPU:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- net()
model$to(device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.&lt;/p&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the loss function? For classification with more than two classes, we use &lt;em&gt;cross entropy&lt;/em&gt;, in &lt;code&gt;torch&lt;/code&gt;: &lt;code&gt;nnf_cross_entropy(prediction, ground_truth)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# this will be called for every batch, see training loop below
loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike categorical cross entropy in &lt;code&gt;keras&lt;/code&gt; , which would expect &lt;code&gt;prediction&lt;/code&gt; to contain probabilities, as obtained by applying a &lt;em&gt;softmax&lt;/em&gt; activation, &lt;code&gt;torch&lt;/code&gt;’s &lt;code&gt;nnf_cross_entropy()&lt;/code&gt; works with the raw outputs (the &lt;em&gt;logits&lt;/em&gt;). This is why the network’s last linear layer was not followed by any activation.&lt;/p&gt;
&lt;p&gt;The training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in 1:5) {

  l &amp;lt;- c()

  for (b in enumerate(train_dl)) {
    # make sure each batch&amp;#39;s gradient updates are calculated from a fresh start
    optimizer$zero_grad()
    # get model predictions
    output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate loss
    loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate gradient
    loss$backward()
    # apply weight updates
    optimizer$step()
    # track losses
    l &amp;lt;- c(l, loss$item())
  }

  cat(sprintf(&amp;quot;Loss at epoch %d: %3f\n&amp;quot;, epoch, mean(l)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: 1.795564
Loss at epoch 2: 1.540063
Loss at epoch 3: 1.495343
Loss at epoch 4: 1.461649
Loss at epoch 5: 1.446628&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there is a lot more that &lt;em&gt;could&lt;/em&gt; be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a &lt;code&gt;torch&lt;/code&gt; training loop.&lt;/p&gt;
&lt;p&gt;The optimizer-related idioms in particular&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$zero_grad()
# ...
loss$backward()
# ...
optimizer$step()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you’ll keep encountering over and over.&lt;/p&gt;
&lt;p&gt;Finally, let’s evaluate model performance on the test set.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Putting a model in &lt;code&gt;eval&lt;/code&gt; mode tells &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;not&lt;/em&gt; to calculate gradients and perform backprop during the operations that follow:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We iterate over the test set, keeping track of losses and accuracies obtained on the batches.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_losses &amp;lt;- c()
total &amp;lt;- 0
correct &amp;lt;- 0

for (b in enumerate(test_dl)) {
  output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
  labels &amp;lt;- b[[2]]$to(device = &amp;quot;cuda&amp;quot;)
  loss &amp;lt;- nnf_cross_entropy(output, labels)
  test_losses &amp;lt;- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values 
  # and position 2 containing the respective indices
  predicted &amp;lt;- torch_max(output$data(), dim = 2)[[2]]
  total &amp;lt;- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct &amp;lt;- correct + (predicted == labels)$sum()$item()
}

mean(test_losses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.53784480643349&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is mean accuracy, computed as proportion of correct classifications:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_accuracy &amp;lt;-  correct/total
test_accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9449&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for our first &lt;code&gt;torch&lt;/code&gt; example. Where to from here?&lt;/p&gt;
&lt;h2 id="learn"&gt;Learn&lt;/h2&gt;
&lt;p&gt;To learn more, check out our vignettes on the &lt;a href="https://mlverse.github.io/torch"&gt;&lt;code&gt;torch&lt;/code&gt; website&lt;/a&gt;. To begin, you may want to check out these in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Getting started” series: Build a simple neural network from scratch, starting from &lt;a href="https://mlverse.github.io/tohttps://mlverse.github.io/torch/articles/getting-started/tensors.html"&gt;low-level tensor manipulation&lt;/a&gt; and gradually adding in higher-level features like &lt;a href="https://mlverse.github.io/torch/articles/getting-started/tensors-and-autograd.html"&gt;automatic differentiation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/getting-started/nn.html"&gt;network modules&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;More on tensors: &lt;a href="https://mlverse.github.io/torch/articles/tensor-creation.html"&gt;Tensor creation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/indexing.html"&gt;indexing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Backpropagation in &lt;code&gt;torch&lt;/code&gt;: &lt;a href="https://mlverse.github.io/torch/articles/using-autograd.html"&gt;autograd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have questions, or run into problems, please feel free to ask on &lt;a href="https://github.com/mlverse/torch"&gt;GitHub&lt;/a&gt; or on the &lt;a href="https://community.rstudio.com/c/ml/15"&gt;RStudio community forum&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="we-need-you"&gt;We need you&lt;/h2&gt;
&lt;p&gt;We very much hope that the R community will find the new functionality useful. But that’s not all. We hope that you, many of you, will take part in the journey.&lt;/p&gt;
&lt;p&gt;There is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.&lt;/p&gt;
&lt;p&gt;There is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps &lt;em&gt;the&lt;/em&gt; essential factor in how usable a framework is.&lt;/p&gt;
&lt;p&gt;Then, there is the ever-expanding ecosystem of libraries built on top of PyTorch: &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/CrypTen"&gt;CrypTen&lt;/a&gt; for privacy-preserving machine learning, &lt;a href="https://github.com/rusty1s/pytorch_geometric"&gt;PyTorch Geometric&lt;/a&gt; for deep learning on manifolds, and &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt; for probabilistic programming, to name just a few.&lt;/p&gt;
&lt;p&gt;All this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely &lt;em&gt;any&lt;/em&gt; scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add or improve documentation, add introductory examples&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement missing layers (modules), activations, helper functions…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement model architectures&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Port some of the PyTorch ecosystem&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One component that should be of special interest to the R community is &lt;a href="https://pytorch.org/docs/stable/distributions.html"&gt;Torch distributions&lt;/a&gt;, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt;; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.&lt;/p&gt;
&lt;p&gt;To reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with &lt;code&gt;torch&lt;/code&gt;, and thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-clanuwat2018deep"&gt;
&lt;p&gt;Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. &lt;a href="http://arxiv.org/abs/cs.CV/1812.01718"&gt;http://arxiv.org/abs/cs.CV/1812.01718&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;In a nutshell, &lt;a href="https://twitter.com/javierluraschi"&gt;Javier&lt;/a&gt; had the idea of wrapping &lt;code&gt;libtorch&lt;/code&gt; into &lt;a href="https://github.com/mlverse/lantern"&gt;lantern&lt;/a&gt;, a C interface to &lt;code&gt;libtorch&lt;/code&gt;, thus avoiding cross-compiler issues between MinGW and Visual Studio.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4e028984b62685f317fbdf6861edc437</distill:md5>
      <category>Packages/Releases</category>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</guid>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/pt.png" medium="image" type="image/png" width="919" height="264"/>
    </item>
    <item>
      <title>Introducing sparklyr.flint: A time-series extension for sparklyr</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</link>
      <description>


&lt;p&gt;In this blog post, we will showcase &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/index.html"&gt;&lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/a&gt;, a brand new &lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; extension providing a simple and intuitive R interface to the &lt;a href="https://github.com/twosigma/flint"&gt;&lt;code&gt;Flint&lt;/code&gt;&lt;/a&gt; time series library. &lt;code&gt;sparklyr.flint&lt;/code&gt; is available on &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/index.html"&gt;CRAN&lt;/a&gt; today and can be installed as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr.flint&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first two sections of this post will be a quick bird’s eye view on &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;Flint&lt;/code&gt;, which will ensure readers unfamiliar with &lt;code&gt;sparklyr&lt;/code&gt; or &lt;code&gt;Flint&lt;/code&gt; can see both of them as essential building blocks for &lt;code&gt;sparklyr.flint&lt;/code&gt;. After that, we will feature &lt;code&gt;sparklyr.flint&lt;/code&gt;’s design philosophy, current state, example usages, and last but not least, its future directions as an open-source project in the subsequent sections.&lt;/p&gt;
&lt;h1 id="quick-intro-to-sparklyr"&gt;Quick Intro to &lt;code&gt;sparklyr&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;sparklyr&lt;/code&gt; is an open-source R interface that integrates the power of distributed computing from &lt;a href="https://spark.apache.org"&gt;Apache Spark&lt;/a&gt; with the familiar idioms, tools, and paradigms for data transformation and data modelling in R. It allows data pipelines working well with non-distributed data in R to be easily transformed into analogous ones that can process large-scale, distributed data in Apache Spark.&lt;/p&gt;
&lt;p&gt;Instead of summarizing everything &lt;code&gt;sparklyr&lt;/code&gt; has to offer in a few sentences, which is impossible to do, this section will solely focus on a small subset of &lt;code&gt;sparklyr&lt;/code&gt; functionalities that are relevant to connecting to Apache Spark from R, importing time series data from external data sources to Spark, and also simple transformations which are typically part of data pre-processing steps.&lt;/p&gt;
&lt;h3 id="connecting-to-an-apache-spark-cluster"&gt;Connecting to an Apache Spark cluster&lt;/h3&gt;
&lt;p&gt;The first step in using &lt;code&gt;sparklyr&lt;/code&gt; is to connect to Apache Spark. Usually this means one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Running Apache Spark locally on your machine, and connecting to it to test, debug, or to execute quick demos that don’t require a multi-node Spark cluster:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4.4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connecting to a multi-node Apache Spark cluster that is managed by a cluster manager such as &lt;a href="https://spark.apache.org/docs/latest/running-on-yarn.html"&gt;YARN&lt;/a&gt;, e.g.,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;yarn-client&amp;quot;, spark_home = &amp;quot;/usr/lib/spark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="importing-external-data-to-spark"&gt;Importing external data to Spark&lt;/h3&gt;
&lt;p&gt;Making external data available in Spark is easy with &lt;code&gt;sparklyr&lt;/code&gt; given the large number of data sources &lt;code&gt;sparklyr&lt;/code&gt; supports. For example, given an R dataframe, such as&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dat &amp;lt;- data.frame(id = seq(10), value = rnorm(10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the command to copy it to a Spark dataframe with 3 partitions is simply&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sdf &amp;lt;- copy_to(sc, dat, name = &amp;quot;unique_name_of_my_spark_dataframe&amp;quot;, repartition = 3L)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, there are options for ingesting data in CSV, JSON, ORC, AVRO, and many other well-known formats into Spark as well:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sdf_csv &amp;lt;- spark_read_csv(sc, name = &amp;quot;another_spark_dataframe&amp;quot;, path = &amp;quot;file:///tmp/file.csv&amp;quot;, repartition = 3L)
# or
sdf_json &amp;lt;- spark_read_json(sc, name = &amp;quot;yet_another_one&amp;quot;, path = &amp;quot;file:///tmp/file.json&amp;quot;, repartition = 3L)
# or spark_read_orc, spark_read_avro, etc&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="transforming-a-spark-dataframe"&gt;Transforming a Spark dataframe&lt;/h3&gt;
&lt;p&gt;With &lt;code&gt;sparklyr&lt;/code&gt;, the simplest and most readable way to transformation a Spark dataframe is by using &lt;code&gt;dplyr&lt;/code&gt; verbs and the pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) from &lt;a href="https://cran.r-project.org/web/packages/magrittr/index.html"&gt;magrittr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sparklyr&lt;/code&gt; supports a large number of &lt;code&gt;dplyr&lt;/code&gt; verbs. For example,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sdf &amp;lt;- sdf %&amp;gt;%
  dplyr::filter(!is.null(id)) %&amp;gt;%
  dplyr::mutate(value = value ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ensures &lt;code&gt;sdf&lt;/code&gt; only contains rows with non-null IDs, and then squares the &lt;code&gt;value&lt;/code&gt; column of each row.&lt;/p&gt;
&lt;p&gt;That’s about it for a quick intro to &lt;code&gt;sparklyr&lt;/code&gt;. You can learn more in &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, where you will find links to reference material, books, communities, sponsors, and much more.&lt;/p&gt;
&lt;h1 id="what-is-flint"&gt;What is &lt;code&gt;Flint&lt;/code&gt;?&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Flint&lt;/code&gt; is a powerful open-source library for working with time-series data in Apache Spark. First of all, it supports efficient computation of aggregate statistics on time-series data points having the same timestamp (a.k.a &lt;code&gt;summarizeCycles&lt;/code&gt; in &lt;code&gt;Flint&lt;/code&gt; nomenclature), within a given time window (a.k.a., &lt;code&gt;summarizeWindows&lt;/code&gt;), or within some given time intervals (a.k.a &lt;code&gt;summarizeIntervals&lt;/code&gt;). It can also join two or more time-series datasets based on inexact match of timestamps using asof join functions such as &lt;code&gt;LeftJoin&lt;/code&gt; and &lt;code&gt;FutureLeftJoin&lt;/code&gt;. The author of &lt;code&gt;Flint&lt;/code&gt; has outlined many more of &lt;code&gt;Flint&lt;/code&gt;’s major functionalities in &lt;a href="https://databricks.com/blog/2018/09/11/introducing-flint-a-time-series-library-for-apache-spark.html"&gt;this article&lt;/a&gt;, which I found to be extremely helpful when working out how to build &lt;code&gt;sparklyr.flint&lt;/code&gt; as a simple and straightforward R interface for such functionalities.&lt;/p&gt;
&lt;p&gt;Readers wanting some direct hands-on experience with Flint and Apache Spark can go through the following steps to run a minimal example of using Flint to analyze time-series data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, install Apache Spark locally, and then for convenience reasons, define the &lt;code&gt;SPARK_HOME&lt;/code&gt; environment variable. In this example, we will run Flint with Apache Spark 2.4.4 installed at &lt;code&gt;~/spark&lt;/code&gt;, so:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;export SPARK_HOME=~/spark/spark-2.4.4-bin-hadoop2.7&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Launch Spark shell and instruct it to download &lt;code&gt;Flint&lt;/code&gt; and its Maven dependencies:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;&amp;quot;${SPARK_HOME}&amp;quot;/bin/spark-shell --packages=com.twosigma:flint:0.6.0&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a simple Spark dataframe containing some time-series data:&lt;/p&gt;
&lt;pre class="scala"&gt;&lt;code&gt;import spark.implicits._

val ts_sdf = Seq((1L, 1), (2L, 4), (3L, 9), (4L, 16)).toDF(&amp;quot;time&amp;quot;, &amp;quot;value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Import the dataframe along with additional metadata such as time unit and name of the timestamp column into a &lt;code&gt;TimeSeriesRDD&lt;/code&gt;, so that &lt;code&gt;Flint&lt;/code&gt; can interpret the time-series data unambiguously:&lt;/p&gt;
&lt;pre class="scala"&gt;&lt;code&gt;import com.twosigma.flint.timeseries.TimeSeriesRDD

val ts_rdd = TimeSeriesRDD.fromDF(
  ts_sdf
)(
  isSorted = true, // rows are already sorted by time
  timeUnit = java.util.concurrent.TimeUnit.SECONDS,
  timeColumn = &amp;quot;time&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, after all the hard work above, we can leverage various time-series functionalities provided by &lt;code&gt;Flint&lt;/code&gt; to analyze &lt;code&gt;ts_rdd&lt;/code&gt;. For example, the following will produce a new column named &lt;code&gt;value_sum&lt;/code&gt;. For each row, &lt;code&gt;value_sum&lt;/code&gt; will contain the summation of &lt;code&gt;value&lt;/code&gt;s that occurred within the past 2 seconds from the timestamp of that row:&lt;/p&gt;
&lt;pre class="scala"&gt;&lt;code&gt;import com.twosigma.flint.timeseries.Windows
import com.twosigma.flint.timeseries.Summarizers

val window = Windows.pastAbsoluteTime(&amp;quot;2s&amp;quot;)
val summarizer = Summarizers.sum(&amp;quot;value&amp;quot;)
val result = ts_rdd.summarizeWindows(window, summarizer)

result.toDF.show()&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;    +-------------------+-----+---------+
    |               time|value|value_sum|
    +-------------------+-----+---------+
    |1970-01-01 00:00:01|    1|      1.0|
    |1970-01-01 00:00:02|    4|      5.0|
    |1970-01-01 00:00:03|    9|     14.0|
    |1970-01-01 00:00:04|   16|     29.0|
    +-------------------+-----+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;div class="line-block"&gt;     In other words, given a timestamp &lt;code&gt;t&lt;/code&gt; and a row in the result having &lt;code&gt;time&lt;/code&gt; equal to &lt;code&gt;t&lt;/code&gt;, one can notice the &lt;code&gt;value_sum&lt;/code&gt; column of that row contains sum of &lt;code&gt;value&lt;/code&gt;s within the time window of &lt;code&gt;[t - 2, t]&lt;/code&gt; from &lt;code&gt;ts_rdd&lt;/code&gt;.&lt;/div&gt;
&lt;h1 id="intro-to-sparklyr.flint"&gt;Intro to &lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;The purpose of &lt;code&gt;sparklyr.flint&lt;/code&gt; is to make time-series functionalities of &lt;code&gt;Flint&lt;/code&gt; easily accessible from &lt;code&gt;sparklyr&lt;/code&gt;. To see &lt;code&gt;sparklyr.flint&lt;/code&gt; in action, one can skim through the example in the previous section, go through the following to produce the exact R-equivalent of each step in that example, and then obtain the same summarization as the final result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First of all, install &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;sparklyr.flint&lt;/code&gt; if you haven’t done so already.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)
install.packages(&amp;quot;sparklyr.flint&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connect to Apache Spark that is running locally from &lt;code&gt;sparklyr&lt;/code&gt;, but remember to attach &lt;code&gt;sparklyr.flint&lt;/code&gt; before running &lt;code&gt;sparklyr::spark_connect&lt;/code&gt;, and then import our example time-series data to Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
library(sparklyr.flint)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4&amp;quot;)
sdf &amp;lt;- copy_to(sc, data.frame(time = seq(4), value = seq(4)^2))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convert &lt;code&gt;sdf&lt;/code&gt; above into a &lt;code&gt;TimeSeriesRDD&lt;/code&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ts_rdd &amp;lt;- fromSDF(sdf, is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And finally, run the ‘sum’ summarizer to obtain a summation of &lt;code&gt;value&lt;/code&gt;s in all past-2-second time windows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result &amp;lt;- summarize_sum(ts_rdd, column = &amp;quot;value&amp;quot;, window = in_past(&amp;quot;2s&amp;quot;))

print(result %&amp;gt;% collect())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;## # A tibble: 4 x 3
##   time                value value_sum
##   &amp;lt;dttm&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 1970-01-01 00:00:01     1         1
## 2 1970-01-01 00:00:02     4         5
## 3 1970-01-01 00:00:03     9        14
## 4 1970-01-01 00:00:04    16        29&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="why-create-a-sparklyr-extension"&gt;Why create a &lt;code&gt;sparklyr&lt;/code&gt; extension?&lt;/h1&gt;
&lt;p&gt;The alternative to making &lt;code&gt;sparklyr.flint&lt;/code&gt; a &lt;code&gt;sparklyr&lt;/code&gt; extension is to bundle all time-series functionalities it provides with &lt;code&gt;sparklyr&lt;/code&gt; itself. We decided that this would not be a good idea because of the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not all &lt;code&gt;sparklyr&lt;/code&gt; users will need those time-series functionalities&lt;/li&gt;
&lt;li&gt;&lt;code&gt;com.twosigma:flint:0.6.0&lt;/code&gt; and all Maven packages it transitively relies on are quite heavy dependency-wise&lt;/li&gt;
&lt;li&gt;Implementing an intuitive R interface for &lt;code&gt;Flint&lt;/code&gt; also takes a non-trivial number of R source files, and making all of that part of &lt;code&gt;sparklyr&lt;/code&gt; itself would be too much&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, considering all of the above, building &lt;code&gt;sparklyr.flint&lt;/code&gt; as an extension of &lt;code&gt;sparklyr&lt;/code&gt; seems to be a much more reasonable choice.&lt;/p&gt;
&lt;h1 id="current-state-of-sparklyr.flint-and-its-future-directions"&gt;Current state of &lt;code&gt;sparklyr.flint&lt;/code&gt; and its future directions&lt;/h1&gt;
&lt;p&gt;Recently &lt;code&gt;sparklyr.flint&lt;/code&gt; has had its first successful release on CRAN. At the moment, &lt;code&gt;sparklyr.flint&lt;/code&gt; only supports the &lt;code&gt;summarizeCycle&lt;/code&gt; and &lt;code&gt;summarizeWindow&lt;/code&gt; functionalities of &lt;code&gt;Flint&lt;/code&gt;, and does not yet support asof join and other useful time-series operations. While &lt;code&gt;sparklyr.flint&lt;/code&gt; contains R interfaces to most of the summarizers in &lt;code&gt;Flint&lt;/code&gt; (one can find the list of summarizers currently supported by &lt;code&gt;sparklyr.flint&lt;/code&gt; in &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/sparklyr.flint.pdf"&gt;here&lt;/a&gt;), there are still a few of them missing (e.g., the support for &lt;code&gt;OLSRegressionSummarizer&lt;/code&gt;, among others).&lt;/p&gt;
&lt;p&gt;In general, the goal of building &lt;code&gt;sparklyr.flint&lt;/code&gt; is for it to be a thin “translation layer” between &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;Flint&lt;/code&gt;. It should be as simple and intuitive as possibly can be, while supporting a rich set of &lt;code&gt;Flint&lt;/code&gt; time-series functionalities.&lt;/p&gt;
&lt;p&gt;We cordially welcome any open-source contribution towards &lt;code&gt;sparklyr.flint&lt;/code&gt;. Please visit &lt;a href="https://github.com/r-spark/sparklyr.flint/issues" class="uri"&gt;https://github.com/r-spark/sparklyr.flint/issues&lt;/a&gt; if you would like to initiate discussions, report bugs, or propose new features related to &lt;code&gt;sparklyr.flint&lt;/code&gt;, and &lt;a href="https://github.com/r-spark/sparklyr.flint/pulls" class="uri"&gt;https://github.com/r-spark/sparklyr.flint/pulls&lt;/a&gt; if you would like to send pull requests.&lt;/p&gt;
&lt;h1 id="acknowledgement"&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First and foremost, the author wishes to thank Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) for proposing the idea of creating &lt;code&gt;sparklyr.flint&lt;/code&gt; as the R interface for &lt;code&gt;Flint&lt;/code&gt;, and for his guidance on how to build it as an extension to &lt;code&gt;sparklyr&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Both Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) and Daniel (&lt;a href="https://github.com/dfalbel"&gt;@dfalbel&lt;/a&gt;) have offered numerous helpful tips on making the initial submission of &lt;code&gt;sparklyr.flint&lt;/code&gt; to CRAN successful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We really appreciate the enthusiasm from &lt;code&gt;sparklyr&lt;/code&gt; users who were willing to give &lt;code&gt;sparklyr.flint&lt;/code&gt; a try shortly after it was released on CRAN (and there were quite a few downloads of &lt;code&gt;sparklyr.flint&lt;/code&gt; in the past week according to CRAN stats, which was quite encouraging for us to see). We hope you enjoy using &lt;code&gt;sparklyr.flint&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The author is also grateful for valuable editorial suggestions from Mara (&lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;), Sigrid (&lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;), and Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) on this blog post.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">236e5d73d14c1f89b6315b649028fb28</distill:md5>
      <category>R</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</guid>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint/images/thumb.png" medium="image" type="image/png" width="126" height="77"/>
    </item>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>


&lt;p&gt;With all that is going on in the world these days, is it frivolous to talk about weather prediction? Asked in the 21st century, this is bound to be a rhetorical question. In the 1930s, when German poet Bertolt Brecht wrote the famous lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Was sind das für Zeiten, wo&lt;br /&gt;
Ein Gespräch über Bäume fast ein Verbrechen ist&lt;br /&gt;
Weil es ein Schweigen über so viele Untaten einschließt!&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(“What kind of times are these, where a conversation about trees is almost a crime, for it means silence about so many atrocities!”),&lt;/p&gt;
&lt;p&gt;he couldn’t have anticipated the responses he would get in the second half of that century, with trees symbolizing, as well as literally falling victim to, environmental pollution and climate change.&lt;/p&gt;
&lt;p&gt;Today, no lengthy justification is needed as to why prediction of atmospheric states is vital: Due to global warming, frequency and intensity of severe weather conditions – droughts, wildfires, hurricanes, heatwaves – have risen and will continue to rise. And while accurate forecasts don’t change those events per se, they constitute essential information in mitigating their consequences. This goes for atmospheric forecasts on all scales: from so-called “nowcasting” (operating on a range of about six hours), over medium-range (three to five days) and sub-seasonal (weekly/monthly), to climate forecasts (concerned with years and decades). Medium-range forecasts especially are extremely important in acute disaster prevention.&lt;/p&gt;
&lt;p&gt;This post will show how deep learning (DL) methods can be used to generate atmospheric forecasts, using a newly published &lt;a href="https://github.com/pangeo-data/WeatherBench"&gt;benchmark dataset&lt;/a&gt;&lt;span class="citation"&gt;(Rasp et al. 2020)&lt;/span&gt;. Future posts may refine the model used here and/or discuss the role of DL (“AI”) in mitigating climate change – and its implications – more globally.&lt;/p&gt;
&lt;p&gt;That said, let’s put the current endeavor in context. In a way, we have here the usual &lt;em&gt;dejà vu&lt;/em&gt; of using DL as a black-box-like, magic instrument on a task where &lt;em&gt;human knowledge&lt;/em&gt; used to be required. Of course, this characterization is overly dichotomizing; many choices are made in creating DL models, and performance is necessarily constrained by available algorithms – which may, or may not, fit the domain to be modeled to a sufficient degree.&lt;/p&gt;
&lt;p&gt;If you’ve started learning about image recognition rather recently, you may well have been using DL methods from the outset, and not have heard much about the rich set of feature engineering methods developed in pre-DL image recognition. In the context of atmospheric prediction, then, let’s begin by asking: How in the world did they do that &lt;em&gt;before&lt;/em&gt;?&lt;/p&gt;
&lt;h2 id="numerical-weather-prediction-in-a-nutshell"&gt;Numerical weather prediction in a nutshell&lt;/h2&gt;
&lt;p&gt;It is not like machine learning and/or statistics are not already used in numerical weather prediction – on the contrary. For example, every model has to start from somewhere; but raw observations are not suited to direct use as initial conditions. Instead, they have to be assimilated to the four-dimensional &lt;em&gt;grid&lt;/em&gt;&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; over which model computations are performed. At the other end, namely, model output, statistical post-processing is used to refine the predictions. And very importantly, ensemble forecasts are employed to determine uncertainty.&lt;/p&gt;
&lt;p&gt;That said, the model &lt;em&gt;core&lt;/em&gt;, the part that extrapolates into the future atmospheric conditions observed today, is based on a set of differential&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; equations, the so-called &lt;a href="https://en.wikipedia.org/wiki/Primitive_equations"&gt;primitive equations&lt;/a&gt;, that are due to the conservation laws of &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_momentum"&gt;momentum&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_energy"&gt;energy&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_mass"&gt;mass&lt;/a&gt;. These differential equations cannot be solved analytically; rather, they have to be solved numerically, and that on a grid of resolution as high as possible. In that light, even deep learning could appear as just “moderately resource-intensive” (dependent, though, on the model in question). So how, then, could a DL approach look?&lt;/p&gt;
&lt;h2 id="deep-learning-models-for-weather-prediction"&gt;Deep learning models for weather prediction&lt;/h2&gt;
&lt;p&gt;Accompanying the benchmark dataset they created, Rasp et al.&lt;span class="citation"&gt;(Rasp et al. 2020)&lt;/span&gt; provide a set of notebooks, including one demonstrating the use of a simple convolutional neural network to predict two of the available atmospheric variables, &lt;em&gt;500hPa geopotential&lt;/em&gt; and &lt;em&gt;850hPa temperature&lt;/em&gt;. Here &lt;em&gt;850hPa temperature&lt;/em&gt; is the (spatially varying) temperature at a fix atmospheric height of 850hPa (~ 1.5 kms)&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; ; &lt;em&gt;500hPa geopotential&lt;/em&gt; is proportional to the (again, spatially varying) altitude associated with the pressure level in question (500hPa).&lt;/p&gt;
&lt;p&gt;For this task, two-dimensional convnets, as usually employed in image processing, are a natural fit: Image width and height map to longitude and latitude of the spatial grid, respectively; target variables appear as channels. In this architecture, the time series character of the data is essentially lost: Every sample stands alone, without dependency on either past or present. In this respect, as well as given its size and simplicity, the convnet presented below is only a toy model, meant to introduce the approach as well as the application overall. It may also serve as a &lt;em&gt;deep learning baseline&lt;/em&gt;, along with two other types of baseline commonly used in numerical weather prediction introduced below.&lt;/p&gt;
&lt;p&gt;Directions on how to improve on that baseline are given by recent publications. Weyn et al.&lt;span class="citation"&gt;(Weyn, Durran, and Caruana, n.d.)&lt;/span&gt;, in addition to applying more geometrically-adequate spatial preprocessing, use a U-Net-based architecture instead of a plain convnet. Rasp and Thuerey &lt;span class="citation"&gt;(Rasp and Thuerey 2020)&lt;/span&gt;, building on a fully convolutional, high-capacity ResNet architecture, add a key new procedural ingredient: pre-training on climate models. With their method, they are able to not just compete with physical models, but also, show evidence of the network learning about physical structure and dependencies. Unfortunately, compute facilities of this order are not available to the average individual, which is why we’ll content ourselves with demonstrating a simple toy model. Still, having seen a simple model in action, as well as the type of data it works on, should help a lot in understanding how DL can be used for weather prediction.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/pangeo-data/WeatherBench"&gt;Weatherbench&lt;/a&gt; was explicitly created as a benchmark dataset and thus, as is common for this species, hides a lot of preprocessing and standardization effort from the user. Atmospheric data are available on an hourly basis, ranging from 1979 to 2018, at different spatial resolutions. Depending on resolution, there are about 15 to 20 measured variables, including temperature, geopotential, wind speed, and humidity. Of these variables, some are available at several pressure levels. Thus, our example makes use of a small subset of available “channels”. To save storage, network and computational resources, it also operates at the smallest available resolution.&lt;/p&gt;
&lt;p&gt;This post is accompanied by executable code on &lt;a href="https://colab.research.google.com/drive/1URSjWfcnhgesHWCqd7Un5GHECYblY04r?usp=sharing"&gt;Google Colaboratory&lt;/a&gt;, which should not just render unnecessary any copy-pasting of code snippets but also, allow for uncomplicated modification and experimentation.&lt;/p&gt;
&lt;p&gt;To read in and extract the data, stored as &lt;a href="https://www.unidata.ucar.edu/software/netcdf/"&gt;NetCDF&lt;/a&gt; files, we use &lt;a href="https://docs.ropensci.org/tidync/"&gt;tidync&lt;/a&gt;, a high-level package built on top of &lt;a href="https://cran.r-project.org/package=ncdf4"&gt;ncdf4&lt;/a&gt; and &lt;a href="https://cran.r-project.org/package=RNetCDF"&gt;RNetCDF&lt;/a&gt;. Otherwise, availability of the usual “TensorFlow family” as well as a subset of tidyverse packages is assumed.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)

library(dplyr)
library(ggplot2)
library(tidyr)
library(purrr)
library(lubridate)

library(tidync)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As already alluded to, our example makes use of two spatio-temporal series: 500hPa geopotential and 850hPa temperature. The following commands will download and unpack the respective sets of by-year files, for a spatial resolution of 5.625 degrees:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;download.file(&amp;quot;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Ftemperature_850&amp;amp;files=temperature_850_5.625deg.zip&amp;quot;,
              &amp;quot;temperature_850_5.625deg.zip&amp;quot;)
unzip(&amp;quot;temperature_850_5.625deg.zip&amp;quot;, exdir = &amp;quot;temperature_850&amp;quot;)

download.file(&amp;quot;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&amp;amp;files=geopotential_500_5.625deg.zip&amp;quot;,
              &amp;quot;geopotential_500_5.625deg.zip&amp;quot;)
unzip(&amp;quot;geopotential_500_5.625deg.zip&amp;quot;, exdir = &amp;quot;geopotential_500&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspecting one of those files’ contents, we see that its data array is structured along three dimensions, longitude (64 different values), latitude (32) and time (8760). The data itself is &lt;code&gt;z&lt;/code&gt;, the geopotential.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Class: tidync_data (list of tidync data arrays)
Variables (1): &amp;#39;z&amp;#39;
Dimension (3): lon,lat,time (64, 32, 8760)
Source: /[...]/geopotential_500/geopotential_500hPa_2015_5.625deg.nc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extraction of the data array is as easy as telling &lt;code&gt;tidync&lt;/code&gt; to read the first in the list of arrays:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;z500_2015 &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;%
                hyper_array())[[1]]

dim(z500_2015)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 64 32 8760&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While we delegate further introduction to &lt;code&gt;tidync&lt;/code&gt; to a comprehensive &lt;a href="https://ropensci.org/blog/2019/11/05/tidync/"&gt;blog post&lt;/a&gt; on the ROpenSci website, let’s at least look at a quick visualization, for which we pick the very first time point. (Extraction and visualization code is analogous for 850hPa temperature.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image(z500_2015[ , , 1],
      col = hcl.colors(20, &amp;quot;viridis&amp;quot;), # for temperature, the color scheme used is YlOrRd 
      xaxt = &amp;#39;n&amp;#39;,
      yaxt = &amp;#39;n&amp;#39;,
      main = &amp;quot;500hPa geopotential&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The maps show how pressure&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and temperature strongly depend on latitude. Furthermore, it’s easy to spot the &lt;a href="https://en.wikipedia.org/wiki/Atmospheric_wave"&gt;atmospheric waves&lt;/a&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-01-weather-prediction/images/viz.png" alt="Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h." width="726" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For training, validation and testing, we choose consecutive years: 2015, 2016, and 2017, respectively.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;z500_train &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_train &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

z500_valid &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2016_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_valid &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2016_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

z500_test &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2017_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_test &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2017_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since geopotential and temperature will be treated as channels, we concatenate the corresponding arrays. To transform the data into the format needed for images, a permutation is necessary:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_all &amp;lt;- abind::abind(z500_train, t850_train, along = 4)
train_all &amp;lt;- aperm(train_all, perm = c(3, 2, 1, 4))
dim(train_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8760 32 64 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All data will be standardized according to mean and standard deviation as obtained from the training set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;level_means &amp;lt;- apply(train_all, 4, mean)
level_sds &amp;lt;- apply(train_all, 4, sd)

round(level_means, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;54124.91  274.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In words, the mean geopotential height (see footnote 5 for more on this term), as measured at an isobaric surface of 500hPa, amounts to about 5400 metres&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, while the mean temperature at the 850hPa level approximates 275 Kelvin (about 2 degrees Celsius).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train &amp;lt;- train_all
train[, , , 1] &amp;lt;- (train[, , , 1] - level_means[1]) / level_sds[1]
train[, , , 2] &amp;lt;- (train[, , , 2] - level_means[2]) / level_sds[2]

valid_all &amp;lt;- abind::abind(z500_valid, t850_valid, along = 4)
valid_all &amp;lt;- aperm(valid_all, perm = c(3, 2, 1, 4))

valid &amp;lt;- valid_all
valid[, , , 1] &amp;lt;- (valid[, , , 1] - level_means[1]) / level_sds[1]
valid[, , , 2] &amp;lt;- (valid[, , , 2] - level_means[2]) / level_sds[2]

test_all &amp;lt;- abind::abind(z500_test, t850_test, along = 4)
test_all &amp;lt;- aperm(test_all, perm = c(3, 2, 1, 4))

test &amp;lt;- test_all
test[, , , 1] &amp;lt;- (test[, , , 1] - level_means[1]) / level_sds[1]
test[, , , 2] &amp;lt;- (test[, , , 2] - level_means[2]) / level_sds[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll attempt to predict three days ahead.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lead_time &amp;lt;- 3 * 24 # 3d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all that remains to be done is construct the actual &lt;em&gt;datasets&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 32

train_x &amp;lt;- train %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(train)[1] - lead_time)

train_y &amp;lt;- train %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

train_ds &amp;lt;- zip_datasets(train_x, train_y) %&amp;gt;%
  dataset_shuffle(buffer_size = dim(train)[1] - lead_time) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)

valid_x &amp;lt;- valid %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(valid)[1] - lead_time)

valid_y &amp;lt;- valid %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

valid_ds &amp;lt;- zip_datasets(valid_x, valid_y) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)

test_x &amp;lt;- test %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(test)[1] - lead_time)

test_y &amp;lt;- test %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

test_ds &amp;lt;- zip_datasets(test_x, test_y) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s proceed to defining the model.&lt;/p&gt;
&lt;h2 id="basic-cnn-with-periodic-convolutions"&gt;Basic CNN with periodic convolutions&lt;/h2&gt;
&lt;p&gt;The model is a straightforward convnet, with one exception: Instead of plain convolutions, it uses slightly more sophisticated ones that “wrap around” longitudinally.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;periodic_padding_2d &amp;lt;- function(pad_width,
                                name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$pad_width &amp;lt;- pad_width
    
    function (x, mask = NULL) {
      x &amp;lt;- if (self$pad_width == 0) {
        x
      } else {
        lon_dim &amp;lt;- dim(x)[3]
        pad_width &amp;lt;- tf$cast(self$pad_width, tf$int32)
        # wrap around for longitude
        tf$concat(list(x[, ,-pad_width:lon_dim,],
                       x,
                       x[, , 1:pad_width,]),
                  axis = 2L) %&amp;gt;%
          tf$pad(list(
            list(0L, 0L),
            # zero-pad for latitude
            list(pad_width, pad_width),
            list(0L, 0L),
            list(0L, 0L)
          ))
      }
    }
  })
}

periodic_conv_2d &amp;lt;- function(filters,
                             kernel_size,
                             name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$padding &amp;lt;- periodic_padding_2d(pad_width = (kernel_size - 1) / 2)
    self$conv &amp;lt;-
      layer_conv_2d(filters = filters,
                    kernel_size = kernel_size,
                    padding = &amp;#39;valid&amp;#39;)
    
    function (x, mask = NULL) {
      x %&amp;gt;% self$padding() %&amp;gt;% self$conv()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our purposes of establishing a deep-learning baseline that is fast to train, CNN architecture and parameter defaults are chosen to be simple and moderate, respectively:&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;periodic_cnn &amp;lt;- function(filters = c(64, 64, 64, 64, 2),
                         kernel_size = c(5, 5, 5, 5, 5),
                         dropout = rep(0.2, 5),
                         name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$conv1 &amp;lt;-
      periodic_conv_2d(filters = filters[1], kernel_size = kernel_size[1])
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$drop1 &amp;lt;- layer_dropout(rate = dropout[1])
    self$conv2 &amp;lt;-
      periodic_conv_2d(filters = filters[2], kernel_size = kernel_size[2])
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$drop2 &amp;lt;- layer_dropout(rate =dropout[2])
    self$conv3 &amp;lt;-
      periodic_conv_2d(filters = filters[3], kernel_size = kernel_size[3])
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$drop3 &amp;lt;- layer_dropout(rate = dropout[3])
    self$conv4 &amp;lt;-
      periodic_conv_2d(filters = filters[4], kernel_size = kernel_size[4])
    self$act4 &amp;lt;- layer_activation_leaky_relu()
    self$drop4 &amp;lt;- layer_dropout(rate = dropout[4])
    self$conv5 &amp;lt;-
      periodic_conv_2d(filters = filters[5], kernel_size = kernel_size[5])
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$drop1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$drop2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$drop3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$act4() %&amp;gt;%
        self$drop4() %&amp;gt;%
        self$conv5()
    }
  })
}

model &amp;lt;- periodic_cnn()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;In that same spirit of “default-ness”, we train with MSE loss and Adam optimizer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)
optimizer &amp;lt;- optimizer_adam()

train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)

valid_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;test_loss&amp;#39;)

train_step &amp;lt;- function(train_batch) {

  with (tf$GradientTape() %as% tape, {
    predictions &amp;lt;- model(train_batch[[1]])
    l &amp;lt;- loss(train_batch[[2]], predictions)
  })

  gradients &amp;lt;- tape$gradient(l, model$trainable_variables)
  optimizer$apply_gradients(purrr::transpose(list(
    gradients, model$trainable_variables
  )))

  train_loss(l)

}

valid_step &amp;lt;- function(valid_batch) {
  predictions &amp;lt;- model(valid_batch[[1]])
  l &amp;lt;- loss(valid_batch[[2]], predictions)
  
  valid_loss(l)
}

training_loop &amp;lt;- tf_function(autograph(function(train_ds, valid_ds, epoch) {
  
  for (train_batch in train_ds) {
    train_step(train_batch)
  }
  
  for (valid_batch in valid_ds) {
    valid_step(valid_batch)
  }
  
  tf$print(&amp;quot;MSE: train: &amp;quot;, train_loss$result(), &amp;quot;, validation: &amp;quot;, valid_loss$result()) 
    
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depicted graphically, we see that the model trains well, but extrapolation does not surpass a certain threshold (which is reached early, after training for just two epochs).&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-01-weather-prediction/images/history.png" alt="MSE per epoch on training and validation sets." width="363" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-16)MSE per epoch on training and validation sets.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not too surprising though, given the model’s architectural simplicity and modest size.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Here, we first present two other baselines, which – given a highly complex and chaotic system like the atmosphere – may sound irritatingly simple and yet, be pretty hard to beat. The metric used for comparison is &lt;em&gt;latitudinally weighted root-mean-square error&lt;/em&gt;. Latitudinal weighting up-weights the lower latitudes and down-weights the upper ones.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;deg2rad &amp;lt;- function(d) {
  (d / 180) * pi
}

lats &amp;lt;- tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;)$transforms$lat %&amp;gt;%
  select(lat) %&amp;gt;%
  pull()

lat_weights &amp;lt;- cos(deg2rad(lats))
lat_weights &amp;lt;- lat_weights / mean(lat_weights)

weighted_rmse &amp;lt;- function(forecast, ground_truth) {
  error &amp;lt;- (forecast - ground_truth) ^ 2
  for (i in seq_along(lat_weights)) {
    error[, i, ,] &amp;lt;- error[, i, ,] * lat_weights[i]
  }
  apply(error, 4, mean) %&amp;gt;% sqrt()
}&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-1-weekly-climatology"&gt;Baseline 1: Weekly climatology&lt;/h4&gt;
&lt;p&gt;In general, climatology refers to long-term averages computed over defined time ranges. Here, we first calculate weekly averages based on the training set. These averages are then used to forecast the variables in question for the time period used as test set.&lt;/p&gt;
&lt;p&gt;Step one makes use of &lt;code&gt;tidync&lt;/code&gt;, &lt;code&gt;ncmeta&lt;/code&gt;, &lt;code&gt;RNetCDF&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt; to compute weekly averages for 2015, following the &lt;a href="https://en.wikipedia.org/wiki/ISO_week_date"&gt;ISO week date system&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_file &amp;lt;- &amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;

times_train &amp;lt;- (tidync(train_file) %&amp;gt;% activate(&amp;quot;D2&amp;quot;) %&amp;gt;% hyper_array())$time

time_unit_train &amp;lt;- ncmeta::nc_atts(train_file, &amp;quot;time&amp;quot;) %&amp;gt;%
  tidyr::unnest(cols = c(value)) %&amp;gt;%
  dplyr::filter(name == &amp;quot;units&amp;quot;)

time_parts_train &amp;lt;- RNetCDF::utcal.nc(time_unit_train$value, times_train)

iso_train &amp;lt;- ISOdate(
  time_parts_train[, &amp;quot;year&amp;quot;],
  time_parts_train[, &amp;quot;month&amp;quot;],
  time_parts_train[, &amp;quot;day&amp;quot;],
  time_parts_train[, &amp;quot;hour&amp;quot;],
  time_parts_train[, &amp;quot;minute&amp;quot;],
  time_parts_train[, &amp;quot;second&amp;quot;]
)

isoweeks_train &amp;lt;- map(iso_train, isoweek) %&amp;gt;% unlist()

train_by_week &amp;lt;- apply(train_all, c(2, 3, 4), function(x) {
  tapply(x, isoweeks_train, function(y) {
    mean(y)
  })
})

dim(train_by_week)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;53 32 64 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step two then runs through the test set, mapping dates to corresponding ISO weeks and associating the weekly averages from the training set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_file &amp;lt;- &amp;quot;geopotential_500/geopotential_500hPa_2017_5.625deg.nc&amp;quot;

times_test &amp;lt;- (tidync(test_file) %&amp;gt;% activate(&amp;quot;D2&amp;quot;) %&amp;gt;% hyper_array())$time

time_unit_test &amp;lt;- ncmeta::nc_atts(test_file, &amp;quot;time&amp;quot;) %&amp;gt;%
  tidyr::unnest(cols = c(value)) %&amp;gt;%
  dplyr::filter(name == &amp;quot;units&amp;quot;)

time_parts_test &amp;lt;- RNetCDF::utcal.nc(time_unit_test$value, times_test)

iso_test &amp;lt;- ISOdate(
  time_parts_test[, &amp;quot;year&amp;quot;],
  time_parts_test[, &amp;quot;month&amp;quot;],
  time_parts_test[, &amp;quot;day&amp;quot;],
  time_parts_test[, &amp;quot;hour&amp;quot;],
  time_parts_test[, &amp;quot;minute&amp;quot;],
  time_parts_test[, &amp;quot;second&amp;quot;]
)

isoweeks_test &amp;lt;- map(iso_test, isoweek) %&amp;gt;% unlist()

climatology_forecast &amp;lt;- test_all

for (i in 1:dim(climatology_forecast)[1]) {
  week &amp;lt;- isoweeks_test[i]
  lookup &amp;lt;- train_by_week[week, , , ]
  climatology_forecast[i, , ,] &amp;lt;- lookup
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this baseline, the latitudinally-weighted RMSE amounts to roughly 975 for geopotential and 4 for temperature.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;wrmse &amp;lt;- weighted_rmse(climatology_forecast, test_all)
round(wrmse, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;974.50   4.09&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-2-persistence-forecast"&gt;Baseline 2: Persistence forecast&lt;/h4&gt;
&lt;p&gt;The second baseline commonly used makes a straightforward assumption: Tomorrow’s weather is today’s weather, or, in our case: In three days, things will be just like they are now.&lt;/p&gt;
&lt;p&gt;Computation for this metric is almost a one-liner. And as it turns out, for the given lead time (three days), performance is not too dissimilar from obtained by means of weekly climatology:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;persistence_forecast &amp;lt;- test_all[1:(dim(test_all)[1] - lead_time), , ,]

test_period &amp;lt;- test_all[(lead_time + 1):dim(test_all)[1], , ,]

wrmse &amp;lt;- weighted_rmse(persistence_forecast, test_period)

round(wrmse, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;937.55  4.31&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-3-simple-convnet"&gt;Baseline 3: Simple convnet&lt;/h4&gt;
&lt;p&gt;How does the simple deep learning model stack up against those two?&lt;/p&gt;
&lt;p&gt;To answer that question, we first need to obtain predictions on the test set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_wrmses &amp;lt;- data.frame()

test_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;test_loss&amp;#39;)

test_step &amp;lt;- function(test_batch, batch_index) {
  predictions &amp;lt;- model(test_batch[[1]])
  l &amp;lt;- loss(test_batch[[2]], predictions)
  
  predictions &amp;lt;- predictions %&amp;gt;% as.array()
  predictions[, , , 1] &amp;lt;- predictions[, , , 1] * level_sds[1] + level_means[1]
  predictions[, , , 2] &amp;lt;- predictions[, , , 2] * level_sds[2] + level_means[2]
  
  wrmse &amp;lt;- weighted_rmse(predictions, test_all[batch_index:(batch_index + 31), , ,])
  test_wrmses &amp;lt;&amp;lt;- test_wrmses %&amp;gt;% bind_rows(c(z = wrmse[1], temp = wrmse[2]))

  test_loss(l)
}

test_iterator &amp;lt;- as_iterator(test_ds)

batch_index &amp;lt;- 0
while (TRUE) {
  test_batch &amp;lt;- test_iterator %&amp;gt;% iter_next()
  if (is.null(test_batch))
    break
  batch_index &amp;lt;- batch_index + 1
  test_step(test_batch, as.integer(batch_index))
}

test_loss$result() %&amp;gt;% as.numeric()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3821.016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, average loss on the test set parallels that seen on the validation set. As to latitudinally weighted RMSE, it turns out to be higher for the DL baseline than for the other two:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;apply(test_wrmses, 2, mean) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      z    temp 
1521.47    7.70 &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;At first glance, seeing the DL baseline perform worse than the others might feel anticlimactic. But if you think about it, there is no need to be disappointed.&lt;/p&gt;
&lt;p&gt;For one, given the enormous complexity of the task, these heuristics are not as easy to outsmart. Take persistence: Depending on lead time - how far into the future we’re forecasting - the wisest guess may actually be that everything will stay the same. What would you guess the weather will look like in five minutes? — Same with weekly climatology: Looking back at how warm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.&lt;/p&gt;
&lt;p&gt;Second, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and powerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical models (cf. especially Rasp and Thuerey &lt;span class="citation"&gt;(Rasp and Thuerey 2020)&lt;/span&gt; already mentioned above). Unfortunately, models like that need to be trained on &lt;em&gt;a lot&lt;/em&gt; of data.&lt;/p&gt;
&lt;p&gt;However, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for individuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-weatherbench"&gt;
&lt;p&gt;Rasp, Stephan, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. 2020. “WeatherBench: A benchmark dataset for data-driven weather forecasting.” &lt;em&gt;arXiv E-Prints&lt;/em&gt;, February, arXiv:2002.00469. &lt;a href="http://arxiv.org/abs/2002.00469"&gt;http://arxiv.org/abs/2002.00469&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rasp2020purely"&gt;
&lt;p&gt;Rasp, Stephan, and Nils Thuerey. 2020. “Purely Data-Driven Medium-Range Weather Forecasting Achieves Comparable Skill to Physical Models at Similar Resolution.” &lt;a href="http://arxiv.org/abs/2008.08626"&gt;http://arxiv.org/abs/2008.08626&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-weyn"&gt;
&lt;p&gt;Weyn, Jonathan A., Dale R. Durran, and Rich Caruana. n.d. “Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere.” &lt;em&gt;Journal of Advances in Modeling Earth Systems&lt;/em&gt; n/a (n/a): e2020MS002109. &lt;a href="https://doi.org/10.1029/2020MS002109"&gt;https://doi.org/10.1029/2020MS002109&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;em&gt;An die Nachgeborenen, 1934-38.&lt;/em&gt; The atrocities referred to are those of Nazi Germany.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Four, because in addition to three spatial dimensions, there is the time dimension.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;mostly&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;Pressure and altitude are related by the &lt;a href="https://en.wikipedia.org/wiki/Barometric_formula"&gt;barometric equation&lt;/a&gt;. On weather maps, pressure is often used to represent the vertical dimension.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Whereas we normally might think of atmospheric pressure as varying at a fixed height (for example, at sea level), meteorologists like to display things the other way round, displaying variable heights at fixed constant-pressure (isobaric) surfaces. Still, intuitively these may be read in the same way: High pressure at a given location means that some predefined pressure level is attained at &lt;em&gt;higher altitude&lt;/em&gt; than in some low-pressure location. To be precise, these kinds of “inverted pressure maps” normally display &lt;em&gt;geopotential height&lt;/em&gt;, measured in metres, not &lt;em&gt;geopotential&lt;/em&gt;, the variable we’re dealing with here. &lt;em&gt;Geopotential&lt;/em&gt; (without the “height”) refers to gravitational potential energy per unit mass; it is obtained by multiplying by gravitational acceleration, and is measured in metres squared per second squared. The measures are not a hundred percent equivalent, because gravitational acceleration varies with latitude and longitude as well as elevation.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;As explained in the previous footnote, geopotential height is geopotential divided by standard gravitational acceleration, roughly, 9.8 metres per seconds squared.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;These are the same filter and kernel sizes as employed in &lt;a href="https://github.com/pangeo-data/WeatherBench/blob/master/notebooks/3-cnn-example.ipynb"&gt;Rasp et al.'s simple CNN example on github&lt;/a&gt;.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">47cf15552b82b021f49c79654e45691e</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="600" height="332"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>


&lt;p&gt;&lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt; &lt;span class="citation"&gt;(Deng et al. 2009)&lt;/span&gt; is an image database organized according to the &lt;a href="http://wordnet.princeton.edu/"&gt;WordNet&lt;/a&gt; &lt;span class="citation"&gt;(Miller 1995)&lt;/span&gt; hierarchy which, historically, has been used in computer vision benchmarks and research. However, it was not until AlexNet &lt;span class="citation"&gt;(Krizhevsky, Sutskever, and Hinton 2012)&lt;/span&gt; demonstrated the efficiency of deep learning using convolutional neural networks on GPUs that the computer-vision discipline turned to deep learning to achieve state-of-the-art models that revolutionized their field. Given the importance of ImageNet and AlexNet, this post introduces tools and techniques to consider when training ImageNet and other large-scale datasets with R.&lt;/p&gt;
&lt;p&gt;Now, in order to process ImageNet, we will first have to &lt;em&gt;divide and conquer&lt;/em&gt;, partitioning the dataset into several manageable subsets. Afterwards, we will train ImageNet using AlexNet across multiple GPUs and compute instances. &lt;a href="#preprocessing-imagenet"&gt;Preprocessing ImageNet&lt;/a&gt; and &lt;a href="#distributed-training"&gt;distributed training&lt;/a&gt; are the two topics that this post will present and discuss, starting with preprocessing ImageNet.&lt;/p&gt;
&lt;h2 id="preprocessing-imagenet"&gt;Preprocessing ImageNet&lt;/h2&gt;
&lt;p&gt;When dealing with large datasets, even simple tasks like downloading or reading a dataset can be much harder than what you would expect. For instance, since ImageNet is roughly 300GB in size, you will need to make sure to have at least 600GB of free space to leave some room for download and decompression. But no worries, you can always borrow computers with huge disk drives from your favorite cloud provider. While you are at it, you should also request compute instances with multiple GPUs, Solid State Drives (SSDs), and a reasonable amount of CPUs and memory. If you want to use the exact configuration we used, take a look at the &lt;a href="https://github.com/mlverse/imagenet"&gt;mlverse/imagenet&lt;/a&gt; repo, which contains a Docker image and configuration commands required to provision reasonable computing resources for this task. In summary, make sure you have access to sufficient compute resources.&lt;/p&gt;
&lt;p&gt;Now that we have resources capable of working with ImageNet, we need to find a place to download ImageNet from. The easiest way is to use a variation of ImageNet used in the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale Visual Recognition Challenge (ILSVRC)&lt;/a&gt;, which contains a subset of about 250GB of data and can be easily downloaded from many &lt;a href="https://kaggle.com"&gt;Kaggle&lt;/a&gt; competitions, like the &lt;a href="https://www.kaggle.com/c/imagenet-object-localization-challenge"&gt;ImageNet Object Localization Challenge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’ve read some of our previous posts, you might be already thinking of using the &lt;a href="https://pins.rstudio.com"&gt;pins&lt;/a&gt; package, which you can use to: cache, discover and share resources from many services, including Kaggle. You can learn more about data retrieval from Kaggle in the &lt;a href="http://pins.rstudio.com/articles/boards-kaggle.html"&gt;Using Kaggle Boards&lt;/a&gt; article; in the meantime, let’s assume you are already familiar with this package.&lt;/p&gt;
&lt;p&gt;All we need to do now is register the Kaggle board, retrieve ImageNet as a pin, and decompress this file. Warning, the following code requires you to stare at a progress bar for, potentially, over an hour.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)
board_register(&amp;quot;kaggle&amp;quot;, token = &amp;quot;kaggle.json&amp;quot;)

pin_get(&amp;quot;c/imagenet-object-localization-challenge&amp;quot;, board = &amp;quot;kaggle&amp;quot;)[1] %&amp;gt;%
  untar(exdir = &amp;quot;/localssd/imagenet/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we are going to be training this model over and over using multiple GPUs and even multiple compute instances, we want to make sure we don’t waste too much time downloading ImageNet every single time.&lt;/p&gt;
&lt;p&gt;The first improvement to consider is getting a faster hard drive. In our case, we locally-mounted an array of SSDs into the &lt;code&gt;/localssd&lt;/code&gt; path. We then used &lt;code&gt;/localssd&lt;/code&gt; to extract ImageNet and configured R’s temp path and pins cache to use the SSDs as well. Consult your cloud provider’s documentation to configure SSDs, or take a look at &lt;a href="https://github.com/mlverse/imagenet"&gt;mlverse/imagenet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, a well-known approach we can follow is to partition ImageNet into chunks that can be individually downloaded to perform distributed training later on.&lt;/p&gt;
&lt;p&gt;In addition, it is also faster to download ImageNet from a nearby location, ideally from a URL stored within the same data center where our cloud instance is located. For this, we can also use pins to register a board with our cloud provider and then re-upload each partition. Since ImageNet is already partitioned by category, we can easily split ImageNet into multiple zip files and re-upload to our closest data center as follows. Make sure the storage bucket is created in the same region as your computing instances.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register(&amp;quot;&amp;lt;board&amp;gt;&amp;quot;, name = &amp;quot;imagenet&amp;quot;, bucket = &amp;quot;r-imagenet&amp;quot;)

train_path &amp;lt;- &amp;quot;/localssd/imagenet/ILSVRC/Data/CLS-LOC/train/&amp;quot;
for (path in dir(train_path, full.names = TRUE)) {
  dir(path, full.names = TRUE) %&amp;gt;%
    pin(name = basename(path), board = &amp;quot;imagenet&amp;quot;, zip = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now retrieve a subset of ImageNet quite efficiently. If you are motivated to do so and have about one gigabyte to spare, feel free to follow along executing this code. Notice that ImageNet contains &lt;em&gt;lots&lt;/em&gt; of JPEG images for each WordNet category.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register(&amp;quot;https://storage.googleapis.com/r-imagenet/&amp;quot;, &amp;quot;imagenet&amp;quot;)

categories &amp;lt;- pin_get(&amp;quot;categories&amp;quot;, board = &amp;quot;imagenet&amp;quot;)
pin_get(categories$id[1], board = &amp;quot;imagenet&amp;quot;, extract = TRUE) %&amp;gt;%
  tibble::as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,300 x 1
   value                                                           
   &amp;lt;chr&amp;gt;                                                           
 1 /localssd/pins/storage/n01440764/n01440764_10026.JPEG
 2 /localssd/pins/storage/n01440764/n01440764_10027.JPEG
 3 /localssd/pins/storage/n01440764/n01440764_10029.JPEG
 4 /localssd/pins/storage/n01440764/n01440764_10040.JPEG
 5 /localssd/pins/storage/n01440764/n01440764_10042.JPEG
 6 /localssd/pins/storage/n01440764/n01440764_10043.JPEG
 7 /localssd/pins/storage/n01440764/n01440764_10048.JPEG
 8 /localssd/pins/storage/n01440764/n01440764_10066.JPEG
 9 /localssd/pins/storage/n01440764/n01440764_10074.JPEG
10 /localssd/pins/storage/n01440764/n01440764_1009.JPEG 
# … with 1,290 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When doing distributed training over ImageNet, we can now let a single compute instance process a partition of ImageNet with ease. Say, 1/16 of ImageNet can be retrieved and extracted, in under a minute, using parallel downloads with the &lt;a href="https://callr.r-lib.org/"&gt;callr&lt;/a&gt; package:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;categories &amp;lt;- pin_get(&amp;quot;categories&amp;quot;, board = &amp;quot;imagenet&amp;quot;)
categories &amp;lt;- categories$id[1:(length(categories$id) / 16)]

procs &amp;lt;- lapply(categories, function(cat)
  callr::r_bg(function(cat) {
    library(pins)
    board_register(&amp;quot;https://storage.googleapis.com/r-imagenet/&amp;quot;, &amp;quot;imagenet&amp;quot;)
    
    pin_get(cat, board = &amp;quot;imagenet&amp;quot;, extract = TRUE)
  }, args = list(cat))
)
  
while (any(sapply(procs, function(p) p$is_alive()))) Sys.sleep(1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can wrap this up partition in a list containing a map of images and categories, which we will later use in our AlexNet model through &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/"&gt;tfdatasets&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- list(
    image = unlist(lapply(categories, function(cat) {
        pin_get(cat, board = &amp;quot;imagenet&amp;quot;, download = FALSE)
    })),
    category = unlist(lapply(categories, function(cat) {
        rep(cat, length(pin_get(cat, board = &amp;quot;imagenet&amp;quot;, download = FALSE)))
    })),
    categories = categories
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! We are halfway there training ImageNet. The next section will focus on introducing distributed training using multiple GPUs.&lt;/p&gt;
&lt;h2 id="distributed-training"&gt;Distributed Training&lt;/h2&gt;
&lt;p&gt;Now that we have broken down ImageNet into manageable parts, we can forget for a second about the size of ImageNet and focus on training a deep learning model for this dataset. However, any model we choose is likely to require a GPU, even for a 1/16 subset of ImageNet. So make sure your GPUs are properly configured by running &lt;code&gt;is_gpu_available()&lt;/code&gt;. If you need help getting a GPU configured, the &lt;a href="https://www.youtube.com/watch?v=i5Bjm3jG_d8"&gt;Using GPUs with TensorFlow and Docker&lt;/a&gt; video can help you get up to speed.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tf$test$is_gpu_available()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now decide which deep learning model would best be suited for ImageNet classification tasks. Instead, for this post, we will go back in time to the glory days of AlexNet and use the &lt;a href="https://github.com/r-tensorflow/alexnet"&gt;r-tensorflow/alexnet&lt;/a&gt; repo instead. This repo contains a port of AlexNet to R, but please notice that this port has not been tested and is not ready for any real use cases. In fact, we would appreciate PRs to improve it if someone feels inclined to do so. Regardless, the focus of this post is on workflows and tools, not about achieving state-of-the-art image classification scores. So by all means, feel free to use more appropriate models.&lt;/p&gt;
&lt;p&gt;Once we’ve chosen a model, we will want to me make sure that it properly trains on a subset of ImageNet:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;remotes::install_github(&amp;quot;r-tensorflow/alexnet&amp;quot;)
alexnet::alexnet_train(data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/2
 103/2269 [&amp;gt;...............] - ETA: 5:52 - loss: 72306.4531 - accuracy: 0.9748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! However, this post is about enabling large-scale training across multiple GPUs, so we want to make sure we are using as many as we can. Unfortunately, running &lt;code&gt;nvidia-smi&lt;/code&gt; will show that only one GPU currently being used:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |
| N/A   48C    P0    89W / 149W |  10935MiB / 11441MiB |     28%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |
| N/A   74C    P0    74W / 149W |     71MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to train across multiple GPUs, we need to define a distributed-processing strategy. If this is a new concept, it might be a good time to take a look at the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/distributed/distributed_training_with_keras/"&gt;Distributed Training with Keras&lt;/a&gt; tutorial and the &lt;a href="https://www.tensorflow.org/guide/distributed_training"&gt;distributed training with TensorFlow&lt;/a&gt; docs. Or, if you allow us to oversimplify the process, all you have to do is define and compile your model under the right scope. A step-by-step explanation is available in the &lt;a href="https://www.youtube.com/watch?v=DQyLTlD1IBc"&gt;Distributed Deep Learning with TensorFlow and R&lt;/a&gt; video. In this case, the &lt;code&gt;alexnet&lt;/code&gt; model &lt;a href="https://github.com/r-tensorflow/alexnet/blob/57546/R/alexnet_train.R#L92-L94"&gt;already supports&lt;/a&gt; a strategy parameter, so all we have to do is pass it along.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
strategy &amp;lt;- tf$distribute$MirroredStrategy(
  cross_device_ops = tf$distribute$ReductionToOneDevice())

alexnet::alexnet_train(data = data, strategy = strategy, parallel = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice also &lt;code&gt;parallel = 6&lt;/code&gt; which configures &lt;code&gt;tfdatasets&lt;/code&gt; to make use of multiple CPUs when loading data into our GPUs, see &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/#parallel-mapping"&gt;Parallel Mapping&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;We can now re-run &lt;code&gt;nvidia-smi&lt;/code&gt; to validate all our GPUs are being used:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |
| N/A   49C    P0    94W / 149W |  10936MiB / 11441MiB |     53%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |
| N/A   76C    P0   114W / 149W |  10936MiB / 11441MiB |     26%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;MirroredStrategy&lt;/code&gt; can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on &lt;a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/"&gt;Training Imagenet in 18 Minutes&lt;/a&gt;). So where do we go from here?&lt;/p&gt;
&lt;p&gt;Welcome to &lt;code&gt;MultiWorkerMirroredStrategy&lt;/code&gt;: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a &lt;code&gt;TF_CONFIG&lt;/code&gt; environment variable with the right addresses and run the exact same code in each compute instance.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)

partition &amp;lt;- 0
Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(
    cluster = list(
        worker = c(&amp;quot;10.100.10.100:10090&amp;quot;, &amp;quot;10.100.10.101:10090&amp;quot;)
    ),
    task = list(type = &amp;#39;worker&amp;#39;, index = partition)
), auto_unbox = TRUE))

strategy &amp;lt;- tf$distribute$MultiWorkerMirroredStrategy(
  cross_device_ops = tf$distribute$ReductionToOneDevice())

alexnet::imagenet_partition(partition = partition) %&amp;gt;%
  alexnet::alexnet_train(strategy = strategy, parallel = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that &lt;code&gt;partition&lt;/code&gt; must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, &lt;code&gt;data&lt;/code&gt; should point to a different partition of ImageNet, which we can retrieve with &lt;code&gt;pins&lt;/code&gt;; although, for convenience, &lt;code&gt;alexnet&lt;/code&gt; contains similar code under &lt;code&gt;alexnet::imagenet_partition()&lt;/code&gt;. Other than that, the code that you need to run in each compute instance is exactly the same.&lt;/p&gt;
&lt;p&gt;However, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/#barrier-execution"&gt;barrier execution&lt;/a&gt;. If you are new to Spark, there are many resources available at &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;. To learn just about running Spark and TensorFlow together, watch our &lt;a href="https://www.youtube.com/watch?v=Zm20P3ADa14"&gt;Deep Learning with Spark, TensorFlow and R&lt;/a&gt; video.&lt;/p&gt;
&lt;p&gt;Putting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
sc &amp;lt;- spark_connect(&amp;quot;yarn|mesos|etc&amp;quot;, config = list(&amp;quot;sparklyr.shell.num-executors&amp;quot; = 16))

sdf_len(sc, 16, repartition = 16) %&amp;gt;%
  spark_apply(function(df, barrier) {
      library(tensorflow)

      Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(
        cluster = list(
          worker = paste(
            gsub(&amp;quot;:[0-9]+$&amp;quot;, &amp;quot;&amp;quot;, barrier$address),
            8000 + seq_along(barrier$address), sep = &amp;quot;:&amp;quot;)),
        task = list(type = &amp;#39;worker&amp;#39;, index = barrier$partition)
      ), auto_unbox = TRUE))
      
      if (is.null(tf_version())) install_tensorflow()
      
      strategy &amp;lt;- tf$distribute$MultiWorkerMirroredStrategy()
    
      result &amp;lt;- alexnet::imagenet_partition(partition = barrier$partition) %&amp;gt;%
        alexnet::alexnet_train(strategy = strategy, epochs = 10, parallel = 6)
      
      result$metrics$accuracy
  }, barrier = TRUE, columns = c(accuracy = &amp;quot;numeric&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-deng2009imagenet"&gt;
&lt;p&gt;Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In &lt;em&gt;2009 Ieee Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, 248–55. Ieee.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-krizhevsky2012imagenet"&gt;
&lt;p&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 1097–1105.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-miller1995wordnet"&gt;
&lt;p&gt;Miller, George A. 1995. “WordNet: A Lexical Database for English.” &lt;em&gt;Communications of the ACM&lt;/em&gt; 38 (11): 39–41.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">957d307b9483c9dd27777f72c326943d</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>


&lt;p&gt;This post did not end up quite the way I’d imagined. A quick follow-up on the recent &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-20-fnn-lstm/"&gt;Time series prediction with FNN-LSTM&lt;/a&gt;, it was supposed to demonstrate how &lt;em&gt;noisy&lt;/em&gt; time series (so common in practice) could profit from a change in architecture: Instead of FNN-LSTM, an LSTM autoencoder regularized by false nearest neighbors (FNN) loss, use FNN-VAE, a variational autoencoder constrained by the same. However, FNN-VAE did not seem to handle noise better than FNN-LSTM. No plot, no post, then?&lt;/p&gt;
&lt;p&gt;On the other hand – this is not a scientific study, with hypothesis and experimental setup all preregistered; all that really matters is if there’s something useful to report. And it looks like there is.&lt;/p&gt;
&lt;p&gt;Firstly, FNN-VAE, while on par performance-wise with FNN-LSTM, is far superior in that other meaning of “performance”: Training goes a &lt;em&gt;lot&lt;/em&gt; faster for FNN-VAE.&lt;/p&gt;
&lt;p&gt;Secondly, while we don’t see much difference between FNN-LSTM and FNN-VAE, we &lt;em&gt;do&lt;/em&gt; see a clear impact of using FNN loss. Adding in FNN loss strongly reduces mean squared error with respect to the underlying (denoised) series – especially in the case of VAE, but for LSTM as well. This is of particular interest with VAE, as it comes with a regularizer out-of-the-box – namely, Kullback-Leibler (KL) divergence.&lt;/p&gt;
&lt;p&gt;Of course, we don’t claim that similar results will always be obtained on other noisy series; nor did we tune any of the models “to death”. For what could be the intent of such a post but to show our readers interesting (and promising) ideas to pursue in their own experimentation?&lt;/p&gt;
&lt;h2 id="the-context"&gt;The context&lt;/h2&gt;
&lt;p&gt;This post is the third in a mini-series.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;Deep attractors: Where deep learning meets chaos&lt;/a&gt;, we explained, with a substantial detour into chaos theory, the idea of FNN loss, introduced in &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;. Please consult that first post for theoretical background and intuitions behind the technique.&lt;/p&gt;
&lt;p&gt;The subsequent post, &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-20-fnn-lstm/"&gt;Time series prediction with FNN-LSTM&lt;/a&gt;, showed how to use an LSTM autoencoder, constrained by FNN loss, for forecasting (as opposed to reconstructing an attractor). The results were stunning: In multi-step prediction (12-120 steps, with that number varying by dataset), the short-term forecasts were drastically improved by adding in FNN regularization. See that second post for experimental setup and results on four very different, non-synthetic datasets.&lt;/p&gt;
&lt;p&gt;Today, we show how to replace the LSTM autoencoder by a – convolutional – VAE. In light of the experimentation results, already hinted at above, it is completely plausible that the “variational” part is not even so important here – that a convolutional autoencoder with just MSE loss would have performed just as well on those data. In fact, to find out, it’s enough to remove the call to &lt;code&gt;reparameterize()&lt;/code&gt; and multiply the KL component of the loss by 0. (We leave this to the interested reader, to keep the post at reasonable length.)&lt;/p&gt;
&lt;p&gt;One last piece of context, in case you haven’t read the two previous posts and would like to jump in here directly. We’re doing time series forecasting; so why this talk of autoencoders? Shouldn’t we just be comparing an LSTM (or some other type of RNN, for that matter) to a convnet? In fact, the necessity of a latent representation is due to the very idea of FNN: The latent code is supposed to reflect the true attractor of a dynamical system. That is, if the attractor of the underlying system is roughly two-dimensional, we hope to find that just two of the latent variables have considerable variance. (This reasoning is explained in a lot of detail in the previous posts.)&lt;/p&gt;
&lt;h2 id="fnn-vae"&gt;FNN-VAE&lt;/h2&gt;
&lt;p&gt;So, let’s start with the code for our new model.&lt;/p&gt;
&lt;p&gt;The encoder takes the time series, of format &lt;code&gt;batch_size x num_timesteps x num_features&lt;/code&gt; just like in the LSTM case, and produces a flat, 10-dimensional output: the latent code, which FNN loss is computed on.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)

vae_encoder_model &amp;lt;- function(n_timesteps,
                               n_features,
                               n_latent,
                               name = NULL) {
  keras_model_custom(name = name, function(self) {
    self$conv1 &amp;lt;- layer_conv_1d(kernel_size = 3,
                                filters = 16,
                                strides = 2)
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$conv2 &amp;lt;- layer_conv_1d(kernel_size = 7,
                                filters = 32,
                                strides = 2)
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    self$conv3 &amp;lt;- layer_conv_1d(kernel_size = 9,
                                filters = 64,
                                strides = 2)
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm3 &amp;lt;- layer_batch_normalization()
    self$conv4 &amp;lt;- layer_conv_1d(
      kernel_size = 9,
      filters = n_latent,
      strides = 2,
      activation = &amp;quot;linear&amp;quot; 
    )
    self$batchnorm4 &amp;lt;- layer_batch_normalization()
    self$flat &amp;lt;- layer_flatten()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$batchnorm2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$batchnorm3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$batchnorm4() %&amp;gt;%
        self$flat()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The decoder starts from this – flat – representation and decompresses it into a time sequence. In both encoder and decoder (de-)conv layers, parameters are chosen to handle a sequence length (&lt;code&gt;num_timesteps&lt;/code&gt;) of 120, which is what we’ll use for prediction below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vae_decoder_model &amp;lt;- function(n_timesteps,
                               n_features,
                               n_latent,
                               name = NULL) {
  keras_model_custom(name = name, function(self) {
    self$reshape &amp;lt;- layer_reshape(target_shape = c(1, n_latent))
    self$conv1 &amp;lt;- layer_conv_1d_transpose(kernel_size = 15,
                                          filters = 64,
                                          strides = 3)
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$conv2 &amp;lt;- layer_conv_1d_transpose(kernel_size = 11,
                                          filters = 32,
                                          strides = 3)
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    self$conv3 &amp;lt;- layer_conv_1d_transpose(
      kernel_size = 9,
      filters = 16,
      strides = 2,
      output_padding = 1
    )
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm3 &amp;lt;- layer_batch_normalization()
    self$conv4 &amp;lt;- layer_conv_1d_transpose(
      kernel_size = 7,
      filters = 1,
      strides = 1,
      activation = &amp;quot;linear&amp;quot;
    )
    self$batchnorm4 &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$reshape() %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$batchnorm2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$batchnorm3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$batchnorm4()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that even though we called these constructors &lt;code&gt;vae_encoder_model()&lt;/code&gt; and &lt;code&gt;vae_decoder_model()&lt;/code&gt;, there is nothing variational to these models per se; they are really just an encoder and a decoder, respectively. Metamorphosis into a VAE will happen in the training procedure; in fact, the only two things that will make this a VAE are going to be the reparameterization of the latent layer and the added-in KL loss.&lt;/p&gt;
&lt;p&gt;Speaking of training, these are the routines we’ll call. The function to compute FNN loss, &lt;code&gt;loss_false_nn()&lt;/code&gt;, can be found in both of the abovementioned predecessor posts; we kindly ask the reader to copy it from one of these places.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# to reparameterize encoder output before calling decoder
reparameterize &amp;lt;- function(mean, logvar = 0) {
  eps &amp;lt;- k_random_normal(shape = n_latent)
  eps * k_exp(logvar * 0.5) + mean
}

# loss has 3 components: NLL, KL, and FNN
# otherwise, this is just normal TF2-style training 
train_step_vae &amp;lt;- function(batch) {
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    code &amp;lt;- encoder(batch[[1]])
    z &amp;lt;- reparameterize(code)
    prediction &amp;lt;- decoder(z)
    
    l_mse &amp;lt;- mse_loss(batch[[2]], prediction)
    # see loss_false_nn in 2 previous posts
    l_fnn &amp;lt;- loss_false_nn(code)
    # KL divergence to a standard normal
    l_kl &amp;lt;- -0.5 * k_mean(1 - k_square(z))
    # overall loss is a weighted sum of all 3 components
    loss &amp;lt;- l_mse + fnn_weight * l_fnn + kl_weight * l_kl
  })
  
  encoder_gradients &amp;lt;-
    tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;-
    tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(purrr::transpose(list(
    encoder_gradients, encoder$trainable_variables
  )))
  optimizer$apply_gradients(purrr::transpose(list(
    decoder_gradients, decoder$trainable_variables
  )))
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
  train_kl(l_kl)
}

# wrap it all in autograph
training_loop_vae &amp;lt;- tf_function(autograph(function(ds_train) {
  
  for (batch in ds_train) {
    train_step_vae(batch) 
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  tf$print(&amp;quot;KL loss: &amp;quot;, train_kl$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  train_kl$reset_states()
  
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To finish up the model section, here is the actual training code. This is nearly identical to what we did for FNN-LSTM before.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_latent &amp;lt;- 10L
n_features &amp;lt;- 1

encoder &amp;lt;- vae_encoder_model(n_timesteps,
                         n_features,
                         n_latent)

decoder &amp;lt;- vae_decoder_model(n_timesteps,
                         n_features,
                         n_latent)
mse_loss &amp;lt;-
  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)

train_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_mse&amp;#39;)
train_kl &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_kl&amp;#39;)

fnn_multiplier &amp;lt;- 1 # default value used in nearly all cases (see text)
fnn_weight &amp;lt;- fnn_multiplier * nrow(x_train)/batch_size

kl_weight &amp;lt;- 1

optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:100) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop_vae(ds_train)
 
  test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
  encoded &amp;lt;- encoder(test_batch[[1]][1:1000])
  test_var &amp;lt;- tf$math$reduce_variance(encoded, axis = 0L)
  print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5))
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="experimental-setup-and-data"&gt;Experimental setup and data&lt;/h2&gt;
&lt;p&gt;The idea was to add white noise to a deterministic series. This time, the &lt;a href="https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor"&gt;Roessler system&lt;/a&gt; was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler.png" alt="Roessler attractor, two-dimensional projections." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-5)Roessler attractor, two-dimensional projections.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like we did for the Lorenz system in the first part of this series, we use &lt;code&gt;deSolve&lt;/code&gt; to generate data from the Roessler equations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(deSolve)

parameters &amp;lt;- c(a = .2,
                b = .2,
                c = 5.7)

initial_state &amp;lt;-
  c(x = 1,
    y = 1,
    z = 1.05)

roessler &amp;lt;- function(t, state, parameters) {
  with(as.list(c(state, parameters)), {
    dx &amp;lt;- -y - z
    dy &amp;lt;- x + a * y
    dz = b + z * (x - c)
    
    list(c(dx, dy, dz))
  })
}

times &amp;lt;- seq(0, 2500, length.out = 20000)

roessler_ts &amp;lt;-
  ode(
    y = initial_state,
    times = times,
    func = roessler,
    parms = parameters,
    method = &amp;quot;lsoda&amp;quot;
  ) %&amp;gt;% unclass() %&amp;gt;% as_tibble()

n &amp;lt;- 10000
roessler &amp;lt;- roessler_ts$x[1:n]

roessler &amp;lt;- scale(roessler)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# add noise
noise &amp;lt;- 1 # also used 1.5, 2, 2.5
roessler &amp;lt;- roessler + rnorm(10000, mean = 0, sd = noise)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler_noise.png" alt="Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Otherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_timesteps &amp;lt;- 120
batch_size &amp;lt;- 32

gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
                     function(i) {
                       start &amp;lt;- i
                       end &amp;lt;- i + n_timesteps - 1
                       out &amp;lt;- x[start:end]
                       out
                     })
  ) %&amp;gt;%
    na.omit()
}

train &amp;lt;- gen_timesteps(roessler[1:(n/2)], 2 * n_timesteps)
test &amp;lt;- gen_timesteps(roessler[(n/2):n], 2 * n_timesteps) 

dim(train) &amp;lt;- c(dim(train), 1)
dim(test) &amp;lt;- c(dim(test), 1)

x_train &amp;lt;- train[ , 1:n_timesteps, , drop = FALSE]
y_train &amp;lt;- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

ds_train &amp;lt;- tensor_slices_dataset(list(x_train, y_train)) %&amp;gt;%
  dataset_shuffle(nrow(x_train)) %&amp;gt;%
  dataset_batch(batch_size)

x_test &amp;lt;- test[ , 1:n_timesteps, , drop = FALSE]
y_test &amp;lt;- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

ds_test &amp;lt;- tensor_slices_dataset(list(x_test, y_test)) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;The LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an &lt;code&gt;fnn_multiplier&lt;/code&gt; of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.&lt;/p&gt;
&lt;p&gt;As a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In all cases&lt;/em&gt; here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.&lt;/p&gt;
&lt;h4 id="low-noise"&gt;Low noise&lt;/h4&gt;
&lt;p&gt;Seeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (&lt;code&gt;x&lt;/code&gt;, 120 steps) and output (&lt;code&gt;y&lt;/code&gt;, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?&lt;/p&gt;
&lt;p&gt;Looking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we start to add noise?&lt;/p&gt;
&lt;h4 id="substantial-noise"&gt;Substantial noise&lt;/h4&gt;
&lt;p&gt;Between noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.&lt;/p&gt;
&lt;p&gt;Here first are predictions obtained from the unregularized models.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-12)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were &lt;em&gt;trained&lt;/em&gt; on the noisy version; predict fluctuations is what they learned.&lt;/p&gt;
&lt;p&gt;Do we see the same with the FNN models?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.&lt;/p&gt;
&lt;p&gt;“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.&lt;/p&gt;
&lt;p&gt;However, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.&lt;/p&gt;
&lt;p&gt;In the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, &lt;em&gt;MSEs have been normalized as fractions of the maximum MSE in a category&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we want to predict &lt;em&gt;signal plus noise&lt;/em&gt; (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/mses.png" alt="Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right)." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="summing-up"&gt;Summing up&lt;/h2&gt;
&lt;p&gt;Our experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.&lt;/p&gt;
&lt;p&gt;With that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">14d4183bce425fc7eaa5ddba400775d1</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>


&lt;p&gt;Today, we pick up on the plan alluded to in the conclusion of the recent &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;Deep attractors: Where deep learning meets chaos&lt;/a&gt;: employ that same technique to generate &lt;em&gt;forecasts&lt;/em&gt; for empirical time series data.&lt;/p&gt;
&lt;p&gt;“That same technique”, which for conciseness, I’ll take the liberty of referring to as FNN-LSTM, is due to William Gilpin’s 2020 paper “Deep reconstruction of strange attractors from time series” &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a nutshell&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the problem addressed is as follows: A system, known or assumed to be nonlinear and highly dependent on initial conditions, is observed, resulting in a scalar series of measurements. The measurements are not just – inevitably – noisy, but in addition, they are – at best – a projection of a multidimensional state space onto a line.&lt;/p&gt;
&lt;p&gt;Classically in nonlinear time series analysis, such scalar series of observations are augmented by supplementing, at every point in time, delayed measurements of that same series – a technique called &lt;em&gt;delay coordinate embedding&lt;/em&gt; &lt;span class="citation"&gt;(Sauer, Yorke, and Casdagli 1991)&lt;/span&gt;. For example, instead of just a single vector &lt;code&gt;X1&lt;/code&gt;, we could have a matrix of vectors &lt;code&gt;X1&lt;/code&gt;, &lt;code&gt;X2&lt;/code&gt;, and &lt;code&gt;X3&lt;/code&gt;, with &lt;code&gt;X2&lt;/code&gt; containing the same values as &lt;code&gt;X1&lt;/code&gt;, but starting from the third observation, and &lt;code&gt;X3&lt;/code&gt;, from the fifth. In this case, the &lt;em&gt;delay&lt;/em&gt; would be 2, and the &lt;em&gt;embedding dimension&lt;/em&gt;, 3. Various &lt;a href="https://en.wikipedia.org/wiki/Takens&amp;#39;s_theorem"&gt;theorems&lt;/a&gt; state that if these parameters are chosen adequately, it is possible to reconstruct the complete state space. There is a problem though: The theorems assume that the dimensionality of the true state space is known, which in many real-world applications, won’t be the case.&lt;/p&gt;
&lt;p&gt;This is where Gilpin’s idea comes in: Train an autoencoder, whose intermediate representation encapsulates the system’s attractor. Not just any MSE-optimized autoencoder though. The latent representation is regularized by &lt;em&gt;false nearest neighbors&lt;/em&gt; (FNN) loss, a technique commonly used with delay coordinate embedding to determine an adequate embedding dimension. False neighbors are those who are close in &lt;code&gt;n&lt;/code&gt;-dimensional space, but significantly farther apart in &lt;code&gt;n+1&lt;/code&gt;-dimensional space. In the aforementioned introductory &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;post&lt;/a&gt;, we showed how this technique allowed to reconstruct the attractor of the (synthetic) Lorenz system. Now, we want to move on to prediction.&lt;/p&gt;
&lt;p&gt;We first describe the setup, including model definitions, training procedures, and data preparation. Then, we tell you how it went.&lt;/p&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;h3 id="from-reconstruction-to-forecasting-and-branching-out-into-the-real-world"&gt;From reconstruction to forecasting, and branching out into the real world&lt;/h3&gt;
&lt;p&gt;In the previous post, we trained an LSTM autoencoder to generate a compressed code, representing the attractor of the system. As usual with autoencoders, the target when training is the same as the input, meaning that overall loss consisted of two components: The FNN loss, computed on the latent representation only, and the mean-squared-error loss between input and output. Now for prediction, the target consists of future values, as many as we wish to predict. Put differently: The architecture stays the same, but instead of reconstruction we perform prediction, in the standard RNN way. Where the usual RNN setup would just directly chain the desired number of LSTMs, we have an LSTM encoder that outputs a (timestep-less) latent code, and an LSTM decoder that starting from that code, repeated as many times as required, forecasts the required number of future values.&lt;/p&gt;
&lt;p&gt;This of course means that to evaluate forecast performance, we need to compare against an LSTM-only setup. This is exactly what we’ll do, and comparison will turn out to be interesting not just quantitatively, but &lt;em&gt;qualitatively&lt;/em&gt; as well.&lt;/p&gt;
&lt;p&gt;We perform these comparisons on the four datasets Gilpin chose to demonstrate &lt;a href="https://github.com/williamgilpin/fnn/blob/master/exploratory.ipynb"&gt;attractor reconstruction on observational data&lt;/a&gt;. While all of these, as is evident from the images in that notebook, exhibit nice attractors, we’ll see that not all of them are equally suited to forecasting using simple RNN-based architectures – with or without FNN regularization. But even those that clearly demand a different approach allow for interesting observations as to the impact of FNN loss.&lt;/p&gt;
&lt;h3 id="model-definitions-and-training-setup"&gt;Model definitions and training setup&lt;/h3&gt;
&lt;p&gt;In all four experiments, we use the same model definitions and training procedures, the only differing parameter being the number of timesteps used in the LSTMs (for reasons that will become evident when we introduce the individual datasets).&lt;/p&gt;
&lt;p&gt;Both architectures were chosen to be straightforward, and about comparable in number of parameters – both basically consist of two LSTMs with 32 units (&lt;code&gt;n_recurrent&lt;/code&gt; will be set to 32 for all experiments).&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="fnn-lstm"&gt;FNN-LSTM&lt;/h4&gt;
&lt;p&gt;FNN-LSTM looks nearly like in the previous post, apart from the fact that we split up the encoder LSTM into two, to uncouple capacity (&lt;code&gt;n_recurrent&lt;/code&gt;) from maximal latent state dimensionality (&lt;code&gt;n_latent&lt;/code&gt;, kept at 10 just like before).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# DL-related packages
library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)

# going to need those later
library(tidyverse)
library(cowplot)

encoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_recurrent,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm1 &amp;lt;-  layer_lstm(
      units = n_recurrent,
      input_shape = c(n_timesteps, n_features),
      return_sequences = TRUE
    ) 
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$lstm2 &amp;lt;-  layer_lstm(
      units = n_latent,
      return_sequences = FALSE
    ) 
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$lstm2() %&amp;gt;%
        self$batchnorm2() 
    }
  })
}

decoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_recurrent,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$repeat_vector &amp;lt;- layer_repeat_vector(n = n_timesteps)
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;- layer_lstm(
      units = n_recurrent,
      return_sequences = TRUE,
      go_backwards = TRUE
    ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    self$elu &amp;lt;- layer_activation_elu() 
    self$time_distributed &amp;lt;- time_distributed(layer = layer_dense(units = n_features))
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$repeat_vector() %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() %&amp;gt;%
        self$elu() %&amp;gt;%
        self$time_distributed()
    }
  })
}

n_latent &amp;lt;- 10L
n_features &amp;lt;- 1
n_hidden &amp;lt;- 32

encoder &amp;lt;- encoder_model(n_timesteps,
                         n_features,
                         n_hidden,
                         n_latent)

decoder &amp;lt;- decoder_model(n_timesteps,
                         n_features,
                         n_hidden,
                         n_latent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regularizer, FNN loss, is unchanged:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss_false_nn &amp;lt;- function(x) {
  
  # changing these parameters is equivalent to
  # changing the strength of the regularizer, so we keep these fixed (these values
  # correspond to the original values used in Kennel et al 1992).
  rtol &amp;lt;- 10 
  atol &amp;lt;- 2
  k_frac &amp;lt;- 0.01
  
  k &amp;lt;- max(1, floor(k_frac * batch_size))
  
  ## Vectorized version of distance matrix calculation
  tri_mask &amp;lt;-
    tf$linalg$band_part(
      tf$ones(
        shape = c(tf$cast(n_latent, tf$int32), tf$cast(n_latent, tf$int32)),
        dtype = tf$float32
      ),
      num_lower = -1L,
      num_upper = 0L
    )
  
  # latent x batch_size x latent
  batch_masked &amp;lt;-
    tf$multiply(tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()])
  
  # latent x batch_size x 1
  x_squared &amp;lt;-
    tf$reduce_sum(batch_masked * batch_masked,
                  axis = 2L,
                  keepdims = TRUE)
  
  # latent x batch_size x batch_size
  pdist_vector &amp;lt;- x_squared + tf$transpose(x_squared, perm = c(0L, 2L, 1L)) -
    2 * tf$matmul(batch_masked, tf$transpose(batch_masked, perm = c(0L, 2L, 1L)))
  
  #(latent, batch_size, batch_size)
  all_dists &amp;lt;- pdist_vector
  # latent
  all_ra &amp;lt;-
    tf$sqrt((1 / (
      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)
    )) *
      tf$reduce_sum(tf$square(
        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)
      ), axis = c(1L, 2L)))
  
  # Avoid singularity in the case of zeros
  #(latent, batch_size, batch_size)
  all_dists &amp;lt;-
    tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))
  
  #inds = tf.argsort(all_dists, axis=-1)
  top_k &amp;lt;- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))
  # (#(latent, batch_size, batch_size)
  top_indices &amp;lt;- top_k[[1]]
  
  #(latent, batch_size, batch_size)
  neighbor_dists_d &amp;lt;-
    tf$gather(all_dists, top_indices, batch_dims = -1L)
  #(latent - 1, batch_size, batch_size)
  neighbor_new_dists &amp;lt;-
    tf$gather(all_dists[2:-1, , ],
              top_indices[1:-2, , ],
              batch_dims = -1L)
  
  # Eq. 4 of Kennel et al.
  #(latent - 1, batch_size, batch_size)
  scaled_dist &amp;lt;- tf$sqrt((
    tf$square(neighbor_new_dists) -
      # (9, 8, 2)
      tf$square(neighbor_dists_d[1:-2, , ])) /
      # (9, 8, 2)
      tf$square(neighbor_dists_d[1:-2, , ])
  )
  
  # Kennel condition #1
  #(latent - 1, batch_size, batch_size)
  is_false_change &amp;lt;- (scaled_dist &amp;gt; rtol)
  # Kennel condition 2
  #(latent - 1, batch_size, batch_size)
  is_large_jump &amp;lt;-
    (neighbor_new_dists &amp;gt; atol * all_ra[1:-2, tf$newaxis, tf$newaxis])
  
  is_false_neighbor &amp;lt;-
    tf$math$logical_or(is_false_change, is_large_jump)
  #(latent - 1, batch_size, 1)
  total_false_neighbors &amp;lt;-
    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]
  
  # Pad zero to match dimensionality of latent space
  # (latent - 1)
  reg_weights &amp;lt;-
    1 - tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))
  # (latent,)
  reg_weights &amp;lt;- tf$pad(reg_weights, list(list(1L, 0L)))
  
  # Find batch average activity
  
  # L2 Activity regularization
  activations_batch_averaged &amp;lt;-
    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))
  
  loss &amp;lt;- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))
  loss
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training is unchanged as well, apart from the fact that now, we continually output latent variable variances in addition to the losses. This is because with FNN-LSTM, we have to choose an adequate weight for the FNN loss component. An “adequate weight” is one where the variance drops sharply after the first &lt;code&gt;n&lt;/code&gt; variables, with &lt;code&gt;n&lt;/code&gt; thought to correspond to attractor dimensionality. For the Lorenz system discussed in the previous post, this is how these variances looked:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     V1       V2        V3        V4        V5        V6        V7        V8        V9       V10
 0.0739   0.0582   1.12e-6   3.13e-4   1.43e-5   1.52e-8   1.35e-6   1.86e-4   1.67e-4   4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take variance as an indicator of &lt;em&gt;importance&lt;/em&gt;, the first two variables are clearly more important than the rest. This finding nicely corresponds to “official” estimates of Lorenz attractor dimensionality. For example, the correlation dimension is estimated to lie around 2.05 &lt;span class="citation"&gt;(Grassberger and Procaccia 1983)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, here we have the training routine:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_step &amp;lt;- function(batch) {
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    code &amp;lt;- encoder(batch[[1]])
    prediction &amp;lt;- decoder(code)
    
    l_mse &amp;lt;- mse_loss(batch[[2]], prediction)
    l_fnn &amp;lt;- loss_false_nn(code)
    loss &amp;lt;- l_mse + fnn_weight * l_fnn
  })
  
  encoder_gradients &amp;lt;-
    tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;-
    tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(purrr::transpose(list(
    encoder_gradients, encoder$trainable_variables
  )))
  optimizer$apply_gradients(purrr::transpose(list(
    decoder_gradients, decoder$trainable_variables
  )))
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
  
  
}

training_loop &amp;lt;- tf_function(autograph(function(ds_train) {
  for (batch in ds_train) {
    train_step(batch)
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  
}))


mse_loss &amp;lt;-
  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)

train_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_mse&amp;#39;)

# fnn_multiplier should be chosen individually per dataset
# this is the value we used on the geyser dataset
fnn_multiplier &amp;lt;- 0.7
fnn_weight &amp;lt;- fnn_multiplier * nrow(x_train)/batch_size

# learning rate may also need adjustment
optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:200) {
 cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
 training_loop(ds_train)
 
 test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
 encoded &amp;lt;- encoder(test_batch[[1]]) 
 test_var &amp;lt;- tf$math$reduce_variance(encoded, axis = 0L)
 print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to what we’ll use as a baseline for comparison.&lt;/p&gt;
&lt;h4 id="vanilla-lstm"&gt;Vanilla LSTM&lt;/h4&gt;
&lt;p&gt;Here is the vanilla LSTM, stacking two layers, each, again, of size 32. Dropout and recurrent dropout were chosen individually per dataset, as was the learning rate.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lstm &amp;lt;- function(n_latent, n_timesteps, n_features, n_recurrent, dropout, recurrent_dropout,
                 optimizer = optimizer_adam(lr =  1e-3)) {
  
  model &amp;lt;- keras_model_sequential() %&amp;gt;%
    layer_lstm(
      units = n_recurrent,
      input_shape = c(n_timesteps, n_features),
      dropout = dropout, 
      recurrent_dropout = recurrent_dropout,
      return_sequences = TRUE
    ) %&amp;gt;% 
    layer_lstm(
      units = n_recurrent,
      dropout = dropout,
      recurrent_dropout = recurrent_dropout,
      return_sequences = TRUE
    ) %&amp;gt;% 
    time_distributed(layer_dense(units = 1))
  
  model %&amp;gt;%
    compile(
      loss = &amp;quot;mse&amp;quot;,
      optimizer = optimizer
    )
  model
  
}

model &amp;lt;- lstm(n_latent, n_timesteps, n_features, n_hidden, dropout = 0.2, recurrent_dropout = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="data-preparation"&gt;Data preparation&lt;/h3&gt;
&lt;p&gt;For all experiments, data were prepared in the same way.&lt;/p&gt;
&lt;p&gt;In every case, we used the first 10000 measurements available in the respective &lt;code&gt;.pkl&lt;/code&gt; files &lt;a href="https://github.com/williamgilpin/fnn/tree/master/datasets"&gt;provided by Gilpin in his GitHub repository&lt;/a&gt;. To save on file size and not depend on an external data source, we extracted those first 10000 entries to &lt;code&gt;.csv&lt;/code&gt; files downloadable directly from this blog’s repo:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;geyser &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/geyser.csv&amp;quot;,
  &amp;quot;data/geyser.csv&amp;quot;)

electricity &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/electricity.csv&amp;quot;,
  &amp;quot;data/electricity.csv&amp;quot;)

ecg &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/ecg.csv&amp;quot;,
  &amp;quot;data/ecg.csv&amp;quot;)

mouse &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/mouse.csv&amp;quot;,
  &amp;quot;data/mouse.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Should you want to access the complete time series (of considerably greater lengths), just download them from Gilpin’s repo and load them using &lt;code&gt;reticulate&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# e.g.
geyser &amp;lt;- reticulate::py_load_object(&amp;quot;geyser_train_test.pkl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the data preparation code for the first dataset, &lt;code&gt;geyser&lt;/code&gt; - all other datasets were treated the same way.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the first 10000 measurements from the compilation provided by Gilpin
geyser &amp;lt;- read_csv(&amp;quot;geyser.csv&amp;quot;, col_names = FALSE) %&amp;gt;% select(X1) %&amp;gt;% pull() %&amp;gt;% unclass()

# standardize
geyser &amp;lt;- scale(geyser)

# varies per dataset; see below 
n_timesteps &amp;lt;- 60
batch_size &amp;lt;- 32

# transform into [batch_size, timesteps, features] format required by RNNs
gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
                     function(i) {
                       start &amp;lt;- i
                       end &amp;lt;- i + n_timesteps - 1
                       out &amp;lt;- x[start:end]
                       out
                     })
  ) %&amp;gt;%
    na.omit()
}

n &amp;lt;- 10000
train &amp;lt;- gen_timesteps(geyser[1:(n/2)], 2 * n_timesteps)
test &amp;lt;- gen_timesteps(geyser[(n/2):n], 2 * n_timesteps) 

dim(train) &amp;lt;- c(dim(train), 1)
dim(test) &amp;lt;- c(dim(test), 1)

# split into input and target  
x_train &amp;lt;- train[ , 1:n_timesteps, , drop = FALSE]
y_train &amp;lt;- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

x_test &amp;lt;- test[ , 1:n_timesteps, , drop = FALSE]
y_test &amp;lt;- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

# create tfdatasets
ds_train &amp;lt;- tensor_slices_dataset(list(x_train, y_train)) %&amp;gt;%
  dataset_shuffle(nrow(x_train)) %&amp;gt;%
  dataset_batch(batch_size)

ds_test &amp;lt;- tensor_slices_dataset(list(x_test, y_test)) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to look at how forecasting goes on our four datasets.&lt;/p&gt;
&lt;h2 id="experiments"&gt;Experiments&lt;/h2&gt;
&lt;h3 id="geyser-dataset"&gt;Geyser dataset&lt;/h3&gt;
&lt;p&gt;People working with time series may have heard of &lt;a href="https://en.wikipedia.org/wiki/Old_Faithful"&gt;Old Faithful&lt;/a&gt;, a geyser in Wyoming, US that has continually been erupting every 44 minutes to two hours since the year 2004. For the subset of data Gilpin extracted&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;geyser_train_test.pkl&lt;/code&gt; corresponds to detrended temperature readings from the main runoff pool of the Old Faithful geyser in Yellowstone National Park, downloaded from the &lt;a href="https://geysertimes.org/"&gt;GeyserTimes database&lt;/a&gt;. Temperature measurements start on April 13, 2015 and occur in one-minute increments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Like we said above, &lt;code&gt;geyser.csv&lt;/code&gt; is a subset of these measurements, comprising the first 10000 data points. To choose an adequate timestep for the LSTMs, we inspect the series at various resolutions:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_ts.png" alt="Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It seems like the behavior is periodic with a period of about 40-50; a timestep of 60 thus seemed like a good try.&lt;/p&gt;
&lt;p&gt;Having trained both FNN-LSTM and the vanilla LSTM for 200 epochs, we first inspect the variances of the latent variables on the test set. The value of &lt;code&gt;fnn_multiplier&lt;/code&gt; corresponding to this run was &lt;code&gt;0.7&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
encoded &amp;lt;- encoder(test_batch[[1]]) %&amp;gt;%
  as.array() %&amp;gt;%
  as_tibble()

encoded %&amp;gt;% summarise_all(var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   V1     V2        V3          V4       V5       V6       V7       V8       V9      V10
0.258 0.0262 0.0000627 0.000000600 0.000533 0.000362 0.000238 0.000121 0.000518 0.000365&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a drop in importance between the first two variables and the rest; however, unlike in the Lorenz system, &lt;code&gt;V1&lt;/code&gt; and &lt;code&gt;V2&lt;/code&gt; variances also differ by an order of magnitude.&lt;/p&gt;
&lt;p&gt;Now, it’s interesting to compare prediction errors for both models. We are going to make an observation that will carry through to all three datasets to come.&lt;/p&gt;
&lt;p&gt;Keeping up the suspense for a while, here is the code used to compute per-timestep prediction errors from both models. The same code will be used for all other datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;calc_mse &amp;lt;- function(df, y_true, y_pred) {
  (sum((df[[y_true]] - df[[y_pred]])^2))/nrow(df)
}

get_mse &amp;lt;- function(test_batch, prediction) {
  
  comp_df &amp;lt;- 
    data.frame(
      test_batch[[2]][, , 1] %&amp;gt;%
        as.array()) %&amp;gt;%
        rename_with(function(name) paste0(name, &amp;quot;_true&amp;quot;)) %&amp;gt;%
    bind_cols(
      data.frame(
        prediction[, , 1] %&amp;gt;%
          as.array()) %&amp;gt;%
          rename_with(function(name) paste0(name, &amp;quot;_pred&amp;quot;)))
  
  mse &amp;lt;- purrr::map(1:dim(prediction)[2],
                        function(varno)
                          calc_mse(comp_df,
                                   paste0(&amp;quot;X&amp;quot;, varno, &amp;quot;_true&amp;quot;),
                                   paste0(&amp;quot;X&amp;quot;, varno, &amp;quot;_pred&amp;quot;))) %&amp;gt;%
    unlist()
  
  mse
}

prediction_fnn &amp;lt;- decoder(encoder(test_batch[[1]]))
mse_fnn &amp;lt;- get_mse(test_batch, prediction_fnn)

prediction_lstm &amp;lt;- model %&amp;gt;% predict(ds_test)
mse_lstm &amp;lt;- get_mse(test_batch, prediction_lstm)

mses &amp;lt;- data.frame(timestep = 1:n_timesteps, fnn = mse_fnn, lstm = mse_lstm) %&amp;gt;%
  gather(key = &amp;quot;type&amp;quot;, value = &amp;quot;mse&amp;quot;, -timestep)

ggplot(mses, aes(timestep, mse, color = type)) +
  geom_point() +
  scale_color_manual(values = c(&amp;quot;#00008B&amp;quot;, &amp;quot;#3CB371&amp;quot;)) +
  theme_classic() +
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here is the actual comparison. One thing especially jumps to the eye: FNN-LSTM forecast error is significantly lower for initial timesteps, first and foremost, for the very first prediction, which from this graph we expect to be pretty good!&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see “jumps” in prediction error, for FNN-LSTM, between the very first forecast and the second, and then between the second and the ensuing ones, reminding of the similar jumps in variable importance for the latent code! After the first ten timesteps, vanilla LSTM has caught up with FNN-LSTM, and we won’t interpret further development of the losses based on just a single run’s output.&lt;/p&gt;
&lt;p&gt;Instead, let’s inspect actual predictions. We randomly pick sequences from the test set, and ask both FNN-LSTM and vanilla LSTM for a forecast. The same procedure will be followed for the other datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;given &amp;lt;- data.frame(as.array(tf$concat(list(
  test_batch[[1]][, , 1], test_batch[[2]][, , 1]
),
axis = 1L)) %&amp;gt;% t()) %&amp;gt;%
  add_column(type = &amp;quot;given&amp;quot;) %&amp;gt;%
  add_column(num = 1:(2 * n_timesteps))

fnn &amp;lt;- data.frame(as.array(prediction_fnn[, , 1]) %&amp;gt;%
                    t()) %&amp;gt;%
  add_column(type = &amp;quot;fnn&amp;quot;) %&amp;gt;%
  add_column(num = (n_timesteps  + 1):(2 * n_timesteps))

lstm &amp;lt;- data.frame(as.array(prediction_lstm[, , 1]) %&amp;gt;%
                     t()) %&amp;gt;%
  add_column(type = &amp;quot;lstm&amp;quot;) %&amp;gt;%
  add_column(num = (n_timesteps + 1):(2 * n_timesteps))

compare_preds_df &amp;lt;- bind_rows(given, lstm, fnn)

plots &amp;lt;- 
  purrr::map(sample(1:dim(compare_preds_df)[2], 16),
             function(v) {
               ggplot(compare_preds_df, aes(num, .data[[paste0(&amp;quot;X&amp;quot;, v)]], color = type)) +
                 geom_line() +
                 theme_classic() +
                 theme(legend.position = &amp;quot;none&amp;quot;, axis.title = element_blank()) +
                 scale_color_manual(values = c(&amp;quot;#00008B&amp;quot;, &amp;quot;#DB7093&amp;quot;, &amp;quot;#3CB371&amp;quot;))
             })

plot_grid(plotlist = plots, ncol = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are sixteen random picks of predictions on the test set. The ground truth is displayed in pink; blue forecasts are from FNN-LSTM, green ones from vanilla LSTM.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_preds.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we expect from the error inspection comes true: FNN-LSTM yields significantly better predictions for immediate continuations of a given sequence.&lt;/p&gt;
&lt;p&gt;Let’s move on to the second dataset on our list.&lt;/p&gt;
&lt;h3 id="electricity-dataset"&gt;Electricity dataset&lt;/h3&gt;
&lt;p&gt;This is a dataset on power consumption, aggregated over 321 different households and fifteen-minute-intervals.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;electricity_train_test.pkl&lt;/code&gt; corresponds to average power consumption by 321 Portuguese households between 2012 and 2014, in units of kilowatts consumed in fifteen minute increments. This dataset is from the &lt;a href="http://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"&gt;UCI machine learning database&lt;/a&gt;.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, we see a very regular pattern:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_ts.png" alt="Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With such regular behavior, we immediately tried to predict a higher number of timesteps (&lt;code&gt;120&lt;/code&gt;) – and didn’t have to retract behind that aspiration.&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;0.5&lt;/code&gt;, latent variable variances look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;V1          V2            V3       V4       V5            V6       V7         V8      V9     V10
0.390 0.000637 0.00000000288 1.48e-10 2.10e-11 0.00000000119 6.61e-11 0.00000115 1.11e-4 1.40e-4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We definitely see a sharp drop already after the first variable.&lt;/p&gt;
&lt;p&gt;How do prediction errors compare on the two architectures?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-15)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here, FNN-LSTM performs better over a long range of timesteps, but again, the difference is most visible for immediate predictions. Will an inspection of actual predictions confirm this view?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-16)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It does! In fact, forecasts from FNN-LSTM are very impressive on all time scales.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen the easy and predictable, let’s approach the weird and difficult.&lt;/p&gt;
&lt;h3 id="ecg-dataset"&gt;ECG dataset&lt;/h3&gt;
&lt;p&gt;Says Gilpin,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;ecg_train.pkl&lt;/code&gt; and &lt;code&gt;ecg_test.pkl&lt;/code&gt; correspond to ECG measurements for two different patients, taken from the &lt;a href="https://physionet.org/content/qtdb/1.0.0/"&gt;PhysioNet QT database&lt;/a&gt;.&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How do these look?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_ts.png" alt="ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-17)ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To the layperson that I am, these do not look nearly as regular as expected. First experiments showed that both architectures are not capable of dealing with a high number of timesteps. In every try, FNN-LSTM performed better for the very first timestep.&lt;/p&gt;
&lt;p&gt;This is also the case for &lt;code&gt;n_timesteps = 12&lt;/code&gt;, the final try (after &lt;code&gt;120&lt;/code&gt;, &lt;code&gt;60&lt;/code&gt; and &lt;code&gt;30&lt;/code&gt;). With an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;1&lt;/code&gt;, the latent variances obtained amounted to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     V1        V2          V3        V4         V5       V6       V7         V8         V9       V10
  0.110  1.16e-11     3.78e-9 0.0000992    9.63e-9  4.65e-5  1.21e-4    9.91e-9    3.81e-9   2.71e-8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There &lt;em&gt;is&lt;/em&gt; a gap between the first variable and all other ones; but not much variance is explained by &lt;code&gt;V1&lt;/code&gt; either.&lt;/p&gt;
&lt;p&gt;Apart from the very first prediction, vanilla LSTM shows lower forecast errors this time; however, we have to add that this was not consistently observed when experimenting with other timestep settings.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-18)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Looking at actual predictions, both architectures perform best when a persistence forecast is adequate – in fact, they produce one even when it is &lt;em&gt;not&lt;/em&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On this dataset, we certainly would want to explore other architectures better able to capture the presence of high &lt;em&gt;and&lt;/em&gt; low frequencies in the data, such as mixture models. But – were we forced to stay with one of these, and could do a one-step-ahead, rolling forecast, we’d go with FNN-LSTM.&lt;/p&gt;
&lt;p&gt;Speaking of mixed frequencies – we haven’t seen the extremes yet …&lt;/p&gt;
&lt;h3 id="mouse-dataset"&gt;Mouse dataset&lt;/h3&gt;
&lt;p&gt;“Mouse”, that’s spike rates recorded from a mouse thalamus.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;mouse.pkl&lt;/code&gt; A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from &lt;a href="http://crcns.org/data-sets/thalamus/th-1/about-th-1"&gt;CRCNS&lt;/a&gt; and processed with the authors' code in order to generate a spike rate time series.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_ts.png" alt="Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-20)Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Obviously, this dataset will be very hard to predict. How, after “long” silence, do you know that a neuron is going to fire?&lt;/p&gt;
&lt;p&gt;As usual, we inspect latent code variances (&lt;code&gt;fnn_multiplier&lt;/code&gt; was set to &lt;code&gt;0.4&lt;/code&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;     V1       V2        V3         V4       V5       V6        V7      V8       V9        V10
 0.0796  0.00246  0.000214    2.26e-7   .71e-9  4.22e-8  6.45e-10 1.61e-4 2.63e-10    2.05e-8
&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we don’t see the first variable explaining much variance. Still, interestingly, when inspecting forecast errors we get a picture very similar to the one obtained on our first, &lt;code&gt;geyser&lt;/code&gt;, dataset:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-22)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So here, the latent code definitely seems to help! With every timestep “more” that we try to predict, prediction performance goes down &lt;em&gt;continuously&lt;/em&gt; – or put the other way round, short-time predictions are expected to be pretty good!&lt;/p&gt;
&lt;p&gt;Let’s see:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-23)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In fact on this dataset, the difference in behavior between both architectures is striking. When nothing is “supposed to happen”, vanilla LSTM produces “flat” curves at about the mean of the data, while FNN-LSTM takes the effort to “stay on track” as long as possible before also converging to the mean. Choosing FNN-LSTM – had we to choose one of these two – would be an obvious decision with this dataset.&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;When, in timeseries forecasting, would we consider FNN-LSTM? Judging by the above experiments, conducted on four very different datasets: Whenever we consider a deep learning approach. Of course, this has been a casual exploration – and it was meant to be, as – hopefully – was evident from the nonchalant and bloomy (sometimes) writing style.&lt;/p&gt;
&lt;p&gt;Throughout the text, we’ve emphasized &lt;em&gt;utility&lt;/em&gt; – how could this technique be used to improve predictions? But, looking at the above results, a number of interesting questions come to mind. We already speculated (though in an indirect way) whether the number of high-variance variables in the latent code was relatable to how far we could sensibly forecast into the future. However, even more intriguing is the question of how characteristics of the &lt;em&gt;dataset itself&lt;/em&gt; affect FNN efficiency.&lt;/p&gt;
&lt;p&gt;Such characteristics could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How nonlinear is the dataset? (Put differently, how incompatible, as indicated by some form of test algorithm, is it with the hypothesis that the data generation mechanism was a linear one?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To what degree does the system appear to be sensitively dependent on initial conditions? In other words, what is the value of its (estimated, from the observations) highest &lt;a href="https://en.wikipedia.org/wiki/Lyapunov_exponent"&gt;Lyapunov exponent&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is its (estimated) dimensionality, for example, in terms of &lt;a href="https://en.wikipedia.org/wiki/Correlation_dimension"&gt;correlation dimension&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While it is easy to obtain those estimates, using, for instance, the &lt;a href="https://cran.r-project.org/web/packages/nonlinearTseries/"&gt;nonlinearTseries&lt;/a&gt; package explicitly modeled after practices described in Kantz &amp;amp; Schreiber’s classic &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt;, we don’t want to extrapolate from our tiny sample of datasets, and leave such explorations and analyses to further posts, and/or the interested reader’s ventures :-). In any case, we hope you enjoyed the demonstration of practical usability of an approach that in the preceding post, was mainly introduced in terms of its conceptual attractivity.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-GRASSBERGER1983189"&gt;
&lt;p&gt;Grassberger, Peter, and Itamar Procaccia. 1983. “Measuring the Strangeness of Strange Attractors.” &lt;em&gt;Physica D: Nonlinear Phenomena&lt;/em&gt; 9 (1): 189–208. &lt;a href="https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1"&gt;https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kantz"&gt;
&lt;p&gt;Kantz, Holger, and Thomas Schreiber. 2004. &lt;em&gt;Nonlinear Time Series Analysis&lt;/em&gt;. Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-embedology"&gt;
&lt;p&gt;Sauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” &lt;em&gt;Journal of Statistical Physics&lt;/em&gt; 65 (3-4): 579–616. &lt;a href="https://doi.org/10.1007/BF01053745"&gt;https://doi.org/10.1007/BF01053745&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Please refer to the aforementioned predecessor post for a detailed introduction.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;“Basically” because FNN-LSTM technically has three LSTMs – the third one, with &lt;code&gt;n_latent = 10&lt;/code&gt; units, being used to store the latent code.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;see dataset descriptions in the &lt;a href="https://github.com/williamgilpin/fnn"&gt;repository's README&lt;/a&gt;&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">6611909d56171d2558567c7b24918a6f</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>


&lt;p&gt;For us deep learning practitioners, the world is – not flat, but – linear, mostly. Or piecewise linear.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Like other linear approximations, or maybe even more so, deep learning can be incredibly successful at making predictions. But let’s admit it – sometimes we just miss the thrill of the nonlinear, of good, old, deterministic-yet-unpredictable chaos. Can we have both? It looks like we can. In this post, we’ll see an application of deep learning (DL) to nonlinear time series prediction – or rather, the essential step that predates it: reconstructing the attractor underlying its dynamics. While this post is an introduction, presenting the topic from scratch, further posts will build on this and extrapolate to observational datasets.&lt;/p&gt;
&lt;h3 id="what-to-expect-from-this-post"&gt;What to expect from this post&lt;/h3&gt;
&lt;p&gt;In his 2020 paper &lt;em&gt;Deep reconstruction of strange attractors from time series&lt;/em&gt; &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;, William Gilpin uses an autoencoder architecture, combined with a regularizer implementing the &lt;em&gt;false nearest neighbors&lt;/em&gt; statistic &lt;span class="citation"&gt;(Kennel, Brown, and Abarbanel 1992)&lt;/span&gt;, to reconstruct attractors from univariate observations of multivariate, nonlinear dynamical systems. If you feel you completely understand the sentence you just read, you may as well directly jump to the paper – come back for the code though&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If, on the other hand, you’re more familiar with the chaos on your desk (extrapolating … apologies) than &lt;em&gt;chaos theory chaos&lt;/em&gt;, read on. Here, we’ll first go into what it’s all about&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, and then, show an example application, featuring Edward Lorenz’s famous butterfly attractor. While this initial post is primarily supposed to be a fun introduction to a fascinating topic, we hope to follow up with applications to real-world datasets in the future.&lt;/p&gt;
&lt;h2 id="rabbits-butterflies-and-low-dimensional-projections-our-problem-statement-in-context"&gt;Rabbits, butterflies, and low-dimensional projections: Our problem statement in context&lt;/h2&gt;
&lt;p&gt;In curious misalignment with how we use “chaos” in day-to-day language, chaos, the technical concept, is very different from stochasticity, or randomness. Chaos may emerge from purely deterministic processes - very simplistic ones, even. Let’s see how; with rabbits.&lt;/p&gt;
&lt;h3 id="rabbits-or-sensitive-dependence-on-initial-conditions"&gt;Rabbits, or: Sensitive dependence on initial conditions&lt;/h3&gt;
&lt;p&gt;You may be familiar with the &lt;em&gt;logistic&lt;/em&gt; equation, used as a toy model for population growth. Often it’s written like this – with &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; being the size of the population, expressed as a fraction of the maximal size (a fraction of possible rabbits, thus), and &lt;span class="math inline"&gt;\(r\)&lt;/span&gt; being the growth rate (the rate at which rabbits reproduce):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
x_{n + 1} = r \ x_n \ (1 - x_n)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation describes an &lt;em&gt;iterated map&lt;/em&gt; over discrete timesteps &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;. Its repeated application results in a &lt;em&gt;trajectory&lt;/em&gt; describing how the population of rabbits evolves. Maps can have &lt;em&gt;fixed points&lt;/em&gt;, states where further function application goes on producing the same result forever. Example-wise, say the growth rate amounts to &lt;span class="math inline"&gt;\(2.1\)&lt;/span&gt;, and we start at two (pretty different!) initial values, &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.8\)&lt;/span&gt;. Both trajectories arrive at a fixed point – the same fixed point – in fewer than 10 iterations. Were we asked to predict the population size after a hundred iterations, we could make a very confident guess, whatever the of starting value. (If the initial value is &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, we stay at &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, but we can be pretty certain of that as well.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/single_fixedpoint.png" alt="Trajectory of the logistic map for r = 2.1 and two different initial values." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Trajectory of the logistic map for r = 2.1 and two different initial values.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What if the growth rate were somewhat higher, at &lt;span class="math inline"&gt;\(3.3\)&lt;/span&gt;, say? Again, we immediately compare trajectories resulting from initial values &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.9\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/2cycle.png" alt="Trajectory of the logistic map for r = 3.3 and two different initial values." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-2)Trajectory of the logistic map for r = 3.3 and two different initial values.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This time, don’t see a single fixed point, but a &lt;em&gt;two-cycle&lt;/em&gt;: As the trajectories stabilize, population size inevitably is at one of two possible values – either too many rabbits or too few, you could say. The two trajectories are phase-shifted, but again, the attracting values – the &lt;em&gt;attractor&lt;/em&gt; – is shared by both initial conditions. So still, predictability is pretty high. But we haven’t seen everything yet.&lt;/p&gt;
&lt;p&gt;Let’s again enhance the growth rate some. Now &lt;em&gt;this&lt;/em&gt; (literally) is chaos:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Even after a hundred iterations, there is no set of values the trajectories recur to. We can’t be confident about any prediction we might make.&lt;/p&gt;
&lt;p&gt;Or can we? After all, we have the governing equation, which is deterministic. So we should be able to calculate the size of the population at, say, time &lt;span class="math inline"&gt;\(150\)&lt;/span&gt;? In principle, yes; but this presupposes we have an accurate measurement for the starting state.&lt;/p&gt;
&lt;p&gt;How accurate? Let’s compare trajectories for initial values &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.301\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos2.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At first, trajectories seem to jump around in unison; but during the second dozen iterations already, they dissociate more and more, and increasingly, all bets are off. What if initial values are &lt;em&gt;really&lt;/em&gt; close, as in, &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; vs. &lt;span class="math inline"&gt;\(0.30000001\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;It just takes a bit longer for the disassociation to surface.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos3.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-5)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we’re seeing here is &lt;em&gt;sensitive dependence on initial conditions&lt;/em&gt;, an essential precondition for a system to be chaotic. In an nutshell: Chaos arises when a &lt;em&gt;deterministic&lt;/em&gt; system shows &lt;em&gt;sensitive dependence on initial conditions&lt;/em&gt;. Or as Edward Lorenz &lt;a href="https://en.wikipedia.org/wiki/Chaos_theory"&gt;is said to have put it&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the present determines the future, but the approximate present does not approximately determine the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now if these unstructured, random-looking point clouds constitute chaos, what with the all-but-amorphous butterfly (to be displayed very soon)?&lt;/p&gt;
&lt;h3 id="butterflies-or-attractors-and-strange-attractors"&gt;Butterflies, or: Attractors and strange attractors&lt;/h3&gt;
&lt;p&gt;Actually, in the context of chaos theory, the term butterfly may be encountered in different contexts.&lt;/p&gt;
&lt;p&gt;Firstly, as so-called “butterfly effect”, it is an instantiation of the templatic phrase “the flap of a butterfly’s wing in _________ profoundly affects the course of the weather in _________.”&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; In this usage, it is mostly a metaphor for sensitive dependence on initial conditions.&lt;/p&gt;
&lt;p&gt;Secondly, the existence of this metaphor led to a Rorschach-test-like identification with two-dimensional visualizations of attractors of the Lorenz system. The Lorenz system is a set of three first-order differential equations designed to describe &lt;a href="https://en.wikipedia.org/wiki/Atmospheric_convection"&gt;atmospheric convection&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{aligned}
&amp;amp; \frac{dx}{dt} = \sigma (y - x)\\
&amp;amp; \frac{dy}{dt} = \rho x - x z - y\\
&amp;amp; \frac{dz}{dt} = x y - \beta z
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This set of equations is nonlinear, as required for chaotic behavior to appear. It also has the required dimensionality, which for smooth, continuous systems, is at least 3&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. Whether we actually see chaotic attractors – among which, the butterfly – depends on the settings of the parameters &lt;span class="math inline"&gt;\(\sigma\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;. For the values conventionally chosen, &lt;span class="math inline"&gt;\(\sigma=10\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\rho=28\)&lt;/span&gt;, and &lt;span class="math inline"&gt;\(\beta=8/3\)&lt;/span&gt; , we see it when projecting the trajectory on the &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; axes:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/lorenz_attractors.png" alt="Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The butterfly is an &lt;em&gt;attractor&lt;/em&gt; (as are the other two projections), but it is neither a point nor a cycle. It is an attractor in the sense that starting from a variety of different initial values, we end up in some sub-region of the state space, and we don’t get to escape no more. This is easier to see when watching evolution over time, as in this animation:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/x_z.gif" alt="How the Lorenz attractor traces out the famous &amp;quot;butterfly&amp;quot; shape."  /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)How the Lorenz attractor traces out the famous “butterfly” shape.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, to plot the attractor in two dimensions, we threw away the third. But in “real life”, we don’t usually have too &lt;em&gt;much&lt;/em&gt; information (although it may sometimes seem like we had). We might have a lot of measurements, but these don’t usually reflect the actual state variables we’re interested in. In these cases, we may want to actually &lt;em&gt;add&lt;/em&gt; information.&lt;/p&gt;
&lt;h3 id="embeddings-as-a-non-dl-term-or-undoing-the-projection"&gt;Embeddings (as a non-DL term), or: Undoing the projection&lt;/h3&gt;
&lt;p&gt;Assume that instead of all three variables of the Lorenz system, we had measured just one: &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, the rate of convection. Often in nonlinear dynamics, the technique of delay coordinate embedding &lt;span class="citation"&gt;(Sauer, Yorke, and Casdagli 1991)&lt;/span&gt; is used to enhance a series of univariate measurements.&lt;/p&gt;
&lt;p&gt;In this method – or family of methods – the univariate series is augmented by time-shifted copies of itself. There are two decisions to be made: How many copies to add, and how big the delay should be. To illustrate, if we had a scalar series,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 2 3 4 5 6 7 8 9 10 11 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;a three-dimensional embedding with time delay 2 would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 3 5
2 4 6
3 5 7
4 6 8
5 7 9
6 8 10
7 9 11
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of the two decisions to be made – number of shifted series and time lag – the first is a decision on the dimensionality of the reconstruction space. Various theorems, such as &lt;a href="https://en.wikipedia.org/wiki/Takens%27s_theorem"&gt;Taken's theorem&lt;/a&gt;, indicate bounds on the number of dimensions required, provided the dimensionality of the true state space is known – which, in real-world applications, often is not the case.The second has been of little interest to mathematicians, but is important in practice. In fact, Kantz and Schreiber &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt; argue that in practice, it is the product of both parameters that matters, as it indicates the time span represented by an embedding vector.&lt;/p&gt;
&lt;p&gt;How are these parameters chosen? Regarding reconstruction dimensionality, the reasoning goes that even in chaotic systems, points that are close in state space at time &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; should still be close at time &lt;span class="math inline"&gt;\(t + \Delta t\)&lt;/span&gt;, provided &lt;span class="math inline"&gt;\(\Delta t\)&lt;/span&gt; is very small. So say we have two points that are close, by some metric, when represented in two-dimensional space. But in three dimensions, that is, if we don’t “project away” the third dimension, they are a lot more distant. As illustrated in &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/fnn.png" alt="In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020)." width="380" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If this happens, then projecting down has eliminated some essential information. In 2d, the points were &lt;em&gt;false neighbors&lt;/em&gt;. The &lt;em&gt;false nearest neighbors&lt;/em&gt; (FNN) statistic can be used to determine an adequate embedding size, like this:&lt;/p&gt;
&lt;p&gt;For each point, take its closest neighbor in &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; dimensions, and compute the ratio of their distances in &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(m+1\)&lt;/span&gt; dimensions. If the ratio is larger than some threshold &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;, the neighbor was false. Sum the number of false neighbors over all points. Do this for different &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;, and inspect the resulting curves.&lt;/p&gt;
&lt;p&gt;At this point, let’s look ahead at the autoencoder approach. The autoencoder will use that same FNN statistic as a regularizer, in addition to the usual autoencoder reconstruction loss. This will result in a new heuristic regarding embedding dimensionality that involves fewer decisions.&lt;/p&gt;
&lt;p&gt;Going back to the classic method for an instant, the second parameter, the time lag, is even more difficult to sort out &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt;. Usually, mutual information is plotted for different delays and then, the first delay where it falls below some threshold is chosen. We don’t further elaborate on this question as it is rendered obsolete in the neural network approach. Which we’ll see now.&lt;/p&gt;
&lt;h2 id="learning-the-lorenz-attractor"&gt;Learning the Lorenz attractor&lt;/h2&gt;
&lt;p&gt;Our code closely follows the architecture, parameter settings, and data setup used in the &lt;a href="https://github.com/williamgilpin/fnn"&gt;reference implementation&lt;/a&gt; William provided. The loss function, especially, has been ported one-to-one.&lt;/p&gt;
&lt;p&gt;The general idea is the following. An autoencoder – for example, an LSTM autoencoder as presented here – is used to compress the univariate time series into a latent representation of some dimensionality, which will constitute an upper bound on the dimensionality of the learned attractor. In addition to mean squared error between input and reconstructions, there will be a second loss term, applying the FNN regularizer. This results in the latent units being roughly ordered by &lt;em&gt;importance&lt;/em&gt;, as measured by their variance. It is expected that somewhere in the listing of variances, a sharp drop will appear. The units before the drop are then assumed to encode the &lt;em&gt;attractor&lt;/em&gt; of the system in question.&lt;/p&gt;
&lt;p&gt;In this setup, there is still a choice to be made: how to weight the FNN loss. One would run training for different weights &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; and look for the drop. Surely, this could in principle be automated, but given the newness of the method – the paper was published this year – it makes sense to focus on thorough analysis first.&lt;/p&gt;
&lt;h3 id="data-generation"&gt;Data generation&lt;/h3&gt;
&lt;p&gt;We use the &lt;code&gt;deSolve&lt;/code&gt; package to generate data from the Lorenz equations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(deSolve)
library(tidyverse)

parameters &amp;lt;- c(sigma = 10,
                rho = 28,
                beta = 8/3)

initial_state &amp;lt;-
  c(x = -8.60632853,
    y = -14.85273055,
    z = 15.53352487)

lorenz &amp;lt;- function(t, state, parameters) {
  with(as.list(c(state, parameters)), {
    dx &amp;lt;- sigma * (y - x)
    dy &amp;lt;- x * (rho - z) - y
    dz &amp;lt;- x * y - beta * z
    
    list(c(dx, dy, dz))
  })
}

times &amp;lt;- seq(0, 500, length.out = 125000)

lorenz_ts &amp;lt;-
  ode(
    y = initial_state,
    times = times,
    func = lorenz,
    parms = parameters,
    method = &amp;quot;lsoda&amp;quot;
  ) %&amp;gt;% as_tibble()

lorenz_ts[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 4
      time      x     y     z
     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1 0        -8.61 -14.9  15.5
 2 0.00400  -8.86 -15.2  15.9
 3 0.00800  -9.12 -15.6  16.3
 4 0.0120   -9.38 -16.0  16.7
 5 0.0160   -9.64 -16.3  17.1
 6 0.0200   -9.91 -16.7  17.6
 7 0.0240  -10.2  -17.0  18.1
 8 0.0280  -10.5  -17.3  18.6
 9 0.0320  -10.7  -17.7  19.1
10 0.0360  -11.0  -18.0  19.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve already seen the attractor, or rather, its three two-dimensional projections, in figure 6 above. But now our scenario is different. We only have access to &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, a univariate time series. As the time interval used to numerically integrate the differential equations was rather tiny, we just use every tenth observation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;obs &amp;lt;- lorenz_ts %&amp;gt;%
  select(time, x) %&amp;gt;%
  filter(row_number() %% 10 == 0)

ggplot(obs, aes(time, x)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 100)) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/obs.png" alt="Convection rates as a univariate time series." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Convection rates as a univariate time series.
&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="preprocessing"&gt;Preprocessing&lt;/h3&gt;
&lt;p&gt;The first half of the series is used for training. The data is scaled and transformed into the three-dimensional form expected by recurrent layers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)

# scale observations
obs &amp;lt;- obs %&amp;gt;% mutate(
  x = scale(x)
)

# generate timesteps
n &amp;lt;- nrow(obs)
n_timesteps &amp;lt;- 10

gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
             function(i) {
               start &amp;lt;- i
               end &amp;lt;- i + n_timesteps - 1
               out &amp;lt;- x[start:end]
               out
             })
  ) %&amp;gt;%
    na.omit()
}

# train with start of time series, test with end of time series 
x_train &amp;lt;- gen_timesteps(as.matrix(obs$x)[1:(n/2)], n_timesteps)
x_test &amp;lt;- gen_timesteps(as.matrix(obs$x)[(n/2):n], n_timesteps) 

# add required dimension for features (we have one)
dim(x_train) &amp;lt;- c(dim(x_train), 1)
dim(x_test) &amp;lt;- c(dim(x_test), 1)

# some batch size (value not crucial)
batch_size &amp;lt;- 100

# transform to datasets so we can use custom training
ds_train &amp;lt;- tensor_slices_dataset(x_train) %&amp;gt;%
  dataset_batch(batch_size)

ds_test &amp;lt;- tensor_slices_dataset(x_test) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="autoencoder"&gt;Autoencoder&lt;/h3&gt;
&lt;p&gt;With newer versions of TensorFlow (&amp;gt;= 2.0, certainly if &amp;gt;= 2.2), autoencoder-like models are best coded as custom models, and trained in an “autographed” loop.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The encoder is centered around a single LSTM layer, whose size determines the maximum dimensionality of the attractor. The decoder then undoes the compression – again, mainly using a single LSTM.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# size of the latent code
n_latent &amp;lt;- 10L
n_features &amp;lt;- 1

encoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;-  layer_lstm(
      units = n_latent,
      input_shape = c(n_timesteps, n_features),
      return_sequences = FALSE
    ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() 
    }
  })
}

decoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$repeat_vector &amp;lt;- layer_repeat_vector(n = n_timesteps)
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;- layer_lstm(
        units = n_latent,
        return_sequences = TRUE,
        go_backwards = TRUE
      ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    self$elu &amp;lt;- layer_activation_elu() 
    self$time_distributed &amp;lt;- time_distributed(layer = layer_dense(units = n_features))
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$repeat_vector() %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() %&amp;gt;%
        self$elu() %&amp;gt;%
        self$time_distributed()
    }
  })
}


encoder &amp;lt;- encoder_model(n_timesteps, n_features, n_latent)
decoder &amp;lt;- decoder_model(n_timesteps, n_features, n_latent)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="loss"&gt;Loss&lt;/h3&gt;
&lt;p&gt;As already explained above, the loss function we train with is twofold. On the one hand, we compare the original inputs with the decoder outputs (the reconstruction), using mean squared error:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_loss &amp;lt;- tf$keras$losses$MeanSquaredError(
  reduction = tf$keras$losses$Reduction$SUM)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, we try to keep the number of false neighbors small, by means of the following regularizer.&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss_false_nn &amp;lt;- function(x) {
 
  # original values used in Kennel et al. (1992)
  rtol &amp;lt;- 10 
  atol &amp;lt;- 2
  k_frac &amp;lt;- 0.01
  
  k &amp;lt;- max(1, floor(k_frac * batch_size))
  
  tri_mask &amp;lt;-
    tf$linalg$band_part(
      tf$ones(
        shape = c(n_latent, n_latent),
        dtype = tf$float32
      ),
      num_lower = -1L,
      num_upper = 0L
    )
  
   batch_masked &amp;lt;- tf$multiply(
     tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()]
   )
  
  x_squared &amp;lt;- tf$reduce_sum(
    batch_masked * batch_masked,
    axis = 2L,
    keepdims = TRUE
  )

  pdist_vector &amp;lt;- x_squared +
  tf$transpose(
    x_squared, perm = c(0L, 2L, 1L)
  ) -
  2 * tf$matmul(
    batch_masked,
    tf$transpose(batch_masked, perm = c(0L, 2L, 1L))
  )

  all_dists &amp;lt;- pdist_vector
  all_ra &amp;lt;-
    tf$sqrt((1 / (
      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)
    )) *
      tf$reduce_sum(tf$square(
        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)
      ), axis = c(1L, 2L)))
  
  all_dists &amp;lt;- tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))

  top_k &amp;lt;- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))
  top_indices &amp;lt;- top_k[[1]]

  neighbor_dists_d &amp;lt;- tf$gather(all_dists, top_indices, batch_dims = -1L)
  
  neighbor_new_dists &amp;lt;- tf$gather(
    all_dists[2:-1, , ],
    top_indices[1:-2, , ],
    batch_dims = -1L
  )
  
  # Eq. 4 of Kennel et al. (1992)
  scaled_dist &amp;lt;- tf$sqrt((
    tf$square(neighbor_new_dists) -
      tf$square(neighbor_dists_d[1:-2, , ])) /
      tf$square(neighbor_dists_d[1:-2, , ])
  )
  
  # Kennel condition #1
  is_false_change &amp;lt;- (scaled_dist &amp;gt; rtol)
  # Kennel condition #2
  is_large_jump &amp;lt;-
    (neighbor_new_dists &amp;gt; atol * all_ra[1:-2, tf$newaxis, tf$newaxis])
  
  is_false_neighbor &amp;lt;-
    tf$math$logical_or(is_false_change, is_large_jump)
  
  total_false_neighbors &amp;lt;-
    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]
  
  reg_weights &amp;lt;- 1 -
    tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))
  reg_weights &amp;lt;- tf$pad(reg_weights, list(list(1L, 0L)))
  
  activations_batch_averaged &amp;lt;-
    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))
  
  loss &amp;lt;- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))
  loss
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MSE and FNN are added , with FNN loss weighted according to the essential hyperparameter of this model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fnn_weight &amp;lt;- 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This value was experimentally chosen as the one best conforming to our &lt;em&gt;look-for-the-highest-drop&lt;/em&gt; heuristic.&lt;/p&gt;
&lt;h3 id="model-training"&gt;Model training&lt;/h3&gt;
&lt;p&gt;The training loop closely follows the aforementioned &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;recipe&lt;/a&gt; on how to train with custom models and &lt;code&gt;tfautograph&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name=&amp;#39;train_mse&amp;#39;)

train_step &amp;lt;- function(batch) {
  
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    
    code &amp;lt;- encoder(batch)
    reconstructed &amp;lt;- decoder(code)
    
    l_mse &amp;lt;- mse_loss(batch, reconstructed)
    l_fnn &amp;lt;- loss_false_nn(code)
    loss &amp;lt;- l_mse + fnn_weight * l_fnn
    
  })
  
  encoder_gradients &amp;lt;- tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;- tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(
    purrr::transpose(list(encoder_gradients, encoder$trainable_variables))
  )
  optimizer$apply_gradients(
    purrr::transpose(list(decoder_gradients, decoder$trainable_variables))
  )
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
}

training_loop &amp;lt;- tf_function(autograph(function(ds_train) {
  
  for (batch in ds_train) {
    train_step(batch)
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  
}))

optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:200) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop(ds_train)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.&lt;/p&gt;
&lt;h3 id="obtaining-the-attractor-from-the-test-set"&gt;Obtaining the attractor from the test set&lt;/h3&gt;
&lt;p&gt;We use the test set to inspect the latent code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
predicted &amp;lt;- encoder(test_batch) %&amp;gt;%
  as.array(predicted) %&amp;gt;%
  as_tibble()

predicted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,242 x 10
      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 
 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 
 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 
 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 
 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127
 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 
 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 
 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 
 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 
10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 
# … with 6,232 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_weight&lt;/code&gt; of 10, we do see a drop after the first two units:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;predicted %&amp;gt;% summarise_all(var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 10
      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance&lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. Here, this results in three projections of the set &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/predicted_attractors.png" alt="Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-20)Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="wrapping-up-for-this-time"&gt;Wrapping up (for this time)&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom &lt;em&gt;false nearest neighbors&lt;/em&gt; loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.&lt;/p&gt;
&lt;p&gt;This is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again &lt;em&gt;of course&lt;/em&gt;, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic&lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;? There is a lot to explore, stay tuned – and as always, thanks for reading!&lt;/p&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kantz"&gt;
&lt;p&gt;Kantz, Holger, and Thomas Schreiber. 2004. &lt;em&gt;Nonlinear Time Series Analysis&lt;/em&gt;. Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-PhysRevA.45.3403"&gt;
&lt;p&gt;Kennel, Matthew B., Reggie Brown, and Henry D. I. Abarbanel. 1992. “Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction.” &lt;em&gt;Phys. Rev. A&lt;/em&gt; 45 (6): 3403–11. &lt;a href="https://doi.org/10.1103/PhysRevA.45.3403"&gt;https://doi.org/10.1103/PhysRevA.45.3403&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-embedology"&gt;
&lt;p&gt;Sauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” &lt;em&gt;Journal of Statistical Physics&lt;/em&gt; 65 (3-4): 579–616. &lt;a href="https://doi.org/10.1007/BF01053745"&gt;https://doi.org/10.1007/BF01053745&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Gilbert"&gt;
&lt;p&gt;Strang, Gilbert. 2019. &lt;em&gt;Linear Algebra and Learning from Data&lt;/em&gt;. Wellesley Cambridge Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Strogatz"&gt;
&lt;p&gt;Strogatz, Steven. 2015. &lt;em&gt;Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering&lt;/em&gt;. Westview Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;For many popular activation functions at least (such as ReLU). See e.g. &lt;span class="citation"&gt;(Strang 2019)&lt;/span&gt;.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;The paper is also accompanied by a &lt;a href="https://github.com/williamgilpin/fnn"&gt;Python implementation&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;To people who want to learn more about this topic, the usual recommendation is &lt;span class="citation"&gt;(Strogatz 2015)&lt;/span&gt;. Personally I prefer another source, which I can’t recommend highly enough: Santa Fe Institute’s &lt;a href="https://www.complexityexplorer.org/courses/100-nonlinear-dynamics-mathematical-and-computational-approaches"&gt;Nonlinear Dynamics: Mathematical and Computational Approaches&lt;/a&gt;, taught by Liz Bradley.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See e.g. &lt;a href="https://en.wikipedia.org/wiki/Butterfly_effect"&gt;Wikipedia&lt;/a&gt; for some history and links to sources.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;In discrete systems, like the logistic map, a single dimension is enough.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;See the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;custom training tutorial&lt;/a&gt; for a blueprint.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;See the appendix of &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt; for a pseudocode-like documentation.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;As per author recommendation (personal communication).&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;See &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt; for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9ffa8ff9d7baa88fa6d32d9bdc6c28d4</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>


&lt;p&gt;We’ve seen quite a few examples of unsupervised learning (or self-supervised learning, to choose the more correct but less popular term) on this blog.&lt;/p&gt;
&lt;p&gt;Often, these involved &lt;em&gt;Variational Autoencoders (VAEs)&lt;/em&gt;, whose appeal lies in them allowing to model a &lt;em&gt;latent space&lt;/em&gt; of underlying, independent (preferably) factors that determine the visible features. A possible downside can be the inferior quality of generated samples. Generative Adversarial Networks (GANs) are another popular approach. Conceptually, these are highly attractive due to their game-theoretic framing. However, they can be difficult to train. &lt;em&gt;PixelCNN&lt;/em&gt; variants, on the other hand – we’ll subsume them all here under PixelCNN – are generally known for their good results. They seem to involve some more alchemy&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; though. Under those circumstances, what could be more welcome than an easy way of experimenting with them? Through TensorFlow Probability (TFP) and its R wrapper, &lt;a href="https://github.io/tfprobability"&gt;tfprobability&lt;/a&gt;, we now have such a way.&lt;/p&gt;
&lt;p&gt;This post first gives an introduction to PixelCNN, concentrating on high-level concepts (leaving the details for the curious to look them up in the respective papers). We’ll then show an example of using &lt;code&gt;tfprobability&lt;/code&gt; to experiment with the TFP implementation.&lt;/p&gt;
&lt;h2 id="pixelcnn-principles"&gt;PixelCNN principles&lt;/h2&gt;
&lt;h3 id="autoregressivity-or-we-need-some-order"&gt;Autoregressivity, or: We need (some) order&lt;/h3&gt;
&lt;p&gt;The basic idea in PixelCNN is autoregressivity. Each pixel is modeled as depending on all prior pixels. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(\mathbf{x}) = \prod_{i}p(x_i|x_0, x_1, ..., x_{i-1})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now wait a second - what even &lt;em&gt;are&lt;/em&gt; prior pixels? Last I saw one images were two-dimensional. So this means we have to impose an &lt;em&gt;order&lt;/em&gt; on the pixels. Commonly this will be &lt;em&gt;raster scan&lt;/em&gt; order: row after row, from left to right. But when dealing with color images, there’s something else: At each position, we actually have &lt;em&gt;three&lt;/em&gt; intensity values, one for each of red, green, and blue. The original PixelCNN paper&lt;span class="citation"&gt;(Oord, Kalchbrenner, and Kavukcuoglu 2016)&lt;/span&gt; carried through autoregressivity here as well, with a pixel’s intensity for red depending on just prior pixels, those for green depending on these same prior pixels but additionally, the current value for red, and those for blue depending on the prior pixels as well as the current values for red and green.&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(x_i|\mathbf{x}&amp;lt;i) = p(x_{i,R}|\mathbf{x}&amp;lt;i)\ p(x_{i,G}|\mathbf{x}&amp;lt;i, x_{i,R})\ p(x_{i,B}|\mathbf{x}&amp;lt;i, x_{i,R}, x_{i,G})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the variant implemented in TFP, PixelCNN++&lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt; , introduces a simplification; it factorizes the joint distribution in a less compute-intensive way.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Technically, then, we know how autoregressivity is realized; intuitively, it may still seem surprising that imposing a raster scan order “just works” (to me, at least, it is). Maybe this is one of those points where compute power successfully compensates for lack of an equivalent of a cognitive prior.&lt;/p&gt;
&lt;h3 id="masking-or-where-not-to-look"&gt;Masking, or: Where not to look&lt;/h3&gt;
&lt;p&gt;Now, PixelCNN ends in “CNN” for a reason – as usual in image processing, convolutional layers (or blocks thereof) are involved. But – is it not the very nature of a convolution that it computes an average of some sorts, looking, for each output pixel, not just at the corresponding input but also, at its spatial (or temporal) surroundings? How does that rhyme with the look-at-just-prior-pixels strategy?&lt;/p&gt;
&lt;p&gt;Surprisingly, this problem is easier to solve than it sounds. When applying the convolutional kernel, just multiply with a mask that zeroes out any “forbidden pixels” – like in this example for a 5x5 kernel, where we’re about to compute the convolved value for row 3, column 3:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\left[\begin{array}
{rrr}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
\end{array}\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This makes the algorithm honest, but introduces a different problem: With each successive convolutional layer consuming its predecessor’s output, there is a continuously growing &lt;em&gt;blind spot&lt;/em&gt; (so-called in analogy to the blind spot on the retina, but located in the top right) of pixels that are never &lt;em&gt;seen&lt;/em&gt; by the algorithm. Van den Oord et al. (2016)&lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt; fix this by using two different convolutional stacks, one proceeding from top to bottom, the other from left to right&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/stacks.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 1: Left: Blind spot, growing over layers. Right: Using two different stacks (a vertical and a horizontal one) solves the problem. Source: van den Oord et al., 2016.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="conditioning-or-show-me-a-kitten"&gt;Conditioning, or: Show me a kitten&lt;/h3&gt;
&lt;p&gt;So far, we’ve always talked about “generating images” in a purely generic way. But the real attraction lies in creating samples of some specified type – one of the classes we’ve been training on, or orthogonal information fed into the network. This is where PixelCNN becomes &lt;em&gt;Conditional PixelCNN&lt;/em&gt;&lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;, and it is also where that feeling of magic resurfaces. Again, as “general math” it’s not hard to conceive. Here, &lt;span class="math inline"&gt;\(\mathbf{h}\)&lt;/span&gt; is the additional input we’re conditioning on:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(\mathbf{x}| \mathbf{h}) = \prod_{i}p(x_i|x_0, x_1, ..., x_{i-1}, \mathbf{h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But how does this translate into neural network operations? It’s just another matrix multiplication (&lt;span class="math inline"&gt;\(V^T \mathbf{h}\)&lt;/span&gt;) added to the convolutional outputs (&lt;span class="math inline"&gt;\(W \mathbf{x}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\mathbf{y} = tanh(W_{k,f} \mathbf{x} + V^T_{k,f} \mathbf{h}) \odot \sigma(W_{k,g} \mathbf{x} + V^T_{k,g} \mathbf{h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(If you’re wondering about the second part on the right, after the Hadamard product sign – we won’t go into details, but in a nutshell, it’s another modification introduced by &lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;, a transfer of the “gating” principle from recurrent neural networks, such as GRUs and LSTMs, to the convolutional setting.)&lt;/p&gt;
&lt;p&gt;So we see what goes into the decision of a pixel value to sample. But how is that decision actually &lt;em&gt;made&lt;/em&gt;?&lt;/p&gt;
&lt;h3 id="logistic-mixture-likelihood-or-no-pixel-is-an-island"&gt;Logistic mixture likelihood , or: No pixel is an island&lt;/h3&gt;
&lt;p&gt;Again, this is where the TFP implementation does not follow the original paper, but the latter PixelCNN++ one. Originally, pixels were modeled as discrete values, decided on by a softmax over 256 (0-255) possible values. (That this actually worked seems like another instance of deep learning magic. Imagine: In this model, 254 is as far from 255 as it is from 0.)&lt;/p&gt;
&lt;p&gt;In contrast, PixelCNN++ assumes an underlying continuous distribution of color intensity, and rounds to the nearest integer. That underlying distribution is a mixture of logistic distributions, thus allowing for multimodality:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\nu \sim \sum_{i} \pi_i \ logistic(\mu_i, \sigma_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="overall-architecture-and-the-pixelcnn-distribution"&gt;Overall architecture and the PixelCNN distribution&lt;/h3&gt;
&lt;p&gt;Overall, PixelCNN++, as described in &lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt;, consists of six blocks. The blocks together make up a UNet-like structure, successively downsizing the input and then, upsampling again:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/high-level.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 2: Overall structure of PixelCNN++. From: Salimans et al., 2017.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In TFP’s PixelCNN distribution, the number of blocks is configurable as &lt;code&gt;num_hierarchies&lt;/code&gt;, the default being 3.&lt;/p&gt;
&lt;p&gt;Each block consists of a customizable number of layers, called &lt;em&gt;ResNet layers&lt;/em&gt; due to the residual connection (visible on the right) complementing the convolutional operations in the horizontal stack:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/layer.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 3: One so-called "ResNet layer", featuring both a vertical and a horizontal convolutional stack. Source: van den Oord et al., 2017.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In TFP, the number of these layers per block is configurable as &lt;code&gt;num_resnet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;num_resnet&lt;/code&gt; and &lt;code&gt;num_hierarchies&lt;/code&gt; are the parameters you’re most likely to experiment with, but there are a few more you can check out in the &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_pixel_cnn.html"&gt;documentation&lt;/a&gt;. The number of logistic distributions in the mixture is also configurable, but from my experiments it’s best to keep that number rather low to avoid producing &lt;code&gt;NaN&lt;/code&gt;s during training.&lt;/p&gt;
&lt;p&gt;Let’s now see a complete example.&lt;/p&gt;
&lt;h2 id="end-to-end-example"&gt;End-to-end example&lt;/h2&gt;
&lt;p&gt;Our playground will be &lt;a href="https://github.com/googlecreativelab/quickdraw-dataset"&gt;QuickDraw&lt;/a&gt;, a dataset – still growing – obtained by asking people to draw some object in at most twenty seconds, using the mouse. (To see for yourself, just check out the &lt;a href="https://quickdraw.withgoogle.com/"&gt;website&lt;/a&gt;). As of today, there are more than a fifty million instances, from 345 different classes.&lt;/p&gt;
&lt;p&gt;First and foremost, these data were chosen to take a break from MNIST and its variants. But just like those (and many more!), QuickDraw can be obtained, in &lt;code&gt;tfdatasets&lt;/code&gt;-ready form, via &lt;a href="https://github.com/rstudio/tfds"&gt;tfds&lt;/a&gt;, the R wrapper to TensorFlow datasets. In contrast to the MNIST “family” though, the “real samples” are themselves highly irregular, and often even missing essential parts. So to anchor judgment, when displaying generated samples we always show eight actual drawings with them.&lt;/p&gt;
&lt;h3 id="preparing-the-data"&gt;Preparing the data&lt;/h3&gt;
&lt;p&gt;The dataset being gigantic, we instruct &lt;code&gt;tfds&lt;/code&gt; to load the first 500,000 drawings “only”.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)

# &amp;gt;= 2.2 required
library(tensorflow)
library(keras)

# make sure to use at least version 0.10
library(tfprobability)

library(tfdatasets)
# currently to be installed from github
library(tfds)

# load just the first 500,000 images
# nonetheless, initially the complete dataset will be downloaded and unpacked
# ... be prepared for this to take some time
train_ds &amp;lt;- tfds_load(&amp;quot;quickdraw_bitmap&amp;quot;, split=&amp;#39;train[:500000]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To speed up training further, we then zoom in on twenty classes. This effectively leaves us with ~ 1,100 - 1,500 drawings per class.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# bee, bicycle, broccoli, butterfly, cactus,
# frog, guitar, lightning, penguin, pizza,
# rollerskates, sea turtle, sheep, snowflake, sun,
# swan, The Eiffel Tower, tractor, train, tree
classes &amp;lt;- c(26, 29, 43, 49, 50,
             125, 134, 172, 218, 225,
             246, 255, 258, 271, 295,
             296, 308, 320, 322, 323
)

classes_tensor &amp;lt;- tf$cast(classes, tf$int64)

train_ds &amp;lt;- train_ds %&amp;gt;%
  dataset_filter(
    function(record) tf$reduce_any(tf$equal(classes_tensor, record$label), -1L)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The PixelCNN distribution expects values in the range from 0 to 255 – no normalization required. Preprocessing then consists of just casting pixels and labels each to &lt;code&gt;float&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;preprocess &amp;lt;- function(record) {
  record$image &amp;lt;- tf$cast(record$image, tf$float32) 
  record$label &amp;lt;- tf$cast(record$label, tf$float32)
  list(tuple(record$image, record$label))
}

batch_size &amp;lt;- 32

train &amp;lt;- train_ds %&amp;gt;%
  dataset_map(preprocess) %&amp;gt;%
  dataset_shuffle(10000) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="creating-the-model"&gt;Creating the model&lt;/h3&gt;
&lt;p&gt;We now use &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_pixel_cnn.html"&gt;tfd_pixel_cnn&lt;/a&gt; to define what will be the loglikelihood used by the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dist &amp;lt;- tfd_pixel_cnn(
  image_shape = c(28, 28, 1),
  conditional_shape = list(),
  num_resnet = 5,
  num_hierarchies = 3,
  num_filters = 128,
  num_logistic_mix = 5,
  dropout_p =.5
)

image_input &amp;lt;- layer_input(shape = c(28, 28, 1))
label_input &amp;lt;- layer_input(shape = list())
log_prob &amp;lt;- dist %&amp;gt;% tfd_log_prob(image_input, conditional_input = label_input)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This custom loglikelihood is added as a loss to the model, and then, the model is compiled with just an optimizer specification only. During training, loss first decreased quickly, but improvements from later epochs were smaller.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(inputs = list(image_input, label_input), outputs = log_prob)
model$add_loss(-tf$reduce_mean(log_prob))
model$compile(optimizer = optimizer_adam(lr = .001))

model %&amp;gt;% fit(train, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To jointly display real and fake images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (i in classes) {
  
  real_images &amp;lt;- train_ds %&amp;gt;%
    dataset_filter(
      function(record) record$label == tf$cast(i, tf$int64)
    ) %&amp;gt;% 
    dataset_take(8) %&amp;gt;%
    dataset_batch(8)
  it &amp;lt;- as_iterator(real_images)
  real_images &amp;lt;- iter_next(it)
  real_images &amp;lt;- real_images$image %&amp;gt;% as.array()
  real_images &amp;lt;- real_images[ , , , 1]/255
  
  generated_images &amp;lt;- dist %&amp;gt;% tfd_sample(8, conditional_input = i)
  generated_images &amp;lt;- generated_images %&amp;gt;% as.array()
  generated_images &amp;lt;- generated_images[ , , , 1]/255
  
  images &amp;lt;- abind::abind(real_images, generated_images, along = 1)
  png(paste0(&amp;quot;draw_&amp;quot;, i, &amp;quot;.png&amp;quot;), width = 8 * 28 * 10, height = 2 * 28 * 10)
  par(mfrow = c(2, 8), mar = c(0, 0, 0, 0))
  images %&amp;gt;%
    purrr::array_tree(1) %&amp;gt;%
    purrr::map(as.raster) %&amp;gt;%
    purrr::iwalk(plot)
  dev.off()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From our twenty classes, here’s a choice of six, each showing real drawings in the top row, and fake ones below.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_29.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 4: Bicycles, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_43.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 5: Broccoli, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_49.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 6: Butterflies, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_134.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 7: Guitars, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_218.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 8: Penguins, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_246.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 9: Roller skates, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We probably wouldn’t confuse the first and second rows, but then, the actual human drawings exhibit enormous variation, too. And no one ever said PixelCNN was an architecture for concept learning. Feel free to play around with other datasets of your choice – TFP’s PixelCNN distribution makes it easy.&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In this post, we had &lt;code&gt;tfprobability&lt;/code&gt; / TFP do all the heavy lifting for us, and so, could focus on the underlying concepts. Depending on your inclinations, this can be an ideal situation – you don’t lose sight of the forest for the trees. On the other hand: Should you find that changing the provided parameters doesn’t achieve what you want, you have a reference implementation to start from. So whatever the outcome, the addition of such higher-level functionality to TFP is a win for the users. (If you’re a TFP developer reading this: Yes, we’d like more :-)).&lt;/p&gt;
&lt;p&gt;To everyone though, thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-OordKK16"&gt;
&lt;p&gt;Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. “Pixel Recurrent Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1601.06759. &lt;a href="http://arxiv.org/abs/1601.06759"&gt;http://arxiv.org/abs/1601.06759&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-OordKVEGK16"&gt;
&lt;p&gt;Oord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with Pixelcnn Decoders.” &lt;em&gt;CoRR&lt;/em&gt; abs/1606.05328. &lt;a href="http://arxiv.org/abs/1606.05328"&gt;http://arxiv.org/abs/1606.05328&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Salimans2017PixeCNN"&gt;
&lt;p&gt;Salimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: A Pixelcnn Implementation with Discretized Logistic Mixture Likelihood and Other Modifications.” In &lt;em&gt;ICLR&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Alluding to Ali Rahimi’s (in)famous “deep learning is alchemy” &lt;a href="https://www.youtube.com/watch?v=Qi1Yry33TQE"&gt;talk&lt;/a&gt; at NeurIPS 2017. I would suspect that to some degree, that statement resonates with many DL practitioners – although one need not agree that more mathematical rigor is the solution.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;For details, see &lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;.For details, see &lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c31e1544cc97c983f93b63438fe1899f</distill:md5>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="400" height="203"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>


&lt;p&gt;How private are individual data in the context of machine learning models? The data used to train the model, say. There are types of models where the answer is simple. Take k-nearest-neighbors, for example. There &lt;em&gt;is not&lt;/em&gt; even a model without the complete dataset. Or support vector machines. There is no model without the support vectors. But neural networks? They’re just some composition of functions, – no data included.&lt;/p&gt;
&lt;p&gt;The same is true for data fed to a deployed deep-learning model. It’s pretty unlikely one could invert the final softmax output from a big ResNet and get back the raw input data.&lt;/p&gt;
&lt;p&gt;In theory, then, “hacking” a standard neural net to spy on input data sounds illusory. In practice, however, there is always some real-world &lt;em&gt;context&lt;/em&gt;. The context may be other datasets, publicly available, that can be linked to the “private” data in question. This is a popular showcase used in advocating for differential privacy&lt;span class="citation"&gt;(Dwork et al. 2006)&lt;/span&gt;: Take an “anonymized” dataset, dig up complementary information from public sources, and de-anonymize records ad libitum. Some context in that sense will often be used in “black-box” attacks, ones that presuppose no insider information about the model to be hacked.&lt;/p&gt;
&lt;p&gt;But context can also be structural, such as in the scenario demonstrated in this post. For example, assume a distributed model, where sets of layers run on different devices – embedded devices or mobile phones, for example. (A scenario like that is sometimes seen as “white-box”&lt;span class="citation"&gt;(Wu et al. 2016)&lt;/span&gt;, but in common understanding, white-box attacks probably presuppose some more insider knowledge, such as access to model architecture or even, weights. I’d therefore prefer calling this white-ish at most.) — Now assume that in this context, it is possible to intercept, and interact with, a system that executes the deeper layers of the model. Based on that system’s intermediate-level output, it is possible to perform &lt;em&gt;model inversion&lt;/em&gt;&lt;span class="citation"&gt;(Fredrikson et al. 2014)&lt;/span&gt;, that is, to reconstruct the input data fed into the system.&lt;/p&gt;
&lt;p&gt;In this post, we’ll demonstrate such a model inversion attack, basically porting the approach given in a &lt;a href="https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/privacy_attacks/Tutorial%201%20-%20Black%20box%20model%20inversion.ipynb%20Sy"&gt;notebook&lt;/a&gt; found in the &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt; repository. We then experiment with different levels of &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-privacy, exploring impact on reconstruction success. This second part will make use of TensorFlow Privacy, introduced in a &lt;a href="https://blogs.rstudio.com/ai/posts/2019-12-20-differential-privacy/"&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="part-1-model-inversion-in-action"&gt;Part 1: Model inversion in action&lt;/h2&gt;
&lt;h3 id="example-dataset-all-the-worlds-letters1"&gt;Example dataset: All the world’s letters&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The overall process of model inversion used here is the following. With no, or scarcely any, insider knowledge about a model, – but given opportunities to repeatedly query it –, I want to learn how to reconstruct unknown inputs based on just model outputs . Independently of original model training, this, too, is a training process; however, in general it will not involve the original data, as those won’t be publicly available. Still, for best success, the attacker model is trained with data as similar as possible to the original training data assumed. Thinking of images, for example, and presupposing the popular view of successive layers representing successively coarse-grained features, we want that the surrogate data to share as many representation spaces with the real data as possible – up to the very highest layers before final classification, ideally.&lt;/p&gt;
&lt;p&gt;If we wanted to use classical MNIST as an example, one thing we could do is to only use some of the digits for training the “real” model; and the rest, for training the adversary. Let’s try something different though, something that might make the undertaking harder as well as easier at the same time. Harder, because the dataset features exemplars more complex than MNIST digits; easier because of the same reason: More could possibly be learned, by the adversary, from a complex task.&lt;/p&gt;
&lt;p&gt;Originally designed to develop a machine model of concept learning and generalization &lt;span class="citation"&gt;(Lake, Salakhutdinov, and Tenenbaum 2015)&lt;/span&gt;, the &lt;a href="https://github.com/brendenlake/omniglot/"&gt;OmniGlot&lt;/a&gt; dataset incorporates characters from fifty alphabets, split into two disjoint groups of thirty and twenty alphabets each. We’ll use the group of twenty to train our target model. Here is a sample:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/training_set_images.png" alt="Sample from the twenty-alphabet set used to train the target model (originally: 'evaluation set')" width="398" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Sample from the twenty-alphabet set used to train the target model (originally: ‘evaluation set’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The group of thirty we don’t use; instead, we’ll employ two small five-alphabet collections to train the adversary and to test reconstruction, respectively. (These small subsets of the original “big” thirty-alphabet set are again disjoint.)&lt;/p&gt;
&lt;p&gt;Here first is a sample from the set used to train the adversary.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/attacker_ds.png" alt="Sample from the five-alphabet set used to train the adversary (originally: 'background small 1')" width="404" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-2)Sample from the five-alphabet set used to train the adversary (originally: ‘background small 1’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The other small subset will be used to test the adversary’s spying capabilities after training. Let’s peek at this one, too:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/actual_test_images.png" alt="Sample from the five-alphabet set used to test the adversary after training(originally: 'background small 2')" width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Sample from the five-alphabet set used to test the adversary after training(originally: ‘background small 2’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Conveniently, we can use &lt;a href="https://github.com/rstudio/tfds"&gt;tfds&lt;/a&gt;, the R wrapper to TensorFlow Datasets, to load those subsets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
library(keras)
library(tfdatasets)
library(tfautograph)
library(tfds)

library(purrr)

# we&amp;#39;ll use this to train the target model
# n = 13180
omni_train &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;test&amp;quot;)

# this is used to train the adversary
# n = 2720
omni_spy &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;small1&amp;quot;)

# this we&amp;#39;ll use for testing
# n = 3120
omni_test &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;small2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now first, we train the target model.&lt;/p&gt;
&lt;h3 id="train-target-model"&gt;Train target model&lt;/h3&gt;
&lt;p&gt;The dataset originally has four columns: the image, of size 105 x 105; an alphabet id and a within-dataset character id; and a label. For our use case, we’re not really interested in the task the target model was/is used for; we just want to get at the data. Basically, whatever task we choose, it is not much more than a dummy task. So, let’s just say we train the target to classify characters &lt;em&gt;by alphabet&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We thus throw out all unneeded features, keeping just the alphabet id and the image itself:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# normalize and work with a single channel (images are black-and-white anyway)
preprocess_image &amp;lt;- function(image) {
  image %&amp;gt;%
    tf$cast(dtype = tf$float32) %&amp;gt;%
    tf$truediv(y = 255) %&amp;gt;%
    tf$image$rgb_to_grayscale()
}

# use the first 11000 images for training
train_ds &amp;lt;- omni_train %&amp;gt;% 
  dataset_take(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_shuffle(1000) %&amp;gt;% 
  dataset_batch(32)

# use the remaining 2180 records for validation
val_ds &amp;lt;- omni_train %&amp;gt;% 
  dataset_skip(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_batch(32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model consists of two parts. The first is imagined to run in a distributed fashion; for example, on mobile devices (stage one). These devices then send model outputs to a central server, where final results are computed (stage two). Sure, you will be thinking, this is a convenient setup for our scenario: If we intercept stage one results, we – most probably – gain access to richer information than what is contained in a model’s final output layer. — That is correct, but the scenario is less contrived than one might assume. Just like federated learning &lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt;, it fulfills important desiderata: Actual training data never leaves the devices, thus staying (in theory!) private; at the same time, ingoing traffic to the server is significantly reduced.&lt;/p&gt;
&lt;p&gt;In our example setup, the on-device model is a convnet, while the server model is a simple feedforward network.&lt;/p&gt;
&lt;p&gt;We link both together as a &lt;em&gt;TargetModel&lt;/em&gt; that when called normally, will run both steps in succession. However, we’ll be able to call &lt;code&gt;target_model$mobile_step()&lt;/code&gt; separately, thereby intercepting intermediate results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;on_device_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7),
                input_shape = c(105, 105, 1), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  layer_dropout(0.2) 

server_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;% 
  # we have just 20 different ids, but they are not in lexicographic order
  layer_dense(units = 50, activation = &amp;quot;softmax&amp;quot;)

target_model &amp;lt;- function() {
  keras_model_custom(name = &amp;quot;TargetModel&amp;quot;, function(self) {
    
    self$on_device_model &amp;lt;-on_device_model
    self$server_model &amp;lt;- server_model
    self$mobile_step &amp;lt;- function(inputs) 
      self$on_device_model(inputs)
    self$server_step &amp;lt;- function(inputs)
      self$server_model(inputs)

    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        self$mobile_step() %&amp;gt;%
        self$server_step()
    }
  })
  
}

model &amp;lt;- target_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall model is a Keras custom model, so we train it &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;TensorFlow 2.x - style&lt;/a&gt;. After ten epochs, training and validation accuracy are at ~0.84 and ~0.73, respectively – not bad at all for a 20-class discrimination task.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- loss_sparse_categorical_crossentropy
optimizer &amp;lt;- optimizer_adam()

train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)
train_accuracy &amp;lt;-  tf$keras$metrics$SparseCategoricalAccuracy(name=&amp;#39;train_accuracy&amp;#39;)

val_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;val_loss&amp;#39;)
val_accuracy &amp;lt;-  tf$keras$metrics$SparseCategoricalAccuracy(name=&amp;#39;val_accuracy&amp;#39;)

train_step &amp;lt;- function(images, labels) {
  with (tf$GradientTape() %as% tape, {
    predictions &amp;lt;- model(images)
    l &amp;lt;- loss(labels, predictions)
  })
  gradients &amp;lt;- tape$gradient(l, model$trainable_variables)
  optimizer$apply_gradients(purrr::transpose(list(
    gradients, model$trainable_variables
  )))
  train_loss(l)
  train_accuracy(labels, predictions)
}

val_step &amp;lt;- function(images, labels) {
  predictions &amp;lt;- model(images)
  l &amp;lt;- loss(labels, predictions)
  val_loss(l)
  val_accuracy(labels, predictions)
}


training_loop &amp;lt;- tf_function(autograph(function(train_ds, val_ds) {
  for (b1 in train_ds) {
    train_step(b1[[1]], b1[[2]])
  }
  for (b2 in val_ds) {
    val_step(b2[[1]], b2[[2]])
  }
  
  tf$print(&amp;quot;Train accuracy&amp;quot;, train_accuracy$result(),
           &amp;quot;    Validation Accuracy&amp;quot;, val_accuracy$result())
  
  train_loss$reset_states()
  train_accuracy$reset_states()
  val_loss$reset_states()
  val_accuracy$reset_states()
}))


for (epoch in 1:10) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop(train_ds, val_ds)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:  1  -----------
Train accuracy 0.195090905     Validation Accuracy 0.376605511
Epoch:  2  -----------
Train accuracy 0.472272724     Validation Accuracy 0.5243119
...
...
Epoch:  9  -----------
Train accuracy 0.821454525     Validation Accuracy 0.720183492
Epoch:  10  -----------
Train accuracy 0.840454519     Validation Accuracy 0.726605475&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we train the adversary.&lt;/p&gt;
&lt;h3 id="train-adversary"&gt;Train adversary&lt;/h3&gt;
&lt;p&gt;The adversary’s general strategy will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feed its small, surrogate dataset to the on-device model.&lt;/strong&gt; The output received can be regarded as a (highly) &lt;em&gt;compressed&lt;/em&gt; version of the original images.&lt;/li&gt;
&lt;li&gt;P&lt;strong&gt;ass that “compressed” version as input to its own model,&lt;/strong&gt; which tries to reconstruct the original images from the sparse code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compare original images (those from the surrogate dataset) to the reconstruction pixel-wise.&lt;/strong&gt; The goal is to minimize the mean (squared, say) error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Doesn’t this sound a lot like the decoding side of an autoencoder? No wonder the attacker model is a deconvolutional network. Its input – equivalently, the on-device model’s output – is of size &lt;code&gt;batch_size x 1 x 1 x 32&lt;/code&gt;. That is, the information is encoded in 32 channels, but the spatial resolution is 1. Just like in an autoencoder operating on images, we need to &lt;em&gt;upsample&lt;/em&gt; until we arrive at the original resolution of 105 x 105.&lt;/p&gt;
&lt;p&gt;This is exactly what’s happening in the attacker model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attack_model &amp;lt;- function() {
  
  keras_model_custom(name = &amp;quot;AttackModel&amp;quot;, function(self) {
    
    self$conv1 &amp;lt;-layer_conv_2d_transpose(filters = 32, kernel_size = 9,
                                         padding = &amp;quot;valid&amp;quot;,
                                         strides = 1, activation = &amp;quot;relu&amp;quot;)
    self$conv2 &amp;lt;- layer_conv_2d_transpose(filters = 32, kernel_size = 7,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;) 
    self$conv3 &amp;lt;- layer_conv_2d_transpose(filters = 1, kernel_size = 7,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;)  
    self$conv4 &amp;lt;- layer_conv_2d_transpose(filters = 1, kernel_size = 5,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;)
    
    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        # bs * 9 * 9 * 32
        # output = strides * (input - 1) + kernel_size - 2 * padding
        self$conv1() %&amp;gt;%
        # bs * 23 * 23 * 32
        self$conv2() %&amp;gt;%
        # bs * 51 * 51 * 1
        self$conv3() %&amp;gt;%
        # bs * 105 * 105 * 1
        self$conv4()
    }
  })
  
}

attacker = attack_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To train the adversary, we use one of the small (five-alphabet) subsets. To reiterate what was said above, there is no overlap with the data used to train the target model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attacker_ds &amp;lt;- omni_spy %&amp;gt;% 
dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_batch(32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, then, is the attacker training loop, striving to refine the decoding process over a hundred – short – epochs:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attacker_criterion &amp;lt;- loss_mean_squared_error
attacker_optimizer &amp;lt;- optimizer_adam()
attacker_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;attacker_loss&amp;#39;)
attacker_mse &amp;lt;-  tf$keras$metrics$MeanSquaredError(name=&amp;#39;attacker_mse&amp;#39;)

attacker_step &amp;lt;- function(images) {
  
  attack_input &amp;lt;- model$mobile_step(images)
  
  with (tf$GradientTape() %as% tape, {
    generated &amp;lt;- attacker(attack_input)
    l &amp;lt;- attacker_criterion(images, generated)
  })
  gradients &amp;lt;- tape$gradient(l, attacker$trainable_variables)
  attacker_optimizer$apply_gradients(purrr::transpose(list(
    gradients, attacker$trainable_variables
  )))
  attacker_loss(l)
  attacker_mse(images, generated)
}


attacker_training_loop &amp;lt;- tf_function(autograph(function(attacker_ds) {
  for (b in attacker_ds) {
    attacker_step(b[[1]])
  }
  
  tf$print(&amp;quot;mse: &amp;quot;, attacker_mse$result())
  
  attacker_loss$reset_states()
  attacker_mse$reset_states()
}))

for (epoch in 1:100) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  attacker_training_loop(attacker_ds)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:  1  -----------
  mse:  0.530902684
Epoch:  2  -----------
  mse:  0.201351956
...
...
Epoch:  99  -----------
  mse:  0.0413453057
Epoch:  100  -----------
  mse:  0.0413028933&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The question now is, – does it work? Has the attacker really learned to infer actual data from (stage one) model output?&lt;/p&gt;
&lt;h3 id="test-adversary"&gt;Test adversary&lt;/h3&gt;
&lt;p&gt;To test the adversary, we use the third dataset we downloaded, containing images from five yet-unseen alphabets. For display, we select just the first sixteen records – a completely arbitrary decision, of course.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_ds &amp;lt;- omni_test %&amp;gt;% 
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_take(16) %&amp;gt;%
  dataset_batch(16)

batch &amp;lt;- as_iterator(test_ds) %&amp;gt;% iterator_get_next()
images &amp;lt;- batch[[1]]

attack_input &amp;lt;- model$mobile_step(images)
generated &amp;lt;- attacker(attack_input) %&amp;gt;% as.array()

generated[generated &amp;gt; 1] &amp;lt;- 1
generated &amp;lt;- generated[ , , , 1]
generated %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like during the training process, the adversary queries the target model (stage one), obtains the compressed representation, and attempts to reconstruct the original image. (Of course, in the real world, the setup would be different in that the attacker would &lt;em&gt;not&lt;/em&gt; be able to simply inspect the images, as is the case here. There would thus have to be some way to intercept, and make sense of, network traffic.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attack_input &amp;lt;- model$mobile_step(images)
generated &amp;lt;- attacker(attack_input) %&amp;gt;% as.array()

generated[generated &amp;gt; 1] &amp;lt;- 1
generated &amp;lt;- generated[ , , , 1]
generated %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To allow for easier comparison (and increase suspense …!), here again are the actual images, which we displayed already when introducing the dataset:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/actual_test_images.png" alt="First images from the test set, the way they really look." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)First images from the test set, the way they really look.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And here is the reconstruction:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/recon_noeps_dropout.png" alt="First images from the test set, as reconstructed by the adversary." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)First images from the test set, as reconstructed by the adversary.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Of course, it is hard to say how revealing these “guesses” are. There definitely seems to be a connection to character complexity; overall, it seems like the Greek and Roman letters, which are the least complex, are also the ones most easily reconstructed. Still, in the end, how much privacy is lost will very much depend on contextual factors.&lt;/p&gt;
&lt;p&gt;First and foremost, do the exemplars in the dataset represent &lt;em&gt;individuals&lt;/em&gt; or &lt;em&gt;classes&lt;/em&gt; of individuals? If – as in reality – the character &lt;code&gt;X&lt;/code&gt; represents a class, it might not be so grave if we were able to reconstruct “some X” here: There are many &lt;code&gt;X&lt;/code&gt;s in the dataset, all pretty similar to each other; we’re unlikely to exactly to have reconstructed one special, individual &lt;code&gt;X&lt;/code&gt;. If, however, this was a dataset of individual people, with all &lt;code&gt;X&lt;/code&gt;s being photographs of Alex, then in reconstructing an &lt;code&gt;X&lt;/code&gt; we have effectively reconstructed Alex.&lt;/p&gt;
&lt;p&gt;Second, in less obvious scenarios, evaluating the degree of privacy breach will likely surpass computation of quantitative metrics, and involve the judgment of domain experts.&lt;/p&gt;
&lt;p&gt;Speaking of quantitative metrics though – our example seems like a perfect use case to experiment with &lt;em&gt;differential privacy.&lt;/em&gt; Differential privacy is measured by &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; (lower is better), the main idea being that answers to queries to a system should depend as little as possible on the presence or absence of a single (&lt;em&gt;any&lt;/em&gt; single) datapoint.&lt;/p&gt;
&lt;p&gt;So, we will repeat the above experiment, using TensorFlow Privacy (TFP) to add noise, as well as clip gradients, during optimization of the target model. We’ll try three different conditions, resulting in three different values for &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s, and for each condition, inspect the images reconstructed by the adversary.&lt;/p&gt;
&lt;h2 id="part-2-differential-privacy-to-the-rescue"&gt;Part 2: Differential privacy to the rescue&lt;/h2&gt;
&lt;p&gt;Unfortunately, the setup for this part of the experiment requires a little workaround. Making use of the flexibility afforded by TensorFlow 2.x, our target model has been a custom model, joining two distinct stages (“mobile” and “server”) that could be called independently.&lt;/p&gt;
&lt;p&gt;TFP, however, does still not work with TensorFlow 2.x, meaning we have to use old-style, non-eager model definitions and training. Luckily, the workaround will be easy.&lt;/p&gt;
&lt;p&gt;First, load (and possibly, install) libraries, taking care to disable TensorFlow V2 behavior.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tensorflow)
# still necessary when working with TensorFlow Privacy, as of this writing
tf$compat$v1$disable_v2_behavior()

# if you don&amp;#39;t have it installed:
# reticulate::py_install(&amp;quot;tensorflow_privacy&amp;quot;)
tfp &amp;lt;- import(&amp;quot;tensorflow_privacy&amp;quot;)

library(tfdatasets)
library(tfds)

library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training set is loaded, preprocessed and batched (nearly) as before.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;omni_train &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;test&amp;quot;)

batch_size &amp;lt;- 32

train_ds &amp;lt;- omni_train %&amp;gt;%
  dataset_take(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_shuffle(1000) %&amp;gt;%
  # need dataset_repeat() when not eager
  dataset_repeat() %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="train-target-model-with-tensorflow-privacy"&gt;Train target model – with TensorFlow Privacy&lt;/h3&gt;
&lt;p&gt;To train the target, we put the layers from both stages – “mobile” and “server” – into one sequential model. Note how we remove the dropout. This is because noise will be added during optimization anyway.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;complete_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7),
                input_shape = c(105, 105, 1),
                activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2, name = &amp;quot;mobile_output&amp;quot;) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 50, activation = &amp;quot;softmax&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using TFP mainly means using a TFP optimizer, one that clips gradients according to some defined magnitude and adds noise of defined size. &lt;code&gt;noise_multiplier&lt;/code&gt; is the parameter we are going to vary to arrive at different &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l2_norm_clip &amp;lt;- 1

# ratio of the standard deviation to the clipping norm
# we run training for each of the three values
noise_multiplier &amp;lt;- 0.7
noise_multiplier &amp;lt;- 0.5
noise_multiplier &amp;lt;- 0.3

# same as batch size
num_microbatches &amp;lt;- k_cast(batch_size, &amp;quot;int32&amp;quot;)
learning_rate &amp;lt;- 0.005

optimizer &amp;lt;- tfp$DPAdamGaussianOptimizer(
  l2_norm_clip = l2_norm_clip,
  noise_multiplier = noise_multiplier,
  num_microbatches = num_microbatches,
  learning_rate = learning_rate
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In training the model, the second important change for TFP we need to make is to have loss and gradients computed on the individual level.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# need to add noise to every individual contribution
loss &amp;lt;- tf$keras$losses$SparseCategoricalCrossentropy(reduction =   tf$keras$losses$Reduction$NONE)

complete_model %&amp;gt;% compile(loss = loss, optimizer = optimizer, metrics = &amp;quot;sparse_categorical_accuracy&amp;quot;)

num_epochs &amp;lt;- 20

n_train &amp;lt;- 13180

history &amp;lt;- complete_model %&amp;gt;% fit(
  train_ds,
  # need steps_per_epoch when not in eager mode
  steps_per_epoch = n_train/batch_size,
  epochs = num_epochs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To test three different &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s, we run this thrice, each time with a different &lt;code&gt;noise_multiplier&lt;/code&gt;. Each time we arrive at a different final accuracy.&lt;/p&gt;
&lt;p&gt;Here is a synopsis, where &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; was computed like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_priv &amp;lt;- tfp$privacy$analysis$compute_dp_sgd_privacy

compute_priv$compute_dp_sgd_privacy(
  # number of records in training set
  n_train,
  batch_size,
  # noise_multiplier
  0.7, # or 0.5, or 0.3
  # number of epochs
  20,
  # delta - should not exceed 1/number of examples in training set
  1e-5)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;noise multiplier&lt;/th&gt;
&lt;th&gt;epsilon&lt;/th&gt;
&lt;th&gt;final acc. (training set)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;0.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;12.5&lt;/td&gt;
&lt;td&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;84.7&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now, as the adversary won’t call the complete model, we need to “cut off” the second-stage layers. This leaves us with a model that executes stage-one logic only. We save its weights, so we can later call it from the adversary:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;intercepted &amp;lt;- keras_model(
  complete_model$input,
  complete_model$get_layer(&amp;quot;mobile_output&amp;quot;)$output
)

intercepted %&amp;gt;% save_model_hdf5(&amp;quot;./intercepted.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="train-adversary-against-differentially-private-target"&gt;Train adversary (against differentially private target)&lt;/h3&gt;
&lt;p&gt;In training the adversary, we can keep most of the original code – meaning, we’re back to TF-2 style. Even the definition of the target model is the same as before:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;on_device_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...]

server_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...]

target_model &amp;lt;- function() {
  keras_model_custom(name = &amp;quot;TargetModel&amp;quot;, function(self) {
    
    self$on_device_model &amp;lt;-on_device_model
    self$server_model &amp;lt;- server_model
    self$mobile_step &amp;lt;- function(inputs) 
      self$on_device_model(inputs)
    self$server_step &amp;lt;- function(inputs)
      self$server_model(inputs)
    
    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        self$mobile_step() %&amp;gt;%
        self$server_step()
    }
  })
}

intercepted &amp;lt;- target_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But now, we load the trained target’s weights into the freshly defined model’s “mobile stage”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;intercepted$on_device_model$load_weights(&amp;quot;intercepted.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, we’re back to the old training routine. Testing setup is the same as before, as well.&lt;/p&gt;
&lt;p&gt;So how well does the adversary perform with differential privacy added to the picture?&lt;/p&gt;
&lt;h3 id="test-adversary-against-differentially-private-target"&gt;Test adversary (against differentially private target)&lt;/h3&gt;
&lt;p&gt;Here, ordered by decreasing &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;, are the reconstructions. Again, we refrain from judging the results, for the same reasons as before: In real-world applications, whether privacy is preserved “well enough” will depend on the context.&lt;/p&gt;
&lt;p&gt;Here, first, are reconstructions from the run where the least noise was added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_84.7.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7." width="384" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-24)Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On to the next level of privacy protection:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_12.5.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5." width="385" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-25)Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And the highest-&lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; one:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_4.0.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-26)Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Throughout this post, we’ve refrained from “over-commenting” on results, and focused on the why-and-how instead. This is because in an artificial setup, chosen to facilitate exposition of concepts and methods, there really is no objective frame of reference. What is a good reconstruction? What is a good &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;? What constitutes a data breach? No-one knows.&lt;/p&gt;
&lt;p&gt;In the real world, there is a context to everything – there are people involved, the people whose data we’re talking about. There are organizations, regulations, laws. There are abstract principles, and there are implementations; different implementations of the same “idea” can differ.&lt;/p&gt;
&lt;p&gt;As in machine learning overall, research papers on privacy-, ethics- or otherwise society-related topics are full of LaTeX formulae. Amid the math, let’s not forget the people.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-Dwork2006"&gt;
&lt;p&gt;Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In &lt;em&gt;Proceedings of the Third Conference on Theory of Cryptography&lt;/em&gt;, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. &lt;a href="https://doi.org/10.1007/11681878_14"&gt;https://doi.org/10.1007/11681878_14&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Fred"&gt;
&lt;p&gt;Fredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. “Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.” In &lt;em&gt;Proceedings of the 23rd Usenix Conference on Security Symposium&lt;/em&gt;, 17–32. SEC’14. USA: USENIX Association.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Lake1332"&gt;
&lt;p&gt;Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. “Human-Level Concept Learning Through Probabilistic Program Induction.” &lt;em&gt;Science&lt;/em&gt; 350 (6266): 1332–8. &lt;a href="https://doi.org/10.1126/science.aab3050"&gt;https://doi.org/10.1126/science.aab3050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-McMahanMRA16"&gt;
&lt;p&gt;McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” &lt;em&gt;CoRR&lt;/em&gt; abs/1602.05629. &lt;a href="http://arxiv.org/abs/1602.05629"&gt;http://arxiv.org/abs/1602.05629&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-7536387"&gt;
&lt;p&gt;Wu, X., M. Fredrikson, S. Jha, and J. F. Naughton. 2016. “A Methodology for Formalizing Model-Inversion Attacks.” In &lt;em&gt;2016 Ieee 29th Computer Security Foundations Symposium (Csf)&lt;/em&gt;, 355–70.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Don’t take &lt;em&gt;all&lt;/em&gt; literally please; it’s just a nice phrase.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">bd63aed9dff1f6f4ec1897da9c5c7915</distill:md5>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="600" height="394"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>


&lt;p&gt;The word &lt;em&gt;privacy&lt;/em&gt;, in the context of deep learning (or machine learning, or “AI”), and especially when combined with things like &lt;em&gt;security&lt;/em&gt;, sounds like it could be part of a catch phrase: &lt;em&gt;privacy, safety, security&lt;/em&gt; – like &lt;em&gt;liberté, fraternité, égalité&lt;/em&gt;. In fact, there should probably be a mantra like that. But that’s another topic, and like with the other catch phrase just cited, not everyone interprets these terms in the same way.&lt;/p&gt;
&lt;p&gt;So let’s think about privacy, narrowed down to its role in training or using deep learning models, in a more technical way. Since privacy – or rather, its violations – may appear in various ways, different violations will demand different countermeasures. Of course, in the end, we’d like to see them all integrated – but re privacy-related technologies, the field is really just starting out on a journey. The most important thing we can do, then, is to learn about the concepts, investigate the landscape of implementations under development, and – perhaps – decide to join the effort.&lt;/p&gt;
&lt;p&gt;This post tries to do a tiny little bit of all of those.&lt;/p&gt;
&lt;h2 id="aspects-of-privacy-in-deep-learning"&gt;Aspects of privacy in deep learning&lt;/h2&gt;
&lt;p&gt;Say you work at a hospital, and would be interested in training a deep learning model to help diagnose some disease from brain scans. Where you work, you don’t have many patients with this disease; moreover, they tend to mostly be affected by the same subtypes: Your training set, were you to create one, would not reflect the overall distribution very well. It would, thus, make sense to cooperate with other hospitals; but that isn’t so easy, as the data collected is protected by privacy regulations. So, the first requirement is: The data has to stay where it is; e.g., it may not be sent to a central server.&lt;/p&gt;
&lt;h4 id="federated-learning"&gt;Federated learning&lt;/h4&gt;
&lt;p&gt;This first &lt;em&gt;sine qua non&lt;/em&gt; is addressed by &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/"&gt;federated learning&lt;/a&gt; &lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt;. Federated learning is not “just” desirable for privacy reasons. On the contrary, in many use cases, it may be the only viable way (like with smartphones or sensors, which collect gigantic amounts of data). In federated learning, each participant receives a copy of the model, trains on their own data, and sends back the gradients obtained to the central server, where gradients are averaged and applied to the model.&lt;/p&gt;
&lt;p&gt;This is good insofar as the data never leaves the individual devices; however, a lot of information can still be extracted from plain-text gradients. Imagine a smartphone app that provides trainable auto-completion for text messages. Even if gradient updates from many iterations are averaged, their distributions will greatly vary between individuals. Some form of encryption is needed. But then how is the server going to make sense of the encrypted gradients?&lt;/p&gt;
&lt;p&gt;One way to accomplish this relies on &lt;em&gt;secure multi-party computation&lt;/em&gt; (SMPC).&lt;/p&gt;
&lt;h4 id="secure-multi-party-computation"&gt;Secure multi-party computation&lt;/h4&gt;
&lt;p&gt;In SMPC, we need a system of several agents who collaborate to provide a result no single agent could provide alone: “normal” computations (like addition, multiplication …) on “secret” (encrypted) data. The assumption is that these agents are “honest but curious” – honest, because they won’t tamper with their share of data; curious in the sense that if they &lt;em&gt;were&lt;/em&gt; (curious, that is), they wouldn’t be able to inspect the data because it’s encrypted.&lt;/p&gt;
&lt;p&gt;The principle behind this is &lt;em&gt;secret sharing&lt;/em&gt;. A single piece of data – a salary, say – is “split up” into meaningless (hence, encrypted) parts which, when put together again, yield the original data. Here is an example.&lt;/p&gt;
&lt;p&gt;Say the parties involved are Julia, Greg, and me. The below function encrypts a single value, assigning to each of us their “meaningless” share:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# a big prime number
# all computations are performed in a finite field, for example, the integers modulo that prime
Q &amp;lt;- 78090573363827
 
encrypt &amp;lt;- function(x) {
  # all but the very last share are random 
  julias &amp;lt;- runif(1, min = -Q, max = Q)
  gregs &amp;lt;- runif(1, min = -Q, max = Q)
  mine &amp;lt;- (x - julias - gregs) %% Q
  list (julias, gregs, mine)
}

# some top secret value no-one may get to see
value &amp;lt;- 77777

encrypted &amp;lt;- encrypt(value)
encrypted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 7467283737857

[[2]]
[1] 36307804406429

[[3]]
[1] 34315485297318&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the three of us put our shares together, getting back the plain value is straightforward:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decrypt &amp;lt;- function(shares) {
  Reduce(sum, shares) %% Q  
}

decrypt(encrypted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;77777&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example of how to compute on encrypted data, here’s addition. (Other operations will be a lot less straightforward.) To add two numbers, just have everyone add their respective shares:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;add &amp;lt;- function(x, y) {
  list(
    # julia
    (x[[1]] + y[[1]]) %% Q,
    # greg
    (x[[2]] + y[[2]]) %% Q,
    # me
    (x[[3]] + y[[3]]) %% Q
  )
}
  
x &amp;lt;- encrypt(11)
y &amp;lt;- encrypt(122)

decrypt(add(x, y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Back to the setting of deep learning and the current task to be solved: Have the server apply gradient updates without ever seeing them. With secret sharing, it would work like this:&lt;/p&gt;
&lt;p&gt;Julia, Greg and me each want to train on our own private data. Together, we will be responsible for gradient averaging, that is, we’ll form a &lt;em&gt;cluster&lt;/em&gt; of &lt;em&gt;workers&lt;/em&gt; united in that task. Now, the model owner &lt;em&gt;secret shares&lt;/em&gt; the model, and we start training, each on their own data. After some number of iterations, we use secure averaging to combine our respective gradients. Then, all the server gets to see is the mean gradient, and there is no way to determine our respective contributions.&lt;/p&gt;
&lt;h4 id="beyond-private-gradients"&gt;Beyond private gradients&lt;/h4&gt;
&lt;p&gt;Amazingly, it is even possible to &lt;em&gt;train&lt;/em&gt; on encrypted data – amongst others, using that same technique of secret sharing. Of course, this has to negatively affect training speed. But it’s good to know that if one’s use case were to demand it, it would be feasible. (One possible use case is when training on one party’s data alone doesn’t make any sense, but data is sensitive, so others won’t let you access their data unless encrypted.)&lt;/p&gt;
&lt;p&gt;So with encryption available on an all-you-need basis, are we completely safe, privacy-wise? The answer is no. The model can still leak information. For example, in some cases it is possible to perform &lt;em&gt;model inversion&lt;/em&gt; [@abs-1805-04049], that is, with just black-box access to a model, train an &lt;em&gt;attack model&lt;/em&gt; that allows reconstructing some of the original training data. Needless to say, this kind of leakage has to be avoided. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/"&gt;Differential privacy&lt;/a&gt; &lt;span class="citation"&gt;(Dwork et al. 2006)&lt;/span&gt;, &lt;span class="citation"&gt;(Dwork 2006)&lt;/span&gt; demands that results obtained from querying a model be independent from the presence or absence, in the dataset employed for training, of a single individual. In general, this is ensured by adding noise to the answer to every query. In training deep learning models, we add noise to the gradients, as well as clip them according to some chosen norm.&lt;/p&gt;
&lt;p&gt;At some point, then, we will want all of those in combination: federated learning, encryption, and differential privacy.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Syft&lt;/em&gt; is a very promising, very actively developed framework that aims for providing all of them. Instead of “aims for”, I should perhaps have written “provides” – it depends. We need some more context.&lt;/p&gt;
&lt;h2 id="introducing-syft"&gt;Introducing Syft&lt;/h2&gt;
&lt;p&gt;Syft – also known as &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt;, since as of today, its most mature implementation is written in and for Python – is maintained by &lt;a href="https://www.openmined.org/"&gt;OpenMined&lt;/a&gt;, an open source community dedicated to enabling privacy-preserving AI. It’s worth it reproducing their mission statement here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Industry standard tools for artificial intelligence have been designed with several assumptions: data is centralized into a single compute cluster, the cluster exists in a secure cloud, and the resulting models will be owned by a central authority. We envision a world in which we are not restricted to this scenario - a world in which AI tools treat privacy, security, and multi-owner governance as first class citizens. […] The mission of the OpenMined community is to create an accessible ecosystem of tools for private, secure, multi-owner governed AI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While far from being the only one, PySyft is their most maturely developed framework. Its role is to provide secure federated learning, including encryption and differential privacy. For deep learning, it relies on existing frameworks.&lt;/p&gt;
&lt;p&gt;PyTorch integration seems the most mature, as of today; with PyTorch, encrypted and differentially private training are already available. Integration with TensorFlow is a bit more involved; it does not yet include TensorFlow Federated and TensorFlow Privacy. For encryption, it relies on &lt;a href="https://github.com/tf-encrypted/tf-encrypted"&gt;TensorFlow Encrypted&lt;/a&gt; (TFE), which as of this writing is not an official TensorFlow subproject.&lt;/p&gt;
&lt;p&gt;However, even now it is already possible to &lt;em&gt;secret share&lt;/em&gt; Keras models and administer private predictions. Let’s see how.&lt;/p&gt;
&lt;h2 id="private-predictions-with-syft-tensorflow-encrypted-and-keras"&gt;Private predictions with Syft, TensorFlow Encrypted and Keras&lt;/h2&gt;
&lt;p&gt;Our introductory example will show how to use an externally-provided model to classify private data – without the model owner ever seeing that data, &lt;em&gt;and&lt;/em&gt; without the user ever getting hold of (e.g., downloading) the model. (Think about the model owner wanting to keep the fruits of their labour hidden, as well.)&lt;/p&gt;
&lt;p&gt;Put differently: The model is encrypted, and the data is, too. As you might imagine, this involves a cluster of agents, together performing secure multi-party computation.&lt;/p&gt;
&lt;p&gt;This use case presupposing an already trained model, we start by quickly creating one. There is nothing special going on here.&lt;/p&gt;
&lt;h4 id="prelude-train-a-simple-model-on-mnist"&gt;Prelude: Train a simple model on MNIST&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create_model.R

library(tensorflow)
library(keras)

mnist &amp;lt;- dataset_mnist()
mnist$train$x &amp;lt;- mnist$train$x/255
mnist$test$x &amp;lt;- mnist$test$x/255

dim(mnist$train$x) &amp;lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &amp;lt;- c(dim(mnist$test$x), 1)

input_shape &amp;lt;- c(28, 28, 1)

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 16, kernel_size = c(3, 3), input_shape = input_shape) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;linear&amp;quot;)
  

model %&amp;gt;% compile(
  loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;
)

model %&amp;gt;% fit(
    x = mnist$train$x,
    y = mnist$train$y,
    epochs = 1,
    validation_split = 0.3,
    verbose = 2
)

model$save(filepath = &amp;quot;model.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="set-up-cluster-and-serve-model"&gt;Set up cluster and serve model&lt;/h4&gt;
&lt;p&gt;The easiest way to get all required packages is to install the ensemble OpenMined put together for their &lt;a href="https://www.udacity.com/course/secure-and-private-ai--ud185"&gt;Udacity Course&lt;/a&gt; that introduces federated learning and differential privacy with PySyft. This will install TensorFlow 1.15 and TensorFlow Encrypted, amongst others.&lt;/p&gt;
&lt;p&gt;The following lines of code should all be put together in a single file. I found it practical to “source” this script from an R process running in a console tab.&lt;/p&gt;
&lt;p&gt;To begin, we again define the model, two things being different now. First, for technical reasons, we need to pass in &lt;code&gt;batch_input_shape&lt;/code&gt; instead of &lt;code&gt;input_shape&lt;/code&gt;. Second, the final layer is “missing” the softmax activation. This is not an oversight – SMPC &lt;code&gt;softmax&lt;/code&gt; has not been implemented yet. (Depending on when you read this, that statement may no longer be true.) Were we training this model in &lt;em&gt;secret sharing&lt;/em&gt; mode, this would of course be a problem; for classification though, all we care about is the maximum score.&lt;/p&gt;
&lt;p&gt;After model definition, we load the actual weights from the model we trained in the previous step. Then, the action begins. We create an ensemble of TFE workers that together run a distributed TensorFlow cluster. The model is &lt;em&gt;secret&lt;/em&gt; &lt;em&gt;shared&lt;/em&gt; with the workers, that is, model weights are split up into shares that, each inspected alone, are unusable. Finally, the model is &lt;em&gt;served&lt;/em&gt;, i.e., made available to clients requesting predictions.&lt;/p&gt;
&lt;p&gt;How can a Keras model be &lt;em&gt;shared&lt;/em&gt; and &lt;em&gt;served&lt;/em&gt;? These are not methods provided by Keras itself. The magic comes from Syft &lt;em&gt;hooking&lt;/em&gt; into Keras, extending the &lt;code&gt;model&lt;/code&gt; object: cf. &lt;code&gt;hook &amp;lt;- sy$KerasHook(tf$keras)&lt;/code&gt; right after we import Syft.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# serve.R
# you could start R on the console and &amp;quot;source&amp;quot; this file

# do this just once
reticulate::py_install(&amp;quot;syft[udacity]&amp;quot;)

library(tensorflow)
library(keras)

sy &amp;lt;- reticulate::import((&amp;quot;syft&amp;quot;))
hook &amp;lt;- sy$KerasHook(tf$keras)

batch_input_shape &amp;lt;- c(1, 28, 28, 1)

model &amp;lt;- keras_model_sequential() %&amp;gt;%
 layer_conv_2d(filters = 16, kernel_size = c(3, 3), batch_input_shape = batch_input_shape) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_flatten() %&amp;gt;%
 layer_dense(units = 10) 
 
pre_trained_weights &amp;lt;- &amp;quot;model.hdf5&amp;quot;
model$load_weights(pre_trained_weights)

# create and start TFE cluster
AUTO &amp;lt;- TRUE
julia &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4000&amp;#39;, auto_managed = AUTO)
greg &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4001&amp;#39;, auto_managed = AUTO)
me &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4002&amp;#39;, auto_managed = AUTO)
cluster &amp;lt;- sy$TFECluster(julia, greg, me)
cluster$start()

# split up model weights into shares 
model$share(cluster)

# serve model (limiting number of requests)
model$serve(num_requests = 3L)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the desired number of requests have been served, we can go to this R process, stop model sharing, and shut down the cluster:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# stop model sharing
model$stop()

# stop cluster
cluster$stop()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, on to the client(s).&lt;/p&gt;
&lt;h4 id="request-predictions-on-private-data"&gt;Request predictions on private data&lt;/h4&gt;
&lt;p&gt;In our example, we have one client. The client is a TFE worker, just like the agents that make up the cluster.&lt;/p&gt;
&lt;p&gt;We define the cluster here, client-side, as well; create the client; and connect the client to the model. This will set up a queueing server that takes care of &lt;em&gt;secret sharing&lt;/em&gt; all input data before submitting them for prediction.&lt;/p&gt;
&lt;p&gt;Finally, we have the client asking for classification of the first three MNIST images.&lt;/p&gt;
&lt;p&gt;With the server running in some different R process, we can conveniently run this in RStudio:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# client.R

library(tensorflow)
library(keras)

sy &amp;lt;- reticulate::import((&amp;quot;syft&amp;quot;))
hook &amp;lt;- sy$KerasHook(tf$keras)

mnist &amp;lt;- dataset_mnist()
mnist$train$x &amp;lt;- mnist$train$x/255
mnist$test$x &amp;lt;- mnist$test$x/255

dim(mnist$train$x) &amp;lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &amp;lt;- c(dim(mnist$test$x), 1)

batch_input_shape &amp;lt;- c(1, 28, 28, 1)
batch_output_shape &amp;lt;- c(1, 10)

# define the same TFE cluster
AUTO &amp;lt;- TRUE
julia &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4000&amp;#39;, auto_managed = AUTO)
greg &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4001&amp;#39;, auto_managed = AUTO)
me &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4002&amp;#39;, auto_managed = AUTO)
cluster &amp;lt;- sy$TFECluster(julia, greg, me)

# create the client
client &amp;lt;- sy$TFEWorker()

# create a queueing server on the client that secret shares the data 
# before submitting a prediction request
client$connect_to_model(batch_input_shape, batch_output_shape, cluster)

num_tests &amp;lt;- 3
images &amp;lt;- mnist$test$x[1: num_tests, , , , drop = FALSE]
expected_labels &amp;lt;- mnist$test$y[1: num_tests]

for (i in 1:num_tests) {
  res &amp;lt;- client$query_model(images[i, , , , drop = FALSE])
  predicted_label &amp;lt;- which.max(res) - 1
  cat(&amp;quot;Actual: &amp;quot;, expected_labels[i], &amp;quot;, predicted: &amp;quot;, predicted_label)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Actual:  7 , predicted:  7 
Actual:  2 , predicted:  2 
Actual:  1 , predicted:  1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go. Both model and data did remain secret, yet we were able to classify our data.&lt;/p&gt;
&lt;p&gt;Let’s wrap up.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Our example use case has not been too ambitious – we started with a trained model, thus leaving aside federated learning. Keeping the setup simple, we were able to focus on underlying principles: &lt;em&gt;Secret sharing&lt;/em&gt; as a means of encryption, and setting up a Syft/TFE cluster of workers that together, provide the infrastructure for encrypting model weights as well as client data.&lt;/p&gt;
&lt;p&gt;In case you’ve read our previous post on &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/"&gt;TensorFlow Federated&lt;/a&gt; – that, too, a framework under development – you may have gotten an impression similar to the one I got: Setting up Syft was a lot more straightforward, concepts were easy to grasp, and surprisingly little code was required. As we may gather from a &lt;a href="https://blog.openmined.org/introducing-pysyft-tensorflow/"&gt;recent blog post&lt;/a&gt;, integration of Syft with TensorFlow Federated and TensorFlow Privacy are on the roadmap. I am looking forward &lt;em&gt;a lot&lt;/em&gt; for this to happen.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id="section"&gt;&lt;/h2&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-dwork2006differential"&gt;
&lt;p&gt;Dwork, Cynthia. 2006. “Differential Privacy.” In &lt;em&gt;33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006)&lt;/em&gt;, 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. &lt;a href="https://www.microsoft.com/en-us/research/publication/differential-privacy/"&gt;https://www.microsoft.com/en-us/research/publication/differential-privacy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Dwork2006"&gt;
&lt;p&gt;Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In &lt;em&gt;Proceedings of the Third Conference on Theory of Cryptography&lt;/em&gt;, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. &lt;a href="https://doi.org/10.1007/11681878_14"&gt;https://doi.org/10.1007/11681878_14&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-McMahanMRA16"&gt;
&lt;p&gt;McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” &lt;em&gt;CoRR&lt;/em&gt; abs/1602.05629. &lt;a href="http://arxiv.org/abs/1602.05629"&gt;http://arxiv.org/abs/1602.05629&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">dbe1974c9c6e217e2558943db1540a10</distill:md5>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>


&lt;p&gt;Behold the glory that is &lt;a href="https://sparklyr.ai"&gt;sparklyr&lt;/a&gt; 1.2! In this release, the following new hotnesses have emerged into spotlight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;registerDoSpark&lt;/code&gt; method to create a &lt;a href="#foreach"&gt;foreach&lt;/a&gt; parallel backend powered by Spark that enables hundreds of existing R packages to run in Spark.&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="#databricks-connect"&gt;Databricks Connect&lt;/a&gt;, allowing &lt;code&gt;sparklyr&lt;/code&gt; to connect to remote Databricks clusters.&lt;/li&gt;
&lt;li&gt;Improved support for Spark &lt;a href="#structures"&gt;structures&lt;/a&gt; when collecting and querying their nested attributes with &lt;code&gt;dplyr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of inter-op issues observed with &lt;code&gt;sparklyr&lt;/code&gt; and Spark 3.0 preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, &lt;code&gt;sparklyr&lt;/code&gt; will be fully ready to work with it. Most notably, key features such as &lt;code&gt;spark_submit&lt;/code&gt;, &lt;code&gt;sdf_bind_rows&lt;/code&gt;, and standalone connections are now finally working with Spark 3.0 preview.&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;sparklyr&lt;/code&gt; 1.2 from CRAN run,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full list of changes are available in the sparklyr &lt;a href="https://github.com/sparklyr/sparklyr/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;h2 id="foreach"&gt;Foreach&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package provides the &lt;code&gt;%dopar%&lt;/code&gt; operator to iterate over elements in a collection in parallel. Using &lt;code&gt;sparklyr&lt;/code&gt; 1.2, you can now register Spark as a backend using &lt;code&gt;registerDoSpark()&lt;/code&gt; and then easily iterate over R objects using Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
library(foreach)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4&amp;quot;)

registerDoSpark(sc)
foreach(i = 1:3, .combine = &amp;#39;c&amp;#39;) %dopar% {
  sqrt(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.000000 1.414214 1.732051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since many R packages are based on &lt;code&gt;foreach&lt;/code&gt; to perform parallel computation, we can now make use of all those great packages in Spark as well!&lt;/p&gt;
&lt;p&gt;For instance, we can use &lt;a href="https://tidymodels.github.io/parsnip/"&gt;parsnip&lt;/a&gt; and the &lt;a href="https://tidymodels.github.io/tune/"&gt;tune&lt;/a&gt; package with data from &lt;a href="https://CRAN.R-project.org/package=mlbench"&gt;mlbench&lt;/a&gt; to perform hyperparameter tuning in Spark with ease:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tune)
library(parsnip)
library(mlbench)

data(Ionosphere)
svm_rbf(cost = tune(), rbf_sigma = tune()) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;kernlab&amp;quot;) %&amp;gt;%
  tune_grid(Class ~ .,
    resamples = rsample::bootstraps(dplyr::select(Ionosphere, -V2), times = 30),
    control = control_grid(verbose = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Bootstrap sampling
# A tibble: 30 x 4
   splits            id          .metrics          .notes
 * &amp;lt;list&amp;gt;            &amp;lt;chr&amp;gt;       &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;
 1 &amp;lt;split [351/124]&amp;gt; Bootstrap01 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 2 &amp;lt;split [351/126]&amp;gt; Bootstrap02 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 3 &amp;lt;split [351/125]&amp;gt; Bootstrap03 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 4 &amp;lt;split [351/135]&amp;gt; Bootstrap04 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 5 &amp;lt;split [351/127]&amp;gt; Bootstrap05 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 6 &amp;lt;split [351/131]&amp;gt; Bootstrap06 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 7 &amp;lt;split [351/141]&amp;gt; Bootstrap07 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 8 &amp;lt;split [351/123]&amp;gt; Bootstrap08 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 9 &amp;lt;split [351/118]&amp;gt; Bootstrap09 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
10 &amp;lt;split [351/136]&amp;gt; Bootstrap10 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
# … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Spark connection was already registered, so the code ran in Spark without any additional changes. We can verify this was the case by navigating to the Spark web interface:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-backend-foreach-package.png" /&gt;&lt;/p&gt;
&lt;h2 id="databricks-connect"&gt;Databricks Connect&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.databricks.com/dev-tools/databricks-connect.html"&gt;Databricks Connect&lt;/a&gt; allows you to connect your favorite IDE (like &lt;a href="https://rstudio.com/products/rstudio/download/"&gt;RStudio&lt;/a&gt;!) to a Spark &lt;a href="https://databricks.com/"&gt;Databricks&lt;/a&gt; cluster.&lt;/p&gt;
&lt;p&gt;You will first have to install the &lt;code&gt;databricks-connect&lt;/code&gt; package as described in our &lt;a href="https://github.com/sparklyr/sparklyr#connecting-through-databricks-connect"&gt;README&lt;/a&gt; and start a Databricks cluster, but once that’s ready, connecting to the remote cluster is as easy as running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sc &amp;lt;- spark_connect(
  method = &amp;quot;databricks&amp;quot;,
  spark_home = system2(&amp;quot;databricks-connect&amp;quot;, &amp;quot;get-spark-home&amp;quot;, stdout = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-databricks-connect-rstudio.png" /&gt;&lt;/p&gt;
&lt;p&gt;That’s about it, you are now remotely connected to a Databricks cluster from your local R session.&lt;/p&gt;
&lt;h2 id="structures"&gt;Structures&lt;/h2&gt;
&lt;p&gt;If you previously used &lt;code&gt;collect&lt;/code&gt; to deserialize structurally complex Spark dataframes into their equivalents in R, you likely have noticed Spark SQL struct columns were only mapped into JSON strings in R, which was non-ideal. You might also have run into a much dreaded &lt;code&gt;java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt; error when using &lt;code&gt;dplyr&lt;/code&gt; to query nested attributes from any struct column of a Spark dataframe in sparklyr.&lt;/p&gt;
&lt;p&gt;Unfortunately, often times in real-world Spark use cases, data describing entities comprising of sub-entities (e.g., a product catalog of all hardware components of some computers) needs to be denormalized / shaped in an object-oriented manner in the form of Spark SQL structs to allow efficient read queries. When sparklyr had the limitations mentioned above, users often had to invent their own workarounds when querying Spark struct columns, which explained why there was a mass popular demand for sparklyr to have better support for such use cases.&lt;/p&gt;
&lt;p&gt;The good news is with &lt;code&gt;sparklyr&lt;/code&gt; 1.2, those limitations no longer exist any more when working running with Spark 2.4 or above.&lt;/p&gt;
&lt;p&gt;As a concrete example, consider the following catalog of computers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

computers &amp;lt;- tibble::tibble(
  id = seq(1, 2),
  attributes = list(
    list(
      processor = list(freq = 2.4, num_cores = 256),
      price = 100
   ),
   list(
     processor = list(freq = 1.6, num_cores = 512),
     price = 133
   )
  )
)

computers &amp;lt;- copy_to(sc, computers, overwrite = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A typical &lt;code&gt;dplyr&lt;/code&gt; use case involving &lt;code&gt;computers&lt;/code&gt; would be the following:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;high_freq_computers &amp;lt;- computers %&amp;gt;%
                       filter(attributes.processor.freq &amp;gt;= 2) %&amp;gt;%
                       collect()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As previously mentioned, before &lt;code&gt;sparklyr&lt;/code&gt; 1.2, such query would fail with &lt;code&gt;Error: java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Whereas with &lt;code&gt;sparklyr&lt;/code&gt; 1.2, the expected result is returned in the following form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 2
     id attributes
  &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;
1     1 &amp;lt;named list [2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;high_freq_computers$attributes&lt;/code&gt; is what we would expect:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[[1]]$price
[1] 100

[[1]]$processor
[[1]]$processor$freq
[1] 2.4

[[1]]$processor$num_cores
[1] 256&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="and-more"&gt;And More!&lt;/h2&gt;
&lt;p&gt;Last but not least, we heard about a number of pain points &lt;code&gt;sparklyr&lt;/code&gt; users have run into, and have addressed many of them in this release as well. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date type in R is now correctly serialized into Spark SQL date type by &lt;code&gt;copy_to&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;spark dataframe&amp;gt; %&amp;gt;% print(n = 20)&lt;/code&gt; now actually prints 20 rows as expected instead of 10&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spark_connect(master = "local")&lt;/code&gt; will emit a more informative error message if it’s failing because the loopback interface is not up&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;… to just name a few. We want to thank the open source community for their continuous feedback on &lt;code&gt;sparklyr&lt;/code&gt;, and are looking forward to incorporating more of that feedback to make &lt;code&gt;sparklyr&lt;/code&gt; even better in the future.&lt;/p&gt;
&lt;p&gt;Finally, in chronological order, we wish to thank the following individuals for contributing to &lt;code&gt;sparklyr&lt;/code&gt; 1.2: &lt;a href="https://github.com/zero323"&gt;zero323&lt;/a&gt;, &lt;a href="https://github.com/Loquats"&gt;Andy Zhang&lt;/a&gt;, &lt;a href="https://github.com/yl790"&gt;Yitao Li&lt;/a&gt;, &lt;a href="https://github.com/javierluraschi"&gt;Javier Luraschi&lt;/a&gt;, &lt;a href="https://github.com/falaki"&gt;Hossein Falaki&lt;/a&gt;, &lt;a href="https://github.com/lu-wang-dl"&gt;Lu Wang&lt;/a&gt;, &lt;a href="https://github.com/samuelmacedo83"&gt;Samuel Macedo&lt;/a&gt; and &lt;a href="https://github.com/jozefhajnala"&gt;Jozef Hajnala&lt;/a&gt;. Great job everyone!&lt;/p&gt;
&lt;p&gt;If you need to catch up on &lt;code&gt;sparklyr&lt;/code&gt;, please visit &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, or some of the previous release posts: &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/"&gt;sparklyr 1.1&lt;/a&gt; and &lt;a href="https://blog.rstudio.com/2019/03/15/sparklyr-1-0/"&gt;sparklyr 1.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this post.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">10308072097bf27ae84c0456eefaa91d</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>


&lt;p&gt;A new version of &lt;code&gt;pins&lt;/code&gt; is available on CRAN today, which adds support for &lt;a href="http://pins.rstudio.com/articles/advanced-versions.html"&gt;versioning&lt;/a&gt; your datasets and &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean Spaces&lt;/a&gt; boards!&lt;/p&gt;
&lt;p&gt;As a quick recap, the pins package allows you to cache, discover and share resources. You can use &lt;code&gt;pins&lt;/code&gt; in a wide range of situations, from downloading a dataset from a URL to creating complex automation workflows (learn more at &lt;a href="https://pins.rstudio.com"&gt;pins.rstudio.com&lt;/a&gt;). You can also use &lt;code&gt;pins&lt;/code&gt; in combination with TensorFlow and Keras; for instance, use &lt;a href="https://tensorflow.rstudio.com/tools/cloudml"&gt;cloudml&lt;/a&gt; to train models in cloud GPUs, but rather than manually copying files into the GPU instance, you can store them as pins directly from R.&lt;/p&gt;
&lt;p&gt;To install this new version of &lt;code&gt;pins&lt;/code&gt; from CRAN, simply run:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;pins&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find a detailed list of improvements in the pins &lt;a href="https://github.com/rstudio/pins/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;h1 id="versioning"&gt;Versioning&lt;/h1&gt;
&lt;p&gt;To illustrate the new versioning functionality, let’s start by downloading and caching a remote dataset with pins. For this example, we will download the weather in London, this happens to be in JSON format and requires &lt;code&gt;jsonlite&lt;/code&gt; to be parsed:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)

weather_url &amp;lt;- &amp;quot;https://samples.openweathermap.org/data/2.5/weather?q=London,uk&amp;amp;appid=b6907d289e10d714a6e88b30761fae22&amp;quot;

pin(weather_url, &amp;quot;weather&amp;quot;) %&amp;gt;%
  jsonlite::read_json() %&amp;gt;%
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  coord.lon coord.lat weather.id weather.main     weather.description weather.icon
1     -0.13     51.51        300      Drizzle light intensity drizzle          09d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One advantage of using &lt;code&gt;pins&lt;/code&gt; is that, even if the URL or your internet connection becomes unavailable, the above code will still work.&lt;/p&gt;
&lt;p&gt;But back to &lt;code&gt;pins 0.4&lt;/code&gt;! The new &lt;code&gt;signature&lt;/code&gt; parameter in &lt;code&gt;pin_info()&lt;/code&gt; allows you to retrieve the “version” of this dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_info(&amp;quot;weather&amp;quot;, signature = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: local&amp;lt;weather&amp;gt; [files]
# Signature: 624cca260666c6f090b93c37fd76878e3a12a79b
# Properties:
#   - path: weather&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then validate the remote dataset has not changed by specifying its signature:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin(weather_url, &amp;quot;weather&amp;quot;, signature = &amp;quot;624cca260666c6f090b93c37fd76878e3a12a79b&amp;quot;) %&amp;gt;%
  jsonlite::read_json()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the remote dataset changes, &lt;code&gt;pin()&lt;/code&gt; will fail and you can take the appropriate steps to accept the changes by updating the signature or properly updating your code. The previous example is useful as a way of detecting version changes, but we might also want to retrieve specific versions even when the dataset changes.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pins 0.4&lt;/code&gt; allows you to display and retrieve versions from services like GitHub, Kaggle and RStudio Connect. Even in boards that don’t support versioning natively, you can opt-in by registering a board with &lt;code&gt;versions = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To keep this simple, let’s focus on GitHub first. We will register a GitHub board and pin a dataset to it. Notice that you can also specify the &lt;code&gt;commit&lt;/code&gt; parameter in GitHub boards as the commit message for this change.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register_github(repo = &amp;quot;javierluraschi/datasets&amp;quot;, branch = &amp;quot;datasets&amp;quot;)

pin(iris, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;, commit = &amp;quot;use iris as the main dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now suppose that a colleague comes along and updates this dataset as well:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin(mtcars, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;, commit = &amp;quot;slight preference to mtcars&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From now on, your code could be broken or, even worse, produce incorrect results!&lt;/p&gt;
&lt;p&gt;However, since GitHub was designed as a version control system and &lt;code&gt;pins 0.4&lt;/code&gt; adds support for &lt;code&gt;pin_versions()&lt;/code&gt;, we can now explore particular versions of this dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_versions(&amp;quot;versioned&amp;quot;, board = &amp;quot;github&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 4
  version created              author         message                     
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                       
1 6e6c320 2020-04-02T21:28:07Z javierluraschi slight preference to mtcars 
2 01f8ddf 2020-04-02T21:27:59Z javierluraschi use iris as the main dataset&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then retrieve the version you are interested in as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pin_get(&amp;quot;versioned&amp;quot;, version = &amp;quot;01f8ddf&amp;quot;, board = &amp;quot;github&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 150 x 5
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# … with 140 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can follow similar steps for &lt;a href="http://pins.rstudio.com/articles/boards-rsconnect.html"&gt;RStudio Connect&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-kaggle.html"&gt;Kaggle&lt;/a&gt; boards, even for existing pins! Other boards like &lt;a href="http://pins.rstudio.com/articles/boards-s3.html"&gt;Amazon S3&lt;/a&gt;, &lt;a href="http://pins.rstudio.com/articles/boards-gcloud.html"&gt;Google Cloud&lt;/a&gt;, &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;Digital Ocean&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-azure.html"&gt;Microsoft Azure&lt;/a&gt; require you explicitly enable versioning when registering your boards.&lt;/p&gt;
&lt;h1 id="digitalocean"&gt;DigitalOcean&lt;/h1&gt;
&lt;p&gt;To try out the new &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean Spaces board&lt;/a&gt;, first you will have to register this board and enable versioning by setting &lt;code&gt;versions&lt;/code&gt; to &lt;code&gt;TRUE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)
board_register_dospace(space = &amp;quot;pinstest&amp;quot;,
                       key = &amp;quot;AAAAAAAAAAAAAAAAAAAA&amp;quot;,
                       secret = &amp;quot;ABCABCABCABCABCABCABCABCABCABCABCABCABCA==&amp;quot;,
                       datacenter = &amp;quot;sfo2&amp;quot;,
                       versions = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use all the functionality pins provides, including versioning:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create pin and replace content in digitalocean
pin(iris, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)
pin(mtcars, name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)

# retrieve versions from digitalocean
pin_versions(name = &amp;quot;versioned&amp;quot;, board = &amp;quot;pinstest&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 1
  version
  &amp;lt;chr&amp;gt;  
1 c35da04
2 d9034cd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that enabling versions in cloud services requires additional storage space for each version of the dataset being stored:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-13-pins-04/images/digitalocean-spaces-pins-versioned.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;To learn more visit the &lt;a href="http://pins.rstudio.com/articles/advanced-versions.html"&gt;Versioning&lt;/a&gt; and &lt;a href="http://pins.rstudio.com/articles/boards-dospace.html"&gt;DigitalOcean&lt;/a&gt; articles. To catch up with previous releases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pins.rstudio.com/blog/posts/pins-0-3-0/"&gt;pins 0.3&lt;/a&gt;: Azure, GCloud and S3&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.rstudio.com/2019/09/09/pin-discover-and-share-resources/"&gt;pins 0.2&lt;/a&gt;: Pin, Discover and Share Resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading along!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">0237a491d92720994e751dc6fdfc1a55</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
