<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Naming and locating objects in images</title>

<meta property="description" itemprop="description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2018-11-05"/>
<meta property="article:created" itemprop="dateCreated" content="2018-11-05"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Naming and locating objects in images"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/images/preds_train.jpg"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Naming and locating objects in images"/>
<meta property="twitter:description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/images/preds_train.jpg"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Naming and locating objects in images"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2018/11/05"/>
<meta name="citation_publication_date" content="2018/11/05"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Naming and locating objects in images"]},{"type":"character","attributes":{},"value":["Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2018naminglocatingobjects"]},{"type":"character","attributes":{},"value":["11-05-2018"]},{"type":"character","attributes":{},"value":["TensorFlow/Keras","Image Recognition & Image Processing"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/preds_train.jpg"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/bicycle.jpeg","images/preds_train_2.jpg","images/preds_train.jpg","images/preds_valid_2.jpg","images/preds_valid.jpg","naming-locating-objects_files/bowser-1.9.3/bowser.min.js","naming-locating-objects_files/distill-2.2.21/template.v2.js","naming-locating-objects_files/jquery-1.11.3/jquery.min.js","naming-locating-objects_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createIndex() {
  var options = {
    keys: [
      "title",
      "categories",
      "description",
      "contents"
    ]
  };
  return new window.Fuse([],options);
}

function createFuseIndex() {

  // create fuse index
  var options = { keys: ["title", "description", "contents"] };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
            keys: [
              { name: 'title', weight: 20 },
              { name: 'categories', weight: 15 },
              { name: 'description', weight: 10 },
              { name: 'contents', weight: 5 },
            ],
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
            .filter(function(item) { return !!item.description; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description}
                </div>
                <div class="search-item-preview">
                  <img src="${suggestion.preview ? offsetURL(suggestion.preview) : ''}"</img>
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: hidden;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Naming and locating objects in images","description":"Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2018-11-05T00:00:00.000+00:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Naming and locating objects in images</h1>
<p><p>Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We’ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>11-05-2018
</div>

<div class="d-article">
<p>We’ve all become used to deep learning’s success in image classification. <em>Greater Swiss Mountain dog</em> or <em>Bernese mountain dog</em>? <em>Red panda</em> or <em>giant panda</em>? No problem. However, in real life it’s not enough to name the single most salient object on a picture. Like it or not, one of the most compelling examples is autonomous driving: We don’t want the algorithm to recognize just that car in front of us, but also the pedestrian about to cross the street. And, just detecting the pedestrian is not sufficient. The exact <em>location</em> of objects matters.</p>
<p>The term <em>object detection</em> is commonly used to refer to the task of naming and localizing multiple objects in an image frame. Object detection is difficult; we’ll build up to it in a loose series of posts, focusing on concepts instead of aiming for ultimate performance. Today, we’ll start with a few straightforward building blocks: Classification, both single and multiple; localization; and combining both classification and localization of a single object.</p>
<aside>
The structure and approaches of these posts will follow the excellent <a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb">fast.ai notebook on object detection</a>.
</aside>
<h2 id="dataset">Dataset</h2>
<p>We’ll be using images and annotations from the <em>Pascal VOC dataset</em> which can be downloaded from <a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">this mirror</a>. Specifically, we’ll use data from the 2007 challenge and the same JSON annotation file as used in the <em>fast.ai</em> course.</p>
<p>Quick download/organization instructions, shamelessly taken from a <a href="https://forums.fast.ai/t/quick-google-colab-setup-for-part-2-week-1-along-with-pascal-voc-dataset/13650">helpful post on the fast.ai wiki</a>, are as follows:</p>
<pre><code>
# mkdir data &amp;&amp; cd data
# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar
# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip
# tar -xf VOCtrainval_06-Nov-2007.tar
# unzip PASCAL_VOC.zip
# mv PASCAL_VOC/*.json .
# rmdir PASCAL_VOC
# tar -xvf VOCtrainval_06-Nov-2007.tar</code></pre>
<p>In words, we take the images and the annotation file from different places:</p>
<ul>
<li><a href="http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar">http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</a> provides us with the images, and after unzipping all we care about is the <code>JPEGImages</code> folder.</li>
<li>From <a href="https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip">https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip</a> all we will be needing is the annotation file, <code>pascal_train2007.json</code>.</li>
</ul>
<p>Whether you’re executing the listed commands or arranging files manually, you should eventually end up with directories/files analogous to these:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
img_dir &lt;- &quot;data/VOCdevkit/VOC2007/JPEGImages&quot;
annot_file &lt;- &quot;data/pascal_train2007.json&quot;</code></pre>
</div>
<p>Now we need to extract some information from that <em>json</em> file.</p>
<h2 id="preprocessing">Preprocessing</h2>
<p>Let’s quickly make sure we have all required libraries loaded.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)</code></pre>
</div>
<p>Annotations contain information about three types of things we’re interested in.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
annotations &lt;- fromJSON(file = annot_file)
str(annotations, max.level = 1)</code></pre>
</div>
<pre><code>
List of 4
 $ images     :List of 2501
 $ type       : chr &quot;instances&quot;
 $ annotations:List of 7844
 $ categories :List of 20</code></pre>
<p>First, characteristics of the image itself (height and width) and where it’s stored. Not surprisingly, here it’s one entry per image.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- annotations$images %&gt;% {
  tibble(
    id = map_dbl(., &quot;id&quot;),
    file_name = map_chr(., &quot;file_name&quot;),
    image_height = map_dbl(., &quot;height&quot;),
    image_width = map_dbl(., &quot;width&quot;)
  )
}</code></pre>
</div>
<p>Then, object class ids and bounding box coordinates. There may be multiple of these per image. In Pascal VOC, there are 20 object classes, from ubiquitous vehicles (<code>car</code>, <code>aeroplane</code>) over indispensable animals (<code>cat</code>, <code>sheep</code>) to more rare (in popular datasets) types like <code>potted plant</code> or <code>tv monitor</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
classes &lt;- c(
  &quot;aeroplane&quot;,
  &quot;bicycle&quot;,
  &quot;bird&quot;,
  &quot;boat&quot;,
  &quot;bottle&quot;,
  &quot;bus&quot;,
  &quot;car&quot;,
  &quot;cat&quot;,
  &quot;chair&quot;,
  &quot;cow&quot;,
  &quot;diningtable&quot;,
  &quot;dog&quot;,
  &quot;horse&quot;,
  &quot;motorbike&quot;,
  &quot;person&quot;,
  &quot;pottedplant&quot;,
  &quot;sheep&quot;,
  &quot;sofa&quot;,
  &quot;train&quot;,
  &quot;tvmonitor&quot;
)

boxinfo &lt;- annotations$annotations %&gt;% {
  tibble(
    image_id = map_dbl(., &quot;image_id&quot;),
    category_id = map_dbl(., &quot;category_id&quot;),
    bbox = map(., &quot;bbox&quot;)
  )
}</code></pre>
</div>
<p>The bounding boxes are now stored in a list column and need to be unpacked.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
boxinfo &lt;- boxinfo %&gt;% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = &quot; &quot;))))
boxinfo &lt;- boxinfo %&gt;% 
  separate(bbox, into = c(&quot;x_left&quot;, &quot;y_top&quot;, &quot;bbox_width&quot;, &quot;bbox_height&quot;))
boxinfo &lt;- boxinfo %&gt;% mutate_all(as.numeric)</code></pre>
</div>
<p>For the bounding boxes, the annotation file provides <code>x_left</code> and <code>y_top</code> coordinates, as well as width and height. We will mostly be working with corner coordinates, so we create the missing <code>x_right</code> and <code>y_bottom</code>.</p>
<p>As usual in image processing, the <code>y</code> axis starts from the top.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
boxinfo &lt;- boxinfo %&gt;% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)</code></pre>
</div>
<p>Finally, we still need to match class ids to class names.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
catinfo &lt;- annotations$categories %&gt;%  {
  tibble(id = map_dbl(., &quot;id&quot;), name = map_chr(., &quot;name&quot;))
}</code></pre>
</div>
<p>So, putting it all together:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- imageinfo %&gt;%
  inner_join(boxinfo, by = c(&quot;id&quot; = &quot;image_id&quot;)) %&gt;%
  inner_join(catinfo, by = c(&quot;category_id&quot; = &quot;id&quot;))</code></pre>
</div>
<p>Note that here still, we have several entries per image, each annotated object occupying its own row.</p>
<p>There’s one step that will bitterly hurt our localization performance if we later forget it, so let’s do it now already: We need to scale all bounding box coordinates according to the actual image size we’ll use when we pass it to our network.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
target_height &lt;- 224
target_width &lt;- 224

imageinfo &lt;- imageinfo %&gt;% mutate(
  x_left_scaled = (x_left / image_width * target_width) %&gt;% round(),
  x_right_scaled = (x_right / image_width * target_width) %&gt;% round(),
  y_top_scaled = (y_top / image_height * target_height) %&gt;% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %&gt;% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %&gt;% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %&gt;% round()
)</code></pre>
</div>
<p>Let’s take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
img_data &lt;- imageinfo[4,]
img &lt;- image_read(file.path(img_dir, img_data$file_name))
img &lt;- image_draw(img)
rect(
  img_data$x_left,
  img_data$y_bottom,
  img_data$x_right,
  img_data$y_top,
  border = &quot;white&quot;,
  lwd = 2
)
text(
  img_data$x_left,
  img_data$y_top,
  img_data$name,
  offset = 1,
  pos = 2,
  cex = 1.5,
  col = &quot;white&quot;
)
dev.off()</code></pre>
</div>
<p><img src="images/bicycle.jpeg" style="width:80.0%" /></p>
<p>Now as indicated above, in this post we’ll mostly address handling a single object in an image. This means we have to decide, per image, which object to single out.</p>
<p>A reasonable strategy seems to be choosing the object with the largest ground truth bounding box.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- imageinfo %&gt;% mutate(area = bbox_width_scaled * bbox_height_scaled)

imageinfo_maxbb &lt;- imageinfo %&gt;%
  group_by(id) %&gt;%
  filter(which.max(area) == row_number())</code></pre>
</div>
<p>After this operation, we only have 2501 images to work with - not many at all! For classification, we could simply use data augmentation as provided by Keras, but to work with localization we’d have to spin our own augmentation algorithm. We’ll leave this to a later occasion and for now, focus on the basics.</p>
<p>Finally after train-test split</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_indices &lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &lt;- imageinfo_maxbb[train_indices,]
validation_data &lt;- imageinfo_maxbb[-train_indices,]</code></pre>
</div>
<p>our training set consists of 2000 images with one annotation each. We’re ready to start training, and we’ll start gently, with single-object classification.</p>
<h2 id="single-object-classification">Single-object classification</h2>
<p>In all cases, we will use XCeption as a basic feature extractor. Having been trained on ImageNet, we don’t expect much fine tuning to be necessary to adapt to Pascal VOC, so we leave XCeption’s weights untouched</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &quot;avg&quot;
)

feature_extractor %&gt;% freeze_weights()</code></pre>
</div>
<p>and put just a few custom layers on top.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 20, activation = &quot;softmax&quot;)

model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = &quot;sparse_categorical_crossentropy&quot;,
  metrics = list(&quot;accuracy&quot;)
)</code></pre>
</div>
<p>How should we pass our data to Keras? We could simple use Keras’ <code>image_data_generator</code>, but given we will need custom generators soon, we’ll build a simple one ourselves. This one delivers images as well as the corresponding targets in a stream. Note how the targets are not one-hot-encoded, but integers - using <code>sparse_categorical_crossentropy</code> as a loss function enables this convenience.</p>
<aside>
See the <a href="https://tensorflow.rstudio.com/learn/resources.html">Deep learning with R</a> book for an introduction to writing data generators like this one.
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
batch_size &lt;- 10

load_and_preprocess_image &lt;- function(image_name, target_height, target_width) {
  img_array &lt;- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %&gt;%
    image_to_array() %&gt;%
    xception_preprocess_input() 
  dim(img_array) &lt;- c(1, dim(img_array))
  img_array
}

classification_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]],
                                    target_height, target_width)
        y[j, ] &lt;-
          data[[indices[j], &quot;category_id&quot;]] - 1
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>Now how does training go?</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;class_only&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>For us, after 8 epochs, accuracies on the train resp. validation sets were at 0.68 and 0.74, respectively. Not too bad given given we’re trying to differentiate between 20 classes here.</p>
<p>Now let’s quickly think what we’d change if we were to classify multiple objects in one image. Changes mostly concern preprocessing steps.</p>
<h2 id="multiple-object-classification">Multiple object classification</h2>
<p>This time, we multi-hot-encode our data. For every image (as represented by its filename), here we have a vector of length 20 where 0 indicates absence, 1 means presence of the respective object class:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
image_cats &lt;- imageinfo %&gt;% 
  select(category_id) %&gt;%
  mutate(category_id = category_id - 1) %&gt;%
  pull() %&gt;%
  to_categorical(num_classes = 20)

image_cats &lt;- data.frame(image_cats) %&gt;%
  add_column(file_name = imageinfo$file_name, .before = TRUE)

image_cats &lt;- image_cats %&gt;% 
  group_by(file_name) %&gt;% 
  summarise_all(.funs = funs(max))

n_samples &lt;- nrow(image_cats)
train_indices &lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &lt;- image_cats[train_indices,]
validation_data &lt;- image_cats[-train_indices,]</code></pre>
</div>
<p>Correspondingly, we modify the generator to return a target of dimensions <code>batch_size</code> * 20, instead of <code>batch_size</code> * 1.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
classification_generator &lt;- 
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 20))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y[j, ] &lt;-
          data[indices[j], 2:21] %&gt;% as.matrix()
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>Now, the most interesting change is to the model - even though it’s a change to two lines only. Were we to use <code>categorical_crossentropy</code> now (the non-sparse variant of the above), combined with a <code>softmax</code> activation, we would effectively tell the model to pick just one, namely, the most probable object.</p>
<aside>
See the <a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/">introduction to loss functions and activations</a> on this blog for a demonstration.
</aside>
<p>Instead, we want to decide: For each object class, is it present in the image or not? Thus, instead of <code>softmax</code> we use <code>sigmoid</code>, paired with <code>binary_crossentropy</code>, to obtain an independent verdict on every class.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &quot;avg&quot;
  )

feature_extractor %&gt;% freeze_weights()

model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 20, activation = &quot;sigmoid&quot;)

model %&gt;% compile(optimizer = &quot;adam&quot;,
                  loss = &quot;binary_crossentropy&quot;,
                  metrics = list(&quot;accuracy&quot;))</code></pre>
</div>
<p>And finally, again, we fit the model:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;multiclass&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>This time, (binary) accuracy surpasses 0.95 after one epoch already, on both the train and validation sets. Not surprisingly, accuracy is significantly higher here than when we had to single out one of 20 classes (and that, with other confounding objects present in most cases!).</p>
<p>Now, chances are that if you’ve done any deep learning before, you’ve done image classification in some form, perhaps even in the multiple-object variant. To build up in the direction of object detection, it is time we add a new ingredient: localization.</p>
<h2 id="single-object-localization">Single-object localization</h2>
<p>From here on, we’re back to dealing with a single object per image. So the question now is, how do we learn bounding boxes? If you’ve never heard of this, the answer will sound unbelievably simple (naive even): We formulate this as a regression problem and aim to predict the actual coordinates. To set realistic expectations - we surely shouldn’t expect ultimate precision here. But in a way it’s amazing it does even work at all.</p>
<p>What does this mean, formulate as a regression problem? Concretely, it means we’ll have a <code>dense</code> output layer with 4 units, each corresponding to a corner coordinate.</p>
<p>So let’s start with the model this time. Again, we use Xception, but there’s an important difference here: Whereas before, we said <code>pooling = "avg"</code> to obtain an output tensor of dimensions <code>batch_size</code> * number of filters, here we don’t do any averaging or flattening out of the spatial grid. This is because it’s exactly the spatial information we’re interested in!</p>
<p>For Xception, the output resolution will be 7x7. So a priori, we shouldn’t expect high precision on objects much smaller than about 32x32 pixels (assuming the standard input size of 224x224).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

feature_extractor %&gt;% freeze_weights()</code></pre>
</div>
<p>Now we append our custom regression module.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_flatten() %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 4)</code></pre>
</div>
<p>We will train with one of the loss functions common in regression tasks, mean absolute error. But in tasks like object detection or segmentation, we’re also interested in a more tangible quantity: How much do estimate and ground truth overlap?</p>
<p>Overlap is usually measured as <em>Intersection over Union</em>, or <em>Jaccard distance</em>. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.</p>
<p>To assess the model’s progress, we can easily code this as a custom metric:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
metric_iou &lt;- function(y_true, y_pred) {
  
  # order is [x_left, y_top, x_right, y_bottom]
  intersection_xmin &lt;- k_maximum(y_true[ ,1], y_pred[ ,1])
  intersection_ymin &lt;- k_maximum(y_true[ ,2], y_pred[ ,2])
  intersection_xmax &lt;- k_minimum(y_true[ ,3], y_pred[ ,3])
  intersection_ymax &lt;- k_minimum(y_true[ ,4], y_pred[ ,4])
  
  area_intersection &lt;- (intersection_xmax - intersection_xmin) * 
                       (intersection_ymax - intersection_ymin)
  area_y &lt;- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])
  area_yhat &lt;- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])
  area_union &lt;- area_y + area_yhat - area_intersection
  
  iou &lt;- area_intersection/area_union
  k_mean(iou)
  
}</code></pre>
</div>
<p>Model compilation then goes like</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = &quot;mae&quot;,
  metrics = list(custom_metric(&quot;iou&quot;, metric_iou))
)</code></pre>
</div>
<p>Now modify the generator to return bounding box coordinates as targets…</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
localization_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y[j, ] &lt;-
          data[indices[j], c(&quot;x_left_scaled&quot;,
                             &quot;y_top_scaled&quot;,
                             &quot;x_right_scaled&quot;,
                             &quot;y_bottom_scaled&quot;)] %&gt;% as.matrix()
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- localization_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- localization_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>… and we’re ready to go!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;loc_only&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>After 8 epochs, IOU on both training and test sets is around 0.35. This number doesn’t look too good. To learn more about how training went, we need to see some predictions. Here’s a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot_image_with_boxes &lt;- function(file_name,
                                  object_class,
                                  box,
                                  scaled = FALSE,
                                  class_pred = NULL,
                                  box_pred = NULL) {
  img &lt;- image_read(file.path(img_dir, file_name))
  if(scaled) img &lt;- image_resize(img, geometry = &quot;224x224!&quot;)
  img &lt;- image_draw(img)
  x_left &lt;- box[1]
  y_bottom &lt;- box[2]
  x_right &lt;- box[3]
  y_top &lt;- box[4]
  rect(
    x_left,
    y_bottom,
    x_right,
    y_top,
    border = &quot;cyan&quot;,
    lwd = 2.5
  )
  text(
    x_left,
    y_top,
    object_class,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = &quot;cyan&quot;
  )
  if (!is.null(box_pred))
    rect(box_pred[1],
         box_pred[2],
         box_pred[3],
         box_pred[4],
         border = &quot;yellow&quot;,
         lwd = 2.5)
  if (!is.null(class_pred))
    text(
      box_pred[1],
      box_pred[2],
      class_pred,
      offset = 0,
      pos = 4,
      cex = 1.5,
      col = &quot;yellow&quot;)
  dev.off()
  img %&gt;% image_write(paste0(&quot;preds_&quot;, file_name))
  plot(img)
}</code></pre>
</div>
<p>First, let’s see predictions on sample images from the training set.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_1_8 &lt;- train_data[1:8, c(&quot;file_name&quot;,
                               &quot;name&quot;,
                               &quot;x_left_scaled&quot;,
                               &quot;y_top_scaled&quot;,
                               &quot;x_right_scaled&quot;,
                               &quot;y_bottom_scaled&quot;)]

for (i in 1:8) {
  preds &lt;-
    model %&gt;% predict(
      load_and_preprocess_image(train_1_8[i, &quot;file_name&quot;], 
                                target_height, target_width),
      batch_size = 1
  )
  plot_image_with_boxes(train_1_8$file_name[i],
                        train_1_8$name[i],
                        train_1_8[i, 3:6] %&gt;% as.matrix(),
                        scaled = TRUE,
                        box_pred = preds)
}</code></pre>
</div>
<figure>
<img src="images/preds_train.jpg" style="width:100.0%" alt="" /><figcaption>Sample bounding box predictions on the training set.</figcaption>
</figure>
<p>As you’d guess from looking, the cyan-colored boxes are the ground truth ones. Now looking at the predictions explains a lot about the mediocre IOU values! Let’s take the very first sample image - we wanted the model to focus on the sofa, but it picked the table, which is also a category in the dataset (although in the form of <em>dining</em> <em>table</em>). Similar with the image on the right of the first row - we wanted to it to pick just the dog but it included the person, too (by far the most frequently seen category in the dataset). So we actually made the task a lot more difficult than had we stayed with e.g., ImageNet where normally a single object is salient.</p>
<p>Now check predictions on the validation set.</p>
<figure>
<img src="images/preds_valid.jpg" style="width:100.0%" alt="" /><figcaption>Some bounding box predictions on the validation set.</figcaption>
</figure>
<p>Again, we get a similar impression: The model <em>did</em> learn something, but the task is ill defined. Look at the third image in row 2: Isn’t it pretty consequent the model picks <em>all</em> people instead of singling out some special guy?</p>
<p>If single-object localization is that straightforward, how technically involved can it be to output a class label at the same time? As long as we stay with a single object, the answer indeed is: not much.</p>
<aside>
As a caveat, please note we’re talking about mapping concepts to technical approaches here. Obtaining ultimate performance is a different thing.
</aside>
<p>Let’s finish up today with a constrained combination of classification and localization: detection of a single object.</p>
<h2 id="single-object-detection">Single-object detection</h2>
<p>Combining regression and classification into one means we’ll want to have two outputs in our model. We’ll thus use the functional API this time. Otherwise, there isn’t much new here: We start with an XCeption output of spatial resolution 7x7, append some custom processing and return two outputs, one for bounding box regression and one for classification.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

input &lt;- feature_extractor$input
common &lt;- feature_extractor$output %&gt;%
  layer_flatten(name = &quot;flatten&quot;) %&gt;%
  layer_activation_relu() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5)

regression_output &lt;-
  layer_dense(common, units = 4, name = &quot;regression_output&quot;)
class_output &lt;- layer_dense(
  common,
  units = 20,
  activation = &quot;softmax&quot;,
  name = &quot;class_output&quot;
)

model &lt;- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)</code></pre>
</div>
<p>When defining the losses (mean absolute error and categorical crossentropy, just as in the respective single tasks of regression and classification), we could weight them so they end up on approximately a common scale. In fact that didn’t make much of a difference so we show the respective code in commented form.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% freeze_weights(to = &quot;flatten&quot;)

model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = list(&quot;mae&quot;, &quot;sparse_categorical_crossentropy&quot;),
  #loss_weights = list(
  #  regression_output = 0.05,
  #  class_output = 0.95),
  metrics = list(
    regression_output = custom_metric(&quot;iou&quot;, metric_iou),
    class_output = &quot;accuracy&quot;
  )
)</code></pre>
</div>
<p>Just like model outputs and losses are both lists, the data generator has to return the ground truth samples in a list. Fitting the model then goes as usual.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
loc_class_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y1 &lt;- array(0, dim = c(length(indices), 4))
      y2 &lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y1[j, ] &lt;-
          data[indices[j], c(&quot;x_left&quot;, &quot;y_top&quot;, &quot;x_right&quot;, &quot;y_bottom&quot;)] 
            %&gt;% as.matrix()
        y2[j, ] &lt;-
          data[[indices[j], &quot;category_id&quot;]] - 1
      }
      x &lt;- x / 255
      list(x, list(y1, y2))
    }
  }

train_gen &lt;- loc_class_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- loc_class_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)

model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;loc_class&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>What about model predictions? A priori we might expect the bounding boxes to look better than in the regression-only model, as a significant part of the model is shared between classification and localization. Intuitively, I should be able to more precisely indicate the boundaries of <em>something</em> if I have an idea what that <em>something</em> is.</p>
<p>Unfortunately, that didn’t quite happen. The model has become <em>very</em> biased to detecting a <em>person</em> everywhere, which might be advantageous (thinking safety) in an autonomous driving application but isn’t quite what we’d hoped for here.</p>
<figure>
<img src="images/preds_train_2.jpg" style="width:100.0%" alt="" /><figcaption>Example class and bounding box predictions on the training set.</figcaption>
</figure>
<figure>
<img src="images/preds_valid_2.jpg" style="width:100.0%" alt="" /><figcaption>Example class and bounding box predictions on the validation set.</figcaption>
</figure>
<p>Just to double-check this really has to do with class imbalance, here are the actual frequencies:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo %&gt;% group_by(name)
  %&gt;% summarise(cnt = n()) 
  %&gt;% arrange(desc(cnt))</code></pre>
</div>
<pre><code>
# A tibble: 20 x 2
   name          cnt
   &lt;chr&gt;       &lt;int&gt;
 1 person       2705
 2 car           826
 3 chair         726
 4 bottle        338
 5 pottedplant   305
 6 bird          294
 7 dog           271
 8 sofa          218
 9 boat          208
10 horse         207
11 bicycle       202
12 motorbike     193
13 cat           191
14 sheep         191
15 tvmonitor     191
16 cow           185
17 train         158
18 aeroplane     156
19 diningtable   148
20 bus           131</code></pre>
<p>To get better performance, we’d need to find a successful way to deal with this. However, handling class imbalance in deep learning is a topic of its own, and here we want to build up in the direction of objection detection. So we’ll make a cut here and in an upcoming post, think about how we can classify and localize multiple objects in an image.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We have seen that single-object classification and localization are conceptually straightforward. The big question now is, are these approaches extensible to multiple objects? Or will new ideas have to come in? We’ll follow up on this giving a short overview of approaches and then, singling in on one of those and implementing it.</p>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2018-11-05-naming-locating-objects/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Naming%20and%20locating%20objects%20in%20images&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-11-05-naming-locating-objects%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-11-05-naming-locating-objects%2F&amp;title=Naming%20and%20locating%20objects%20in%20images">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/';
  this.page.identifier = 'posts/2018-11-05-naming-locating-objects/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    document.querySelector("a[href='#category:R']").parentNode.style.display = "None";
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerText == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2018, Nov. 5). RStudio AI Blog: Naming and locating objects in images. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydana2018naminglocatingobjects,
  author = {Keydana, Sigrid},
  title = {RStudio AI Blog: Naming and locating objects in images},
  url = {https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/},
  year = {2018}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
