<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Differential Privacy with TensorFlow</title>
  
  <meta property="description" itemprop="description" content="Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-12-20"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-12-20"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Differential Privacy with TensorFlow"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Differential Privacy with TensorFlow"/>
  <meta property="twitter:description" content="Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Deep ppg: Large-scale heart rate estimation with convolutional neural networks;citation_publication_date=2019;citation_volume=19;citation_doi=10.3390/s19143079;citation_author=Attila Reiss;citation_author=Ina Indlekofer;citation_author=Philip Schmidt;citation_author=Kristof Van Laerhoven"/>
  <meta name="citation_reference" content="citation_title=Photoplethysmography and its application in clinical physiological measurement;citation_publication_date=2007;citation_publisher=IOP Publishing;citation_volume=28;citation_doi=10.1088/0967-3334/28/3/r01;citation_author=John Allen"/>
  <meta name="citation_reference" content="citation_title=Differential privacy: A primer for a non-technical audience;citation_publication_date=2018;citation_doi=10.2139/ssrn.3338027;citation_author=Alexandra Wood;citation_author=Micah Altman;citation_author=Aaron Bembenek;citation_author=Mark Bun;citation_author=Marco Gaboardi;citation_author=James Honaker;citation_author=Kobbi Nissim;citation_author=David O’Brien;citation_author=Thomas Steinke;citation_author=Salil Vadhan"/>
  <meta name="citation_reference" content="citation_title=Differential privacy;citation_publication_date=2006;citation_publisher=Springer Verlag;citation_volume=4052;citation_author=Cynthia Dwork"/>
  <meta name="citation_reference" content="citation_title=The algorithmic foundations of differential privacy;citation_publication_date=2014;citation_publisher=Now Publishers Inc.;citation_volume=9;citation_doi=10.1561/0400000042;citation_issn=1551-305X;citation_author=Cynthia Dwork;citation_author=Aaron Roth"/>
  <meta name="citation_reference" content="citation_title=Calibrating noise to sensitivity in private data analysis;citation_publication_date=2006;citation_publisher=Springer-Verlag;citation_doi=10.1007/11681878_14;citation_author=Cynthia Dwork;citation_author=Frank McSherry;citation_author=Kobbi Nissim;citation_author=Adam Smith"/>
  <meta name="citation_reference" content="citation_title=Deep learning with differential privacy;citation_publication_date=2016;citation_author=Martin Abadi;citation_author=Andy Chu;citation_author=Ian Goodfellow;citation_author=Brendan McMahan;citation_author=Ilya Mironov;citation_author=Kunal Talwar;citation_author=Li Zhang"/>
  <meta name="citation_reference" content="citation_title=A general approach to adding differential privacy to iterative training procedures;citation_publication_date=2018;citation_volume=abs/1812.06210;citation_author=H. Brendan McMahan;citation_author=Galen Andrew"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Differential Privacy with TensorFlow"]},{"type":"character","attributes":{},"value":["Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2019diffpriv"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["12-20-2019"]},{"type":"character","attributes":{},"value":["Privacy & Security","TensorFlow/Keras","Time Series"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/cat.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","differential_privacy_files/bowser-1.9.3/bowser.min.js","differential_privacy_files/distill-2.2.21/template.v2.js","differential_privacy_files/jquery-1.11.3/jquery.min.js","differential_privacy_files/webcomponents-2.0.0/webcomponents.js","images/baseline.png","images/cat.jpg","images/cat.png","images/dp.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="differential_privacy_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="differential_privacy_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="differential_privacy_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="differential_privacy_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Differential Privacy with TensorFlow","description":"Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2019-12-20T00:00:00.000+01:00","citationText":"Keydana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Differential Privacy with TensorFlow</h1>
<p><p>Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>12-20-2019
</div>

<div class="d-article">
<p>What could be treacherous about summary statistics?</p>
<p>The famous <em>cat overweight</em> study (X. et al., 2019) showed that as of May 1st, 2019, 32 of 101 domestic cats held in Y., a cozy Bavarian village, were overweight. Even though I’d be curious to know if my aunt G.’s cat (a happy resident of that village) has been fed too many treats and has accumulated some excess pounds, the study results don’t tell.</p>
<p>Then, six months later, out comes a new study, ambitious to earn scientific fame. The authors report that of 100 cats living in Y., 50 are striped, 31 are black, and the rest are white; the 31 black ones are all overweight. Now, I happen to know that, with one exception, no new cats joined the community, and no cats left. <em>But</em>, my aunt moved away to a retirement home, chosen of course for the possibility to bring one’s cat.</p>
<p>What have I just learned? My aunt’s cat is overweight. (Or was, at least, before they moved to the retirement home.)</p>
<p>Even though none of the studies reported anything but summary statistics, I was able to infer individual-level facts by connecting both studies and adding in another piece of information I had access to.</p>
<p>In reality, mechanisms like the above – technically called <em>linkage</em> – have been shown to lead to privacy breaches many times, thus defeating the purpose of <em>database anonymization</em> seen as a panacea in many organizations. A more promising alternative is offered by the concept of <em>differential privacy</em>.</p>
<h2 id="differential-privacy">Differential Privacy</h2>
<p>In differential privacy (DP)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><span class="citation" data-cites="Dwork2006">(Dwork et al. <a href="#ref-Dwork2006" role="doc-biblioref">2006</a>)</span>, privacy is not a property of what’s in the database; it’s a property of how query results are delivered.</p>
<p>Intuitively paraphrasing results from a domain where results are communicated as theorems and proofs <span class="citation" data-cites="dwork2006differential">(Dwork <a href="#ref-dwork2006differential" role="doc-biblioref">2006</a>)</span><span class="citation" data-cites="Dwork">(Dwork and Roth <a href="#ref-Dwork" role="doc-biblioref">2014</a>)</span>, the only achievable (in a lossy but quantifiable way) objective is that from queries to a database, nothing more should be learned about an individual in that database than if they hadn’t been in there at all.<span class="citation" data-cites="primer">(Wood et al. <a href="#ref-primer" role="doc-biblioref">2018</a>)</span></p>
<p>What this statement does is caution against overly high expectations: Even if query results are reported in a DP way (we’ll see how that goes in a second), they enable some probabilistic inferences about individuals in the respective population. (Otherwise, why conduct studies at all.)</p>
<p>So how is DP being achieved? The main ingredient is <em>noise</em> added to the results of a query. In the above cat example, instead of exact numbers we’d report approximate ones: “Of ~ 100 cats living in Y, about 30 are overweight…”. If this is done for both of the above studies, no inference will be possible about aunt G.’s cat.</p>
<p>Even with random noise added to query results though, answers to <em>repeated</em> queries will leak information. So in reality, there is a <em>privacy budget</em> that can be tracked, and may be used up in the course of consecutive queries.</p>
<p>This is reflected in the formal definition of DP. The idea is that queries to two databases differing in at most one element should give basically the same result. Put formally <span class="citation" data-cites="dwork2006differential">(Dwork <a href="#ref-dwork2006differential" role="doc-biblioref">2006</a>)</span>:</p>
<blockquote>
<p>A randomized function <span class="math inline">\(\mathcal{K}\)</span> gives <span class="math inline">\(\epsilon\)</span> -differential privacy if for all data sets D1 and D2 differing on at most one element, and all <span class="math inline">\(S \subseteq Range(K)\)</span>,</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(Pr[\mathcal{K}(D1)\in S] \leq exp(\epsilon) × Pr[K(D2) \in S]\)</span></p>
</blockquote>
<p>This <span class="math inline">\(\epsilon\)</span> -differential privacy is additive: If one query is <span class="math inline">\(\epsilon\)</span>-DP at a value of 0.01, and another one at 0.03, together they will be 0.04 <span class="math inline">\(\epsilon\)</span>-differentially private.</p>
<p>If <span class="math inline">\(\epsilon\)</span>-DP is to be achieved via adding noise, how exactly should this be done? Here, several mechanisms exist; the basic, intuitively plausible principle though is that the amount of noise should be calibrated to the target function’s <em>sensitivity</em>, defined as the maximum <span class="math inline">\(\ell 1\)</span> norm of the difference of function values computed on all pairs of datasets differing in a single example <span class="citation" data-cites="dwork2006differential">(Dwork <a href="#ref-dwork2006differential" role="doc-biblioref">2006</a>)</span>:</p>
<blockquote>
<p><span class="math inline">\(\Delta f = \max_{D1,D2} {\| f(D1)−f(D2) \|}_1\)</span></p>
</blockquote>
<p>So far, we’ve been talking about databases and datasets. How does this apply to machine and/or deep learning?</p>
<h2 id="tensorflow-privacy">TensorFlow Privacy</h2>
<p>Applying DP to deep learning, we want a model’s parameters to wind up “essentially the same” whether trained on a dataset including that cute little kitty or not. TensorFlow (TF) Privacy <span class="citation" data-cites="abaditfp">(Abadi et al. <a href="#ref-abaditfp" role="doc-biblioref">2016</a>)</span>, a library built on top of TF, makes it easy on users to add privacy guarantees to their models – easy, that is, from a technical point of view. (As with life overall, the hard decisions on <em>how much</em> of an asset we should be reaching for, and how to trade off one asset (here: privacy) with another (here: model performance), remain to be taken by each of us ourselves.)</p>
<p>Concretely, about all we have to do is exchange the optimizer we were using against one provided by TF Privacy.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> TF Privacy optimizers wrap the original TF ones, adding two actions:</p>
<ol type="1">
<li><p>To honor the principle that each individual training example should have just moderate influence on optimization, gradients are <em>clipped</em> (to a degree specifiable by the user). In contrast to the familiar gradient clipping sometimes used to prevent exploding gradients, what is clipped here is gradient contribution <em>per user</em>.</p></li>
<li><p>Before updating the parameters, noise is added to the gradients, thus implementing the main idea of <span class="math inline">\(\epsilon\)</span>-DP algorithms.</p></li>
</ol>
<p>In addition to <span class="math inline">\(\epsilon\)</span>-DP optimization, TF Privacy provides <em>privacy accounting</em>. We’ll see all this applied after an introduction to our example dataset.</p>
<h2 id="dataset">Dataset</h2>
<p>The dataset we’ll be working with<span class="citation" data-cites="ReissISL19">(Reiss et al. <a href="#ref-ReissISL19" role="doc-biblioref">2019</a>)</span>, downloadable from the <a href="https://archive.ics.uci.edu/ml/datasets/PPG-DaLiA">UCI Machine Learning Repository</a>, is dedicated to heart rate estimation via <a href="https://en.wikipedia.org/wiki/Photoplethysmogram">photoplethysmography</a>. Photoplethysmography (PPG) is an optical method of measuring blood volume changes in the microvascular bed of tissue, which are indicative of cardiovascular activity. More precisely,</p>
<blockquote>
<p>The PPG waveform comprises a pulsatile (‘AC’) physiological waveform attributed to cardiac synchronous changes in the blood volume with each heart beat, and is superimposed on a slowly varying (‘DC’) baseline with various lower frequency components attributed to respiration, sympathetic nervous system activity and thermoregulation. <span class="citation" data-cites="Allen_2007">(Allen <a href="#ref-Allen_2007" role="doc-biblioref">2007</a>)</span></p>
</blockquote>
<p>In this dataset, heart rate determined from EKG provides the ground truth; predictors were obtained from two commercial devices, comprising PPG, electrodermal activity, body temperature as well as accelerometer data. Additionally, a wealth of contextual data is available, ranging from age, height, and weight to fitness level and type of activity performed.</p>
<p>With this data, it’s easy to imagine a bunch of interesting data-analysis questions; however here our focus is on differential privacy, so we’ll keep the setup simple. We will try to predict heart rate given the physiological measurements from one of the two devices, Empatica E4. Also, we’ll zoom in on a single subject, <em>S1</em>, who will provide us with 4603 instances of two-second heart rate values.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>As usual, we start with the required libraries; unusually though, as of this writing we need to disable version 2 behavior in TensorFlow, as TensorFlow Privacy does not yet fully work with TF 2. (Hopefully, for many future readers, this won’t be the case anymore.) Note how TF Privacy – a Python library – is imported via <code>reticulate</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tensorflow)
tf$compat$v1$disable_v2_behavior()

library(keras)
library(tfdatasets)
library(tfautograph)

library(purrr)

library(reticulate)
# if you haven&#39;t yet, install TF Privacy, e.g. using reticulate:
# py_install(&quot;tensorflow_privacy&quot;)
priv &lt;- import(&quot;tensorflow_privacy&quot;)</code></pre>
</div>
<p>From the downloaded archive, we just need <code>S1.pkl</code>, saved in a <a href="https://docs.python.org/3.8/library/pickle.html">native Python serialization format</a>, yet nicely loadable using <code>reticulate</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
s1 &lt;- py_load_object(&quot;PPG_FieldStudy/S1/S1.pkl&quot;, encoding = &quot;latin1&quot;)</code></pre>
</div>
<p><code>s1</code> points to an R list comprising elements of different length – the various physical/physiological signals have been sampled with different frequencies:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
### predictors ###

# accelerometer data - sampling freq. 32 Hz
# also note that these are 3 &quot;columns&quot;, for each of x, y, and z axes
s1$signal$wrist$ACC %&gt;% nrow() # 294784
# PPG data - sampling freq. 64 Hz
s1$signal$wrist$BVP %&gt;% nrow() # 589568
# electrodermal activity data - sampling freq. 4 Hz
s1$signal$wrist$EDA %&gt;% nrow() # 36848
# body temperature data - sampling freq. 4 Hz
s1$signal$wrist$TEMP %&gt;% nrow() # 36848

### target ###

# EKG data - provided in already averaged form, at frequency 0.5 Hz
s1$label %&gt;% nrow() # 4603</code></pre>
</div>
<p>In light of the different sampling frequencies, our <code>tfdatasets</code> pipeline will have do some moving averaging, paralleling that applied to construct the ground truth data.</p>
<h2 id="preprocessing-pipeline">Preprocessing pipeline</h2>
<p>As every “column”<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> is of different length and resolution, we build up the final <em>dataset</em> piece-by-piece. The following function serves two purposes:</p>
<ol type="1">
<li>compute running averages over differently sized windows, thus downsampling to 0.5Hz for every modality</li>
<li>transform the data to the <code>(num_timesteps, num_features)</code> format that will be required by the 1d-convnet we’re going to use soon</li>
</ol>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
average_and_make_sequences &lt;-
  function(data, window_size_avg, num_timesteps) {
    data %&gt;% k_cast(&quot;float32&quot;) %&gt;%
      # create an initial tf.data dataset to work with
      tensor_slices_dataset() %&gt;%
      # use dataset_window to compute the running average of size window_size_avg
      dataset_window(window_size_avg) %&gt;%
      dataset_flat_map(function (x)
        x$batch(as.integer(window_size_avg), drop_remainder = TRUE)) %&gt;%
      dataset_map(function(x)
        tf$reduce_mean(x, axis = 0L)) %&gt;%
      # use dataset_window to create a &quot;timesteps&quot; dimension with length num_timesteps)
      dataset_window(num_timesteps, shift = 1) %&gt;%
      dataset_flat_map(function(x)
        x$batch(as.integer(num_timesteps), drop_remainder = TRUE))
  }</code></pre>
</div>
<p>We’ll call this function for every column separately. Not all columns are exactly the same length (in terms of time), thus it’s safest to cut off individual observations that surpass a common length (dictated by the target variable):</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
label &lt;- s1$label %&gt;% matrix() # 4603 observations, each spanning 2 secs
n_total &lt;- 4603 # keep track of this

# keep matching numbers of observations of predictors
acc &lt;- s1$signal$wrist$ACC[1:(n_total * 64), ] # 32 Hz, 3 columns
bvp &lt;- s1$signal$wrist$BVP[1:(n_total * 128)] %&gt;% matrix() # 64 Hz
eda &lt;- s1$signal$wrist$EDA[1:(n_total * 8)] %&gt;% matrix() # 4 Hz
temp &lt;- s1$signal$wrist$TEMP[1:(n_total * 8)] %&gt;% matrix() # 4 Hz</code></pre>
</div>
<p>Some more housekeeping. Both training and the test set need to have a <code>timesteps</code> dimension, as usual with architectures that work on sequential data (1-d convnets and RNNs). To make sure there is no overlap between respective <code>timesteps</code>, we split the data “up front” and assemble both sets separately. We’ll use the first 4000 observations for training.</p>
<p>Housekeeping-wise, we also keep track of actual training and test set cardinalities. The target variable will be matched to the last of any twelve timesteps, so we end up throwing away the first eleven ground truth measurements for each of the training and test datasets. (We don’t have complete sequences building up to them.)</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# number of timesteps used in the second dimension
num_timesteps &lt;- 12

# number of observations to be used for the training set
# a round number for easier checking!
train_max &lt;- 4000

# also keep track of actual number of training and test observations
n_train &lt;- train_max - num_timesteps + 1
n_test &lt;- n_total - train_max - num_timesteps + 1</code></pre>
</div>
<p>Here, then, are the basic building blocks that will go into the final training and test datasets.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
acc_train &lt;-
  average_and_make_sequences(acc[1:(train_max * 64), ], 64, num_timesteps)
bvp_train &lt;-
  average_and_make_sequences(bvp[1:(train_max * 128), , drop = FALSE], 128, num_timesteps)
eda_train &lt;-
  average_and_make_sequences(eda[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)
temp_train &lt;-
  average_and_make_sequences(temp[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)


acc_test &lt;-
  average_and_make_sequences(acc[(train_max * 64 + 1):nrow(acc), ], 64, num_timesteps)
bvp_test &lt;-
  average_and_make_sequences(bvp[(train_max * 128 + 1):nrow(bvp), , drop = FALSE], 128, num_timesteps)
eda_test &lt;-
  average_and_make_sequences(eda[(train_max * 8 + 1):nrow(eda), , drop = FALSE], 8, num_timesteps)
temp_test &lt;-
  average_and_make_sequences(temp[(train_max * 8 + 1):nrow(temp), , drop = FALSE], 8, num_timesteps)</code></pre>
</div>
<p>Now put all predictors together:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# all predictors
x_train &lt;- zip_datasets(acc_train, bvp_train, eda_train, temp_train) %&gt;%
  dataset_map(function(...)
    tf$concat(list(...), axis = 1L))

x_test &lt;- zip_datasets(acc_test, bvp_test, eda_test, temp_test) %&gt;%
  dataset_map(function(...)
    tf$concat(list(...), axis = 1L))</code></pre>
</div>
<p>On the ground truth side, as alluded to before, we leave out the first eleven values in each case:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
y_train &lt;- tensor_slices_dataset(label[num_timesteps:train_max] %&gt;% k_cast(&quot;float32&quot;))

y_test &lt;- tensor_slices_dataset(label[(train_max + num_timesteps):nrow(label)] %&gt;% k_cast(&quot;float32&quot;)</code></pre>
</div>
<p>Zip predictors and targets together, configure shuffling/batching, and the datasets are complete:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ds_train &lt;- zip_datasets(x_train, y_train)
ds_test &lt;- zip_datasets(x_test, y_test)

batch_size &lt;- 32

ds_train &lt;- ds_train %&gt;% 
  dataset_shuffle(n_train) %&gt;%
  # dataset_repeat is needed because of pre-TF 2 style
  # hopefully at a later time, the code can run eagerly and this is no longer needed
  dataset_repeat() %&gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)

ds_test &lt;- ds_test %&gt;%
  # see above reg. dataset_repeat
  dataset_repeat() %&gt;%
  dataset_batch(batch_size)</code></pre>
</div>
<p>With data manipulations as complicated as the above, it’s always worthwhile checking some pipeline outputs. We can do that using the usual <code>reticulate::as_iterator</code> magic, provided that for this test run, we <em>don’t</em> disable V2 behavior. (Just restart the R session between a “pipeline checking” and the later modeling runs.)</p>
<p>Here, in any case, would be the relevant code:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# this piece needs TF 2 behavior enabled
# run after restarting R and commenting the tf$compat$v1$disable_v2_behavior() line
# then to fit the DP model, undo comment, restart R and rerun
iter &lt;- as_iterator(ds_test) # or any other dataset you want to check
while (TRUE) {
 item &lt;- iter_next(iter)
 if (is.null(item)) break
 print(item)
}</code></pre>
</div>
<p>With that we’re ready to create the model.</p>
<h2 id="model">Model</h2>
<p>The model will be a rather simple convnet. The main difference between standard and DP training lies in the optimization procedure; thus, it’s straightforward to first establish a non-DP baseline. Later, when switching to DP, we’ll be able to reuse almost everything.</p>
<p>Here, then, is the model definition valid for both cases:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  layer_conv_1d(
      filters = 32,
      kernel_size = 3,
      activation = &quot;relu&quot;
    ) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_conv_1d(
      filters = 64,
      kernel_size = 5,
      activation = &quot;relu&quot;
    ) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_conv_1d(
      filters = 128,
      kernel_size = 5,
      activation = &quot;relu&quot;
    ) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_global_average_pooling_1d() %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 1)</code></pre>
</div>
<p>We train the model with mean squared error loss.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
optimizer &lt;- optimizer_adam()
model %&gt;% compile(loss = &quot;mse&quot;, optimizer = optimizer, metrics = metric_mean_absolute_error)

num_epochs &lt;- 20
history &lt;- model %&gt;% fit(
  ds_train, 
  steps_per_epoch = n_train/batch_size,
  validation_data = ds_test,
  epochs = num_epochs,
  validation_steps = n_test/batch_size)</code></pre>
</div>
<h2 id="baseline-results">Baseline results</h2>
<p>After 20 epochs, mean absolute error is around 6 bpm:</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-14"></span>
<img src="images/baseline.png" alt="Training history without differential privacy." width="600" />
<p class="caption">
Figure 1: Training history without differential privacy.
</p>
</div>
</div>
<p>Just to put this in context, the MAE reported for subject S1 in the paper<span class="citation" data-cites="ReissISL19">(Reiss et al. <a href="#ref-ReissISL19" role="doc-biblioref">2019</a>)</span> – based on a higher-capacity network, extensive hyperparameter tuning, and naturally, training on the complete dataset – amounts to 8.45 bpm on average; so our setup seems to be sound.</p>
<p>Now we’ll make this differentially private.</p>
<h2 id="dp-training">DP training</h2>
<p>Instead of the plain <code>Adam</code> optimizer, we use the corresponding TF Privacy wrapper, <code>DPAdamGaussianOptimizer</code>.</p>
<p>We need to tell it how aggressive gradient clipping should be (<code>l2_norm_clip</code>) and how much noise to add (<code>noise_multiplier</code>). Furthermore, we define the learning rate (there is no default), going for 10 times the default <code>0.001</code> based on initial experiments.</p>
<p>There is an additional parameter, <code>num_microbatches</code>, that could be used to speed up training <span class="citation" data-cites="abs181206210">(McMahan and Andrew <a href="#ref-abs181206210" role="doc-biblioref">2018</a>)</span>, but, as training duration is not an issue here, we just set it equal to <code>batch_size</code>.</p>
<p>The values for <code>l2_norm_clip</code> and <code>noise_multiplier</code> chosen here follow those used in the <a href="https://github.com/tensorflow/privacy/tree/master/tutorials">tutorials in the TF Privacy repo</a>.</p>
<p>Nicely, TF Privacy comes with a script that allows one to compute the attained <span class="math inline">\(\epsilon\)</span> beforehand, based on number of training examples, <code>batch_size</code>, <code>noise_multiplier</code> and number of training epochs.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>Calling that <a href="https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy.py">script</a>, and assuming we train for 20 epochs here as well,</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
python compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=20</code></pre>
</div>
<p>this is what we get back:</p>
<pre><code>
DP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over
2494 steps satisfies differential privacy with eps = 2.73 and delta = 1e-06.</code></pre>
<p>How good is a value of 2.73? Citing the <a href="https://github.com/tensorflow/privacy/tree/master/tutorials">TF Privacy authors</a>:</p>
<blockquote>
<p><span class="math inline">\(\epsilon\)</span> gives a ceiling on how much the probability of a particular output can increase by including (or removing) a single training example. We usually want it to be a small constant (less than 10, or, for more stringent privacy guarantees, less than 1). However, this is only an upper bound, and a large value of epsilon may still mean good practical privacy.</p>
</blockquote>
<p>Obviously, choice of <span class="math inline">\(\epsilon\)</span> is a (challenging) topic unto itself, and not something we can elaborate on in a post dedicated to the technical aspects of DP with TensorFlow.</p>
<p>How would <span class="math inline">\(\epsilon\)</span> change if we trained for 50 epochs instead? (This is actually what we’ll do, seeing that training results on the test set tend to jump around quite a bit.)</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
python compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=60</code></pre>
</div>
<pre><code>
DP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over
6233 steps satisfies differential privacy with eps = 4.25 and delta = 1e-06.</code></pre>
<p>Having talked about its parameters, now let’s define the DP optimizer:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
l2_norm_clip &lt;- 1
noise_multiplier &lt;- 1.1
num_microbatches &lt;- k_cast(batch_size, &quot;int32&quot;)
learning_rate &lt;- 0.01

optimizer &lt;- priv$DPAdamGaussianOptimizer(
  l2_norm_clip = l2_norm_clip,
  noise_multiplier = noise_multiplier,
  num_microbatches = num_microbatches,
  learning_rate = learning_rate
)</code></pre>
</div>
<p>There is one other change to make for DP. As gradients are clipped on a per-sample basis, the optimizer needs to work with per-sample losses as well:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
loss &lt;- tf$keras$losses$MeanSquaredError(reduction =  tf$keras$losses$Reduction$NONE)</code></pre>
</div>
<p>Everything else stays the same. Training history (like we said above, lasting for 50 epochs now) looks a lot more turbulent, with MAEs on the test set fluctuating between 8 and 20 over the last 10 training epochs:</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="images/dp.png" alt="Training history with differential privacy." width="600" />
<p class="caption">
Figure 2: Training history with differential privacy.
</p>
</div>
</div>
<p>In addition to the above-mentioned command line script, we can also compute <span class="math inline">\(\epsilon\)</span> as part of the training code. Let’s double check:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# probability of an individual training point being included in a minibatch
sampling_probability &lt;- batch_size / n_train

# number of steps the optimizer takes over the training data
steps &lt;- num_epochs * n_train / batch_size

# required for reasons related to how TF Privacy computes privacy
# this actually is Renyi Differential Privacy: https://arxiv.org/abs/1702.07476
# we don&#39;t go into details here and use same values as the command line script
orders &lt;- c((1 + (1:99)/10), 12:63)

rdp &lt;- priv$privacy$analysis$rdp_accountant$compute_rdp(
  q = sampling_probability,
  noise_multiplier = noise_multiplier,
  steps = steps,
  orders = orders)

priv$privacy$analysis$rdp_accountant$get_privacy_spent(
  orders, rdp, target_delta = 1e-6)[[1]]</code></pre>
</div>
<pre><code>
[1] 4.249645</code></pre>
<p>So, we do get the same result.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This post showed how to convert a normal deep learning procedure into an <span class="math inline">\(\epsilon\)</span>-differentially private one. Necessarily, a blog post has to leave open questions. In the present case, some possible questions could be answered by straightforward experimentation:</p>
<ul>
<li>How well do other optimizers work in this setting?</li>
<li>How does the learning rate affect privacy and performance?</li>
<li>What happens if we train for a lot longer?</li>
</ul>
<p>Others sound more like they could lead to a research project:</p>
<ul>
<li>When model performance – and thus, model parameters – fluctuate that much, how do we decide on when to stop training? Is stopping at high model performance <em>cheating</em>? Is model averaging a sound solution?</li>
<li>How good really <em>is</em> any one <span class="math inline">\(\epsilon\)</span>?</li>
</ul>
<p>Finally, yet others transcend the realms of experimentation as well as mathematics:</p>
<ul>
<li>How do we trade off <span class="math inline">\(\epsilon\)</span>-DP against model performance – for different applications, with different types of data, in different societal contexts?</li>
<li>Assuming we “have” <span class="math inline">\(\epsilon\)</span>-DP, what might we still be missing?</li>
</ul>
<p>With questions like these – and more, probably – to ponder: Thanks for reading and a happy new year!</p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-abaditfp">
<p>Abadi, Martin, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In <em>23rd Acm Conference on Computer and Communications Security (Acm Ccs)</em>, 308–18. <a href="https://arxiv.org/abs/1607.00133">https://arxiv.org/abs/1607.00133</a>.</p>
</div>
<div id="ref-Allen_2007">
<p>Allen, John. 2007. “Photoplethysmography and Its Application in Clinical Physiological Measurement.” <em>Physiological Measurement</em> 28 (3): R1–R39. <a href="https://doi.org/10.1088/0967-3334/28/3/r01">https://doi.org/10.1088/0967-3334/28/3/r01</a>.</p>
</div>
<div id="ref-dwork2006differential">
<p>Dwork, Cynthia. 2006. “Differential Privacy.” In <em>33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006)</em>, 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. <a href="https://www.microsoft.com/en-us/research/publication/differential-privacy/">https://www.microsoft.com/en-us/research/publication/differential-privacy/</a>.</p>
</div>
<div id="ref-Dwork2006">
<p>Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In <em>Proceedings of the Third Conference on Theory of Cryptography</em>, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. <a href="https://doi.org/10.1007/11681878_14">https://doi.org/10.1007/11681878_14</a>.</p>
</div>
<div id="ref-Dwork">
<p>Dwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations of Differential Privacy.” <em>Found. Trends Theor. Comput. Sci.</em> 9 (3&amp;#8211;4): 211–407. <a href="https://doi.org/10.1561/0400000042">https://doi.org/10.1561/0400000042</a>.</p>
</div>
<div id="ref-abs181206210">
<p>McMahan, H. Brendan, and Galen Andrew. 2018. “A General Approach to Adding Differential Privacy to Iterative Training Procedures.” <em>CoRR</em> abs/1812.06210. <a href="http://arxiv.org/abs/1812.06210">http://arxiv.org/abs/1812.06210</a>.</p>
</div>
<div id="ref-ReissISL19">
<p>Reiss, Attila, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. 2019. “Deep Ppg: Large-Scale Heart Rate Estimation with Convolutional Neural Networks.” <em>Sensors</em> 19 (14): 3079. <a href="https://doi.org/10.3390/s19143079">https://doi.org/10.3390/s19143079</a>.</p>
</div>
<div id="ref-primer">
<p>Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” <em>SSRN Electronic Journal</em>, January. <a href="https://doi.org/10.2139/ssrn.3338027">https://doi.org/10.2139/ssrn.3338027</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>We’ll be using DP as an acronym for both the noun phrase “differential privacy” and the adjective phrase “differentially private”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>This is not <em>exactly</em> everything, as we’ll see when we get to the code, but “just about”.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Relative files per subject are 1.4G in size.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>For convenience, we take the liberty to talk as if we had the usual rectangular data here.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>There is an additional parameter, <span class="math inline">\(\delta\)</span>, that allows for bounding the risk that the privacy guarantee does not hold. The <a href="https://github.com/tensorflow/privacy/tree/master/tutorials">recommendation</a> is to set this to at most the inverse of the number of training examples, which in out case would mean &lt;= ~ <code>1e-04</code>; the default setting is <code>1e-06</code> so we should be fine here.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{ReissISL19,
  author    = {Attila Reiss and Ina Indlekofer and Philip Schmidt and Kristof Van Laerhoven},
  title     = {Deep PPG: Large-Scale Heart Rate Estimation with Convolutional Neural Networks},
  journal   = {Sensors},
  volume    = {19},
  number    = {14},
  pages     = {3079},
  year      = {2019},
  url       = {https://doi.org/10.3390/s19143079},
  doi       = {10.3390/s19143079},
  timestamp = {Thu, 05 Sep 2019 19:41:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/sensors/ReissISL19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Allen_2007,
doi = {10.1088/0967-3334/28/3/r01},
url = {https://doi.org/10.1088%2F0967-3334%2F28%2F3%2Fr01},
year = 2007,
month = {feb},
publisher = {{IOP} Publishing},
volume = {28},
number = {3},
pages = {R1--R39},
author = {John Allen},
title = {Photoplethysmography and its application in clinical physiological measurement},
journal = {Physiological Measurement}
}

@article{primer,
author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
year = {2018},
month = {01},
pages = {},
title = {Differential Privacy: A Primer for a Non-Technical Audience},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3338027}
}

@InProceedings{dwork2006differential,
author = {Dwork, Cynthia},
title = {Differential Privacy},
series = {Lecture Notes in Computer Science},
booktitle = {33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006)},
year = {2006},
month = {July},
abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius’ goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one’s privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
publisher = {Springer Verlag},
url = {https://www.microsoft.com/en-us/research/publication/differential-privacy/},
pages = {1-12},
volume = {4052},
isbn = {3-540-35907-9},
edition = {33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006)},
}

@article{Dwork,
 author = {Dwork, Cynthia and Roth, Aaron},
 title = {The Algorithmic Foundations of Differential Privacy},
 journal = {Found. Trends Theor. Comput. Sci.},
 issue_date = {August 2014},
 volume = {9},
 number = {3\&\#8211;4},
 month = aug,
 year = {2014},
 issn = {1551-305X},
 pages = {211--407},
 numpages = {197},
 url = {http://dx.doi.org/10.1561/0400000042},
 doi = {10.1561/0400000042},
 acmid = {2693053},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@inproceedings{Dwork2006,
 author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
 title = {Calibrating Noise to Sensitivity in Private Data Analysis},
 booktitle = {Proceedings of the Third Conference on Theory of Cryptography},
 series = {TCC'06},
 year = {2006},
 isbn = {3-540-32731-2, 978-3-540-32731-8},
 location = {New York, NY},
 pages = {265--284},
 numpages = {20},
 url = {http://dx.doi.org/10.1007/11681878_14},
 doi = {10.1007/11681878_14},
 acmid = {2180305},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@inproceedings{abaditfp,
title= {Deep Learning with Differential Privacy},
author= {Martin Abadi and Andy Chu and Ian Goodfellow and Brendan McMahan and Ilya Mironov and Kunal Talwar and Li Zhang},
year= {2016},
URL= {https://arxiv.org/abs/1607.00133},
booktitle= {23rd ACM Conference on Computer and Communications Security (ACM CCS)},
pages= {308-318}
}

@article{abs181206210,
  author    = {H. Brendan McMahan and Galen Andrew},
  title     = {A General Approach to Adding Differential Privacy to Iterative Training Procedures},
  journal   = {CoRR},
  volume    = {abs/1812.06210},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06210},
  archivePrefix = {arXiv},
  eprint    = {1812.06210}
}

</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
