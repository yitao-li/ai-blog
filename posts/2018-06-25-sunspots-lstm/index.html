<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Predicting Sunspot Frequency with Keras</title>

<meta property="description" itemprop="description" content="In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2018-06-25"/>
<meta property="article:created" itemprop="dateCreated" content="2018-06-25"/>
<meta name="article:author" content="Matt Dancho"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Predicting Sunspot Frequency with Keras"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/images/backtested_test.png"/>
<meta property="og:image:width" content="800"/>
<meta property="og:image:height" content="416"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Predicting Sunspot Frequency with Keras"/>
<meta property="twitter:description" content="In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/images/backtested_test.png"/>
<meta property="twitter:image:width" content="800"/>
<meta property="twitter:image:height" content="416"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Predicting Sunspot Frequency with Keras"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"/>
<meta name="citation_online_date" content="2018/06/25"/>
<meta name="citation_publication_date" content="2018/06/25"/>
<meta name="citation_author" content="Matt Dancho"/>
<meta name="citation_author_institution" content="Business Science"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","creative_commons","preview","categories","output","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Predicting Sunspot Frequency with Keras"]},{"type":"character","attributes":{},"value":["In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Matt Dancho"]},{"type":"character","attributes":{},"value":["https://github.com/mdancho84"]},{"type":"character","attributes":{},"value":["Business Science"]},{"type":"character","attributes":{},"value":["https://www.business-science.io/"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["https://github.com/skeydan"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com"]}]}]},{"type":"character","attributes":{},"value":["2018-06-25"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["images/backtested_test.png"]},{"type":"character","attributes":{},"value":["TensorFlow/Keras","Time Series"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/all_splits_zoomed.png","images/all_splits.png","images/backtested_test.png","images/backtested_train.png","images/cowplot.png","images/history.png","images/pred_test.png","images/pred_train.png","images/rnn.png","images/slice1.png","images/slice6.png","images/sunspot_nasa.jpg","images/sunspots_full.png","sunspots-lstm_files/bowser-1.9.3/bowser.min.js","sunspots-lstm_files/distill-2.2.21/template.v2.js","sunspots-lstm_files/jquery-1.11.3/jquery.min.js","sunspots-lstm_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createIndex() {
  var options = {
    keys: [
      "title",
      "categories",
      "description",
      "contents"
    ]
  };
  return new window.Fuse([],options);
}

function createFuseIndex() {

  // create fuse index
  var options = { keys: ["title", "description", "contents"] };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
            keys: [
              { name: 'title', weight: 20 },
              { name: 'categories', weight: 15 },
              { name: 'description', weight: 10 },
              { name: 'contents', weight: 5 },
            ],
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
            .filter(function(item) { return !!item.description; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description}
                </div>
                <div class="search-item-preview">
                  <img src="${suggestion.preview ? offsetURL(suggestion.preview) : ''}"</img>
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: hidden;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Predicting Sunspot Frequency with Keras","description":"In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.","authors":[{"author":"Matt Dancho","authorURL":"https://github.com/mdancho84","affiliation":"Business Science","affiliationURL":"https://www.business-science.io/","orcidID":""},{"author":"Sigrid Keydana","authorURL":"https://github.com/skeydan","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com","orcidID":""}],"publishedDate":"2018-06-25T00:00:00.000+00:00","citationText":"Dancho & Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Predicting Sunspot Frequency with Keras</h1>
<p><p>In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.</p></p>
</div>

<div class="d-byline">
  Matt Dancho <a href="https://github.com/mdancho84" class="uri">https://github.com/mdancho84</a> (Business Science)<a href="https://www.business-science.io/" class="uri">https://www.business-science.io/</a>
  
,   Sigrid Keydana <a href="https://github.com/skeydan" class="uri">https://github.com/skeydan</a> (RStudio)<a href="https://www.rstudio.com" class="uri">https://www.rstudio.com</a>
  
<br/>2018-06-25
</div>

<div class="d-article">
<h2 id="forecasting-sunspots-with-deep-learning">Forecasting sunspots with deep learning</h2>
<p>In this post we will examine making time series predictions using the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/sunspot.month.html">sunspots</a> dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Here’s an image from NASA showing the solar phenomenon.</p>
<figure>
<img src="images/sunspot_nasa.jpg" class="external" style="width:100.0%" alt="" /><figcaption>Figure from <a href="https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycle" class="uri">https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycle</a></figcaption>
</figure>
<p>We’re using the monthly version of the dataset, <code>sunspots.month</code> (there is a yearly version, too). It contains 265 years worth of data (from 1749 through 2013) on the number of sunspots per month.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/sunspots_full.png" width="352" /></p>
</div>
<p>Forecasting this dataset is challenging because of high short term variability as well as long-term irregularities evident in the cycles. For example, maximum amplitudes reached by the low frequency cycle differ a lot, as does the number of high frequency cycle steps needed to reach that maximum low frequency cycle height.</p>
<p>Our post will focus on two dominant aspects: how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain. For the latter, we will use the <a href="https://cran.r-project.org/package=rsample">rsample</a> package that allows to do resampling on time series data. As to the former, our goal is not to reach utmost performance but to show the general course of action when using recurrent neural networks to model this kind of data.</p>
<h2 id="recurrent-neural-networks">Recurrent neural networks</h2>
<p>When our data has a sequential structure, it is recurrent neural networks (RNNs) we use to model it.</p>
<p>As of today, among RNNs, the best established architectures are the GRU (Gated Recurrent Unit) and the LSTM (Long Short Term Memory). For today, let’s not zoom in on what makes them special, but on what they have in common with the most stripped-down RNN: the basic recurrence structure.</p>
<p>In contrast to the prototype of a neural network, often called Multilayer Perceptron (MLP), the RNN has a state that is carried on over time. This is nicely seen in this diagram from <a href="http://www.deeplearningbook.org">Goodfellow et al.</a>, a.k.a. the “bible of deep learning”:</p>
<figure>
<img src="images/rnn.png" class="external" alt="" /><figcaption>Figure from: <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a></figcaption>
</figure>
<p>At each time, the state is a combination of the current input and the previous hidden state. This is reminiscent of autoregressive models, but with neural networks, there has to be some point where we halt the dependence.</p>
<p>That’s because in order to determine the weights, we keep calculating how our loss changes as the input changes. Now if the input we have to consider, at an arbitrary timestep, ranges back indefinitely - then we will not be able to calculate all those gradients. In practice, then, our hidden state will, at every iteration, be carried forward through a fixed number of steps.</p>
<p>We’ll come back to that as soon as we’ve loaded and pre-processed the data.</p>
<h2 id="setup-pre-processing-and-exploration">Setup, pre-processing, and exploration</h2>
<h3 id="libraries">Libraries</h3>
<p>Here, first, are the libraries needed for this tutorial.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
library(tfruns)</code></pre>
</div>
<p>If you have not previously run Keras in R, you will need to install Keras using the <code>install_keras()</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# Install Keras if you have not installed before
install_keras()</code></pre>
</div>
<h3 id="data">Data</h3>
<p><code>sunspot.month</code> is a <code>ts</code> class (not tidy), so we’ll convert to a tidy data set using the <code>tk_tbl()</code> function from <code>timetk</code>. We use this instead of <code>as.tibble()</code> from <code>tibble</code> to automatically preserve the time series index as a <code>zoo</code> <code>yearmon</code> index. Last, we’ll convert the <code>zoo</code> index to date using <code>lubridate::as_date()</code> (loaded with <code>tidyquant</code>) and then change to a <code>tbl_time</code> object to make time series operations easier.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
sun_spots &lt;- datasets::sunspot.month %&gt;%
    tk_tbl() %&gt;%
    mutate(index = as_date(index)) %&gt;%
    as_tbl_time(index = index)

sun_spots</code></pre>
</div>
<pre><code>
# A time tibble: 3,177 x 2
# Index: index
   index      value
   &lt;date&gt;     &lt;dbl&gt;
 1 1749-01-01  58  
 2 1749-02-01  62.6
 3 1749-03-01  70  
 4 1749-04-01  55.7
 5 1749-05-01  85  
 6 1749-06-01  83.5
 7 1749-07-01  94.8
 8 1749-08-01  66.3
 9 1749-09-01  75.9
10 1749-10-01  75.5
# ... with 3,167 more rows</code></pre>
<h3 id="exploratory-data-analysis">Exploratory data analysis</h3>
<p>The time series is long (265 years!). We can visualize the time series both in full, and zoomed in on the first 10 years to get a feel for the series.</p>
<h4 id="visualizing-sunspot-data-with-cowplot">Visualizing sunspot data with cowplot</h4>
<p>We’ll make two <code>ggplot</code>s and combine them using <code>cowplot::plot_grid()</code>. Note that for the zoomed in plot, we make use of <code>tibbletime::time_filter()</code>, which is an easy way to perform time-based filtering.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
p1 &lt;- sun_spots %&gt;%
    ggplot(aes(index, value)) +
    geom_point(color = palette_light()[[1]], alpha = 0.5) +
    theme_tq() +
    labs(
        title = &quot;From 1749 to 2013 (Full Data Set)&quot;
    )

p2 &lt;- sun_spots %&gt;%
    filter_time(&quot;start&quot; ~ &quot;1800&quot;) %&gt;%
    ggplot(aes(index, value)) +
    geom_line(color = palette_light()[[1]], alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = &quot;loess&quot;, span = 0.2, se = FALSE) +
    theme_tq() +
    labs(
        title = &quot;1749 to 1759 (Zoomed In To Show Changes over the Year)&quot;,
        caption = &quot;datasets::sunspot.month&quot;
    )

p_title &lt;- ggdraw() + 
  draw_label(&quot;Sunspots&quot;, size = 18, fontface = &quot;bold&quot;, 
             colour = palette_light()[[1]])

plot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/cowplot.png" width="352" /></p>
</div>
<h3 id="backtesting-time-series-cross-validation">Backtesting: time series cross validation</h3>
<p>When doing cross validation on sequential data, the time dependencies on preceding samples must be preserved. We can create a cross validation sampling plan by offsetting the window used to select sequential sub-samples. In essence, we’re creatively dealing with the fact that there’s no future test data available by creating multiple synthetic “futures” - a process often, esp. in finance, called “backtesting”.</p>
<p>As mentioned in the introduction, the <a href="https://cran.r-project.org/package=rsample">rsample</a> package includes facitlities for backtesting on time series. The vignette, <a href="https://tidymodels.github.io/rsample/articles/Applications/Time_Series.html">“Time Series Analysis Example”</a>, describes a procedure that uses the <code>rolling_origin()</code> function to create samples designed for time series cross validation. We’ll use this approach.</p>
<h4 id="developing-a-backtesting-strategy">Developing a backtesting strategy</h4>
<p>The sampling plan we create uses 100 years (<code>initial</code> = 12 x 100 samples) for the training set and 50 years (<code>assess</code> = 12 x 50) for the testing (validation) set. We select a <code>skip</code> span of about 22 years (<code>skip</code> = 12 x 22 - 1) to approximately evenly distribute the samples into 6 sets that span the entire 265 years of sunspots history. Last, we select <code>cumulative = FALSE</code> to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) over those operating on less recent data. The tibble return contains the <code>rolling_origin_resamples</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
periods_train &lt;- 12 * 100
periods_test  &lt;- 12 * 50
skip_span     &lt;- 12 * 22 - 1

rolling_origin_resamples &lt;- rolling_origin(
  sun_spots,
  initial    = periods_train,
  assess     = periods_test,
  cumulative = FALSE,
  skip       = skip_span
)

rolling_origin_resamples</code></pre>
</div>
<pre><code>
# Rolling origin forecast resampling 
# A tibble: 6 x 2
  splits       id    
  &lt;list&gt;       &lt;chr&gt; 
1 &lt;S3: rsplit&gt; Slice1
2 &lt;S3: rsplit&gt; Slice2
3 &lt;S3: rsplit&gt; Slice3
4 &lt;S3: rsplit&gt; Slice4
5 &lt;S3: rsplit&gt; Slice5
6 &lt;S3: rsplit&gt; Slice6</code></pre>
<h4 id="visualizing-the-backtesting-strategy">Visualizing the backtesting strategy</h4>
<p>We can visualize the resamples with two custom functions. The first, <code>plot_split()</code>, plots one of the resampling splits using <code>ggplot2</code>. Note that an <code>expand_y_axis</code> argument is added to expand the date range to the full <code>sun_spots</code> dataset date range. This will become useful when we visualize all plots together.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# Plotting function for a single split
plot_split &lt;- function(split, expand_y_axis = TRUE, 
                       alpha = 1, size = 1, base_size = 14) {
    
    # Manipulate data
    train_tbl &lt;- training(split) %&gt;%
        add_column(key = &quot;training&quot;) 
    
    test_tbl  &lt;- testing(split) %&gt;%
        add_column(key = &quot;testing&quot;) 
    
    data_manipulated &lt;- bind_rows(train_tbl, test_tbl) %&gt;%
        as_tbl_time(index = index) %&gt;%
        mutate(key = fct_relevel(key, &quot;training&quot;, &quot;testing&quot;))
        
    # Collect attributes
    train_time_summary &lt;- train_tbl %&gt;%
        tk_index() %&gt;%
        tk_get_timeseries_summary()
    
    test_time_summary &lt;- test_tbl %&gt;%
        tk_index() %&gt;%
        tk_get_timeseries_summary()
    
    # Visualize
    g &lt;- data_manipulated %&gt;%
        ggplot(aes(x = index, y = value, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue(&quot;Split: {split$id}&quot;),
          subtitle = glue(&quot;{train_time_summary$start} to &quot;, 
                          &quot;{test_time_summary$end}&quot;),
            y = &quot;&quot;, x = &quot;&quot;
        ) +
        theme(legend.position = &quot;none&quot;) 
    
    if (expand_y_axis) {
        
        sun_spots_time_summary &lt;- sun_spots %&gt;% 
            tk_index() %&gt;% 
            tk_get_timeseries_summary()
        
        g &lt;- g +
            scale_x_date(limits = c(sun_spots_time_summary$start, 
                                    sun_spots_time_summary$end))
    }
    
    g
}</code></pre>
</div>
<p>The <code>plot_split()</code> function takes one split (in this case Slice01), and returns a visual of the sampling strategy. We expand the axis to the range for the full dataset using <code>expand_y_axis = TRUE</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rolling_origin_resamples$splits[[1]] %&gt;%
    plot_split(expand_y_axis = TRUE) +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="images/slice1.png" width="262" /></p>
</div>
<p>The second function, <code>plot_sampling_plan()</code>, scales the <code>plot_split()</code> function to all of the samples using <code>purrr</code> and <code>cowplot</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# Plotting function that scales to all splits 
plot_sampling_plan &lt;- function(sampling_tbl, expand_y_axis = TRUE, 
                               ncol = 3, alpha = 1, size = 1, base_size = 14, 
                               title = &quot;Sampling Plan&quot;) {
    
    # Map plot_split() to sampling_tbl
    sampling_tbl_with_plots &lt;- sampling_tbl %&gt;%
        mutate(gg_plots = map(splits, plot_split, 
                              expand_y_axis = expand_y_axis,
                              alpha = alpha, base_size = base_size))
    
    # Make plots with cowplot
    plot_list &lt;- sampling_tbl_with_plots$gg_plots 
    
    p_temp &lt;- plot_list[[1]] + theme(legend.position = &quot;bottom&quot;)
    legend &lt;- get_legend(p_temp)
    
    p_body  &lt;- plot_grid(plotlist = plot_list, ncol = ncol)
    
    p_title &lt;- ggdraw() + 
        draw_label(title, size = 14, fontface = &quot;bold&quot;, 
                   colour = palette_light()[[1]])
    
    g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, 
                   rel_heights = c(0.05, 1, 0.05))
    
    g
    
}</code></pre>
</div>
<p>We can now visualize the entire backtesting strategy with <code>plot_sampling_plan()</code>. We can see how the sampling plan shifts the sampling window with each progressive slice of the train/test splits.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rolling_origin_resamples %&gt;%
    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = &quot;Backtesting Strategy: Rolling Origin Sampling Plan&quot;)</code></pre>
</div>
<p><br/></p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/all_splits.png" width="400" /></p>
</div>
<p>And, we can set <code>expand_y_axis = FALSE</code> to zoom in on the samples.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rolling_origin_resamples %&gt;%
    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = &quot;Backtesting Strategy: Zoomed In&quot;)</code></pre>
</div>
<p><br/></p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/all_splits_zoomed.png" width="400" /></p>
</div>
<p>We’ll use this backtesting strategy (6 samples from one time series each with 50/10 split in years and a ~20 year offset) when testing the veracity of the LSTM model on the sunspots dataset.</p>
<h2 id="the-lstm-model">The LSTM model</h2>
<p>To begin, we’ll develop an LSTM model on a single sample from the backtesting strategy, namely, the most recent slice. We’ll then apply the model to all samples to investigate modeling performance.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
example_split    &lt;- rolling_origin_resamples$splits[[6]]
example_split_id &lt;- rolling_origin_resamples$id[[6]]</code></pre>
</div>
<p>We can reuse the <code>plot_split()</code> function to visualize the split. Set <code>expand_y_axis = FALSE</code> to zoom in on the subsample.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot_split(example_split, expand_y_axis = FALSE, size = 0.5) +
    theme(legend.position = &quot;bottom&quot;) +
    ggtitle(glue(&quot;Split: {example_split_id}&quot;))</code></pre>
</div>
<p><img src="images/slice6.png" /></p>
<h3 id="data-setup">Data setup</h3>
<p>To aid hyperparameter tuning, besides the training set we also need a validation set. For example, we will use a callback, <code>callback_early_stopping</code>, that stops training when no significant performance is seen on the validation set (what’s considered significant is up to you).</p>
<p>We will dedicate 2 thirds of the analysis set to training, and 1 third to validation.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
df_trn &lt;- analysis(example_split)[1:800, , drop = FALSE]
df_val &lt;- analysis(example_split)[801:1200, , drop = FALSE]
df_tst &lt;- assessment(example_split)</code></pre>
</div>
<p>First, let’s combine the training and testing data sets into a single data set with a column <code>key</code> that specifies where they came from (either “training” or “testing)”. Note that the <code>tbl_time</code> object will need to have the index respecified during the <code>bind_rows()</code> step, but <a href="https://github.com/tidyverse/dplyr/issues/3259">this issue</a> should be corrected in <code>dplyr</code> soon.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
df &lt;- bind_rows(
  df_trn %&gt;% add_column(key = &quot;training&quot;),
  df_val %&gt;% add_column(key = &quot;validation&quot;),
  df_tst %&gt;% add_column(key = &quot;testing&quot;)
) %&gt;%
  as_tbl_time(index = index)

df</code></pre>
</div>
<pre><code>
# A time tibble: 1,800 x 3
# Index: index
   index      value key     
   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;   
 1 1849-06-01  81.1 training
 2 1849-07-01  78   training
 3 1849-08-01  67.7 training
 4 1849-09-01  93.7 training
 5 1849-10-01  71.5 training
 6 1849-11-01  99   training
 7 1849-12-01  97   training
 8 1850-01-01  78   training
 9 1850-02-01  89.4 training
10 1850-03-01  82.6 training
# ... with 1,790 more rows</code></pre>
<h3 id="preprocessing-with-recipes">Preprocessing with recipes</h3>
<p>The LSTM algorithm will usually work better if the input data has been centered and scaled. We can conveniently accomplish this using the <code>recipes</code> package. In addition to <code>step_center</code> and <code>step_scale</code>, we’re using <code>step_sqrt</code> to reduce variance and remov outliers. The actual transformations are executed when we <code>bake</code> the data according to the recipe:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rec_obj &lt;- recipe(value ~ ., df) %&gt;%
    step_sqrt(value) %&gt;%
    step_center(value) %&gt;%
    step_scale(value) %&gt;%
    prep()

df_processed_tbl &lt;- bake(rec_obj, df)

df_processed_tbl</code></pre>
</div>
<pre><code>
# A tibble: 1,800 x 3
   index      value key     
   &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;   
 1 1849-06-01 0.714 training
 2 1849-07-01 0.660 training
 3 1849-08-01 0.473 training
 4 1849-09-01 0.922 training
 5 1849-10-01 0.544 training
 6 1849-11-01 1.01  training
 7 1849-12-01 0.974 training
 8 1850-01-01 0.660 training
 9 1850-02-01 0.852 training
10 1850-03-01 0.739 training
# ... with 1,790 more rows</code></pre>
<p>Next, let’s capture the original center and scale so we can invert the steps after modeling. The square root step can then simply be undone by squaring the back-transformed data.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
center_history &lt;- rec_obj$steps[[2]]$means[&quot;value&quot;]
scale_history  &lt;- rec_obj$steps[[3]]$sds[&quot;value&quot;]

c(&quot;center&quot; = center_history, &quot;scale&quot; = scale_history)</code></pre>
</div>
<pre><code>
center.value  scale.value 
    6.694468     3.238935 </code></pre>
<h3 id="reshaping-the-data">Reshaping the data</h3>
<p>Keras LSTM expects the input as well as the target data to be in a specific shape. The input has to be a 3-d array of size <code>num_samples, num_timesteps, num_features</code>.</p>
<p>Here, <code>num_samples</code> is the number of observations in the set. This will get fed to the model in portions of <code>batch_size</code>. The second dimension, <code>num_timesteps</code>, is the length of the hidden state we were talking about above. Finally, the third dimension is the number of predictors we’re using. For univariate time series, this is 1.</p>
<p>How long should we choose the hidden state to be? This generally depends on the dataset and our goal. If we did one-step-ahead forecasts - thus, forecasting the following month only - our main concern would be choosing a state length that allows to learn any patterns present in the data.</p>
<p>Now say we wanted to forecast 12 months instead, as does <a href="http://sidc.be/silso/home">SILSO</a>, the <em>World Data Center for the production, preservation and dissemination of the international sunspot number</em>. The way we can do this, with Keras, is by wiring the LSTM hidden states to sets of consecutive outputs of the same length. Thus, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12.</p>
<p>These 12 time steps will then get wired to 12 linear predictor units using a <code>time_distributed()</code> wrapper. That wrapper’s task is to apply the same calculation (i.e., the same weight matrix) to every state input it receives.</p>
<p>Now, what’s the target array’s format supposed to be? As we’re forecasting several timesteps here, the target data again needs to be 3-dimensional. Dimension 1 again is the batch dimension, dimension 2 again corresponds to the number of timesteps (the forecasted ones), and dimension 3 is the size of the wrapped layer. In our case, the wrapped layer is a <code>layer_dense()</code> of a single unit, as we want exactly one prediction per point in time.</p>
<p>So, let’s reshape the data. The main action here is creating the sliding windows of 12 steps of input, followed by 12 steps of output each. This is easiest to understand with a shorter and simpler example. Say our input were the numbers from 1 to 10, and our chosen sequence length (state size) were 4. Tthis is how we would want our training input to look:</p>
<pre><code>
1,2,3,4
2,3,4,5
3,4,5,6</code></pre>
<p>And our target data, correspondingly:</p>
<pre><code>
5,6,7,8
6,7,8,9
7,8,9,10</code></pre>
<p>We’ll define a short function that does this reshaping on a given dataset. Then finally, we add the third axis that is formally needed (even though that axis is of size 1 in our case).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# these variables are being defined just because of the order in which
# we present things in this post (first the data, then the model)
# they will be superseded by FLAGS$n_timesteps, FLAGS$batch_size and n_predictions
# in the following snippet
n_timesteps &lt;- 12
n_predictions &lt;- n_timesteps
batch_size &lt;- 10

# functions used
build_matrix &lt;- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d &lt;- function(X) {
  dim(X) &lt;- c(dim(X)[1], dim(X)[2], 1)
  X
}

# extract values from data frame
train_vals &lt;- df_processed_tbl %&gt;%
  filter(key == &quot;training&quot;) %&gt;%
  select(value) %&gt;%
  pull()
valid_vals &lt;- df_processed_tbl %&gt;%
  filter(key == &quot;validation&quot;) %&gt;%
  select(value) %&gt;%
  pull()
test_vals &lt;- df_processed_tbl %&gt;%
  filter(key == &quot;testing&quot;) %&gt;%
  select(value) %&gt;%
  pull()


# build the windowed matrices
train_matrix &lt;-
  build_matrix(train_vals, n_timesteps + n_predictions)
valid_matrix &lt;-
  build_matrix(valid_vals, n_timesteps + n_predictions)
test_matrix &lt;- build_matrix(test_vals, n_timesteps + n_predictions)

# separate matrices into training and testing parts
# also, discard last batch if there are fewer than batch_size samples
# (a purely technical requirement)
X_train &lt;- train_matrix[, 1:n_timesteps]
y_train &lt;- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_train &lt;- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]
y_train &lt;- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]

X_valid &lt;- valid_matrix[, 1:n_timesteps]
y_valid &lt;- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_valid &lt;- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]
y_valid &lt;- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]

X_test &lt;- test_matrix[, 1:n_timesteps]
y_test &lt;- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_test &lt;- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]
y_test &lt;- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]
# add on the required third axis
X_train &lt;- reshape_X_3d(X_train)
X_valid &lt;- reshape_X_3d(X_valid)
X_test &lt;- reshape_X_3d(X_test)

y_train &lt;- reshape_X_3d(y_train)
y_valid &lt;- reshape_X_3d(y_valid)
y_test &lt;- reshape_X_3d(y_test)</code></pre>
</div>
<h3 id="building-the-lstm-model">Building the LSTM model</h3>
<p>Now that we have our data in the required form, let’s finally build the model. As always in deep learning, an important, and often time-consuming, part of the job is tuning hyperparameters. To keep this post self-contained, and considering this is primarily a tutorial on how to use LSTM in R, let’s assume the following settings were found after extensive experimentation (in reality experimentation <em>did</em> take place, but not to a degree that performance couldn’t possibly be improved).</p>
<p>Instead of hard coding the hyperparameters, we’ll use <a href="https://tensorflow.rstudio.com/tools/tfruns/articles/tuning.html">tfruns</a> to set up an environment where we could easily perform grid search.</p>
<p>We’ll quickly comment on what these parameters do but mainly leave those topics to further posts.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
FLAGS &lt;- flags(
  # There is a so-called &quot;stateful LSTM&quot; in Keras. While LSTM is stateful
  # per se, this adds a further tweak where the hidden states get 
  # initialized with values from the item at same position in the previous
  # batch. This is helpful just under specific circumstances, or if you want
  # to create an &quot;infinite stream&quot; of states, in which case you&#39;d use 1 as 
  # the batch size. Below, we show how the code would have to be changed to
  # use this, but it won&#39;t be further discussed here.
  flag_boolean(&quot;stateful&quot;, FALSE),
  # Should we use several layers of LSTM?
  # Again, just included for completeness, it did not yield any superior 
  # performance on this task.
  # This will actually stack exactly one additional layer of LSTM units.
  flag_boolean(&quot;stack_layers&quot;, FALSE),
  # number of samples fed to the model in one go
  flag_integer(&quot;batch_size&quot;, 10),
  # size of the hidden state, equals size of predictions
  flag_integer(&quot;n_timesteps&quot;, 12),
  # how many epochs to train for
  flag_integer(&quot;n_epochs&quot;, 100),
  # fraction of the units to drop for the linear transformation of the inputs
  flag_numeric(&quot;dropout&quot;, 0.2),
  # fraction of the units to drop for the linear transformation of the 
  # recurrent state
  flag_numeric(&quot;recurrent_dropout&quot;, 0.2),
  # loss function. Found to work better for this specific case than mean
  # squared error
  flag_string(&quot;loss&quot;, &quot;logcosh&quot;),
  # optimizer = stochastic gradient descent. Seemed to work better than adam 
  # or rmsprop here (as indicated by limited testing)
  flag_string(&quot;optimizer_type&quot;, &quot;sgd&quot;),
  # size of the LSTM layer
  flag_integer(&quot;n_units&quot;, 128),
  # learning rate
  flag_numeric(&quot;lr&quot;, 0.003),
  # momentum, an additional parameter to the SGD optimizer
  flag_numeric(&quot;momentum&quot;, 0.9),
  # parameter to the early stopping callback
  flag_integer(&quot;patience&quot;, 10)
)

# the number of predictions we&#39;ll make equals the length of the hidden state
n_predictions &lt;- FLAGS$n_timesteps
# how many features = predictors we have
n_features &lt;- 1
# just in case we wanted to try different optimizers, we could add here
optimizer &lt;- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr, 
                                        momentum = FLAGS$momentum)
                    )

# callbacks to be passed to the fit() function
# We just use one here: we may stop before n_epochs if the loss on the
# validation set does not decrease (by a configurable amount, over a 
# configurable time)
callbacks &lt;- list(
  callback_early_stopping(patience = FLAGS$patience)
)</code></pre>
</div>
<p>After all these preparations, the code for constructing and training the model is rather short! Let’s first quickly view the “long version”, that would allow you to test stacking several LSTMs or use a stateful LSTM, then go through the final short version (that does neither) and comment on it.</p>
<p>This, just for reference, is the complete code.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential()

model %&gt;%
  layer_lstm(
    units = FLAGS$n_units,
    batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    return_sequences = TRUE,
    stateful = FLAGS$stateful
  )

if (FLAGS$stack_layers) {
  model %&gt;%
    layer_lstm(
      units = FLAGS$n_units,
      dropout = FLAGS$dropout,
      recurrent_dropout = FLAGS$recurrent_dropout,
      return_sequences = TRUE,
      stateful = FLAGS$stateful
    )
}
model %&gt;% time_distributed(layer_dense(units = 1))

model %&gt;%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    metrics = list(&quot;mean_squared_error&quot;)
  )

if (!FLAGS$stateful) {
  model %&gt;% fit(
    x          = X_train,
    y          = y_train,
    validation_data = list(X_valid, y_valid),
    batch_size = FLAGS$batch_size,
    epochs     = FLAGS$n_epochs,
    callbacks = callbacks
  )
  
} else {
  for (i in 1:FLAGS$n_epochs) {
    model %&gt;% fit(
      x          = X_train,
      y          = y_train,
      validation_data = list(X_valid, y_valid),
      callbacks = callbacks,
      batch_size = FLAGS$batch_size,
      epochs     = 1,
      shuffle    = FALSE
    )
    model %&gt;% reset_states()
  }
}

if (FLAGS$stateful)
  model %&gt;% reset_states()</code></pre>
</div>
<p>Now let’s step through the simpler, yet better (or equally) performing configuration below.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# create the model
model &lt;- keras_model_sequential()

# add layers
# we have just two, the LSTM and the time_distributed 
model %&gt;%
  layer_lstm(
    units = FLAGS$n_units, 
    # the first layer in a model needs to know the shape of the input data
    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    # by default, an LSTM just returns the final state
    return_sequences = TRUE
  ) %&gt;% time_distributed(layer_dense(units = 1))

model %&gt;%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    # in addition to the loss, Keras will inform us about current 
    # MSE while training
    metrics = list(&quot;mean_squared_error&quot;)
  )

history &lt;- model %&gt;% fit(
  x          = X_train,
  y          = y_train,
  validation_data = list(X_valid, y_valid),
  batch_size = FLAGS$batch_size,
  epochs     = FLAGS$n_epochs,
  callbacks = callbacks
)</code></pre>
</div>
<p>As we see, training was stopped after ~55 epochs as validation loss did not decrease any more. We also see that performance on the validation set is way worse than performance on the training set - normally indicating overfitting.</p>
<p>This topic too, we’ll leave to a separate discussion another time, but interestingly regularization using higher values of <code>dropout</code> and <code>recurrent_dropout</code> (combined with increasing model capacity) did not yield better generalization performance. This is probably related to the characteristics of this specific time series we mentioned in the introduction.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot(history, metrics = &quot;loss&quot;)</code></pre>
</div>
<p><img src="images/history.png" /></p>
<p>Now let’s see how well the model was able to capture the characteristics of the training set.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
pred_train &lt;- model %&gt;%
  predict(X_train, batch_size = FLAGS$batch_size) %&gt;%
  .[, , 1]

# Retransform values to original scale
pred_train &lt;- (pred_train * scale_history + center_history) ^2
compare_train &lt;- df %&gt;% filter(key == &quot;training&quot;)

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_train)) {
  varname &lt;- paste0(&quot;pred_train&quot;, i)
  compare_train &lt;-
    mutate(compare_train,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_train[i,],
      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}</code></pre>
</div>
<p>We compute the average RSME over all sequences of predictions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
coln &lt;- colnames(compare_train)[4:ncol(compare_train)]
cols &lt;- map(coln, quo(sym(.)))
rsme_train &lt;-
  map_dbl(cols, function(col)
    rmse(
      compare_train,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&gt;% mean()

rsme_train</code></pre>
</div>
<pre><code>
21.01495</code></pre>
<p>How do these predictions really look? As a visualization of all predicted sequences would look pretty crowded, we arbitrarily pick start points at regular intervals.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggplot(compare_train, aes(x = index, y = value)) + geom_line() +
  geom_line(aes(y = pred_train1), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_train50), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train100), color = &quot;green&quot;) +
  geom_line(aes(y = pred_train150), color = &quot;violet&quot;) +
  geom_line(aes(y = pred_train200), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_train250), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train300), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train350), color = &quot;green&quot;) +
  geom_line(aes(y = pred_train400), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_train450), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train500), color = &quot;green&quot;) +
  geom_line(aes(y = pred_train550), color = &quot;violet&quot;) +
  geom_line(aes(y = pred_train600), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_train650), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train700), color = &quot;red&quot;) +
  geom_line(aes(y = pred_train750), color = &quot;green&quot;) +
  ggtitle(&quot;Predictions on the training set&quot;)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/pred_train.png" width="400" /></p>
</div>
<p>This looks pretty good. From the validation loss, we don’t quite expect the same from the test set, though.</p>
<p>Let’s see.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
pred_test &lt;- model %&gt;%
  predict(X_test, batch_size = FLAGS$batch_size) %&gt;%
  .[, , 1]

# Retransform values to original scale
pred_test &lt;- (pred_test * scale_history + center_history) ^2
pred_test[1:10, 1:5] %&gt;% print()
compare_test &lt;- df %&gt;% filter(key == &quot;testing&quot;)

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_test)) {
  varname &lt;- paste0(&quot;pred_test&quot;, i)
  compare_test &lt;-
    mutate(compare_test,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_test[i,],
      rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}

compare_test %&gt;% write_csv(str_replace(model_path, &quot;.hdf5&quot;, &quot;.test.csv&quot;))
compare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %&gt;% print()

coln &lt;- colnames(compare_test)[4:ncol(compare_test)]
cols &lt;- map(coln, quo(sym(.)))
rsme_test &lt;-
  map_dbl(cols, function(col)
    rmse(
      compare_test,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&gt;% mean()

rsme_test</code></pre>
</div>
<pre><code>
31.31616</code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggplot(compare_test, aes(x = index, y = value)) + geom_line() +
  geom_line(aes(y = pred_test1), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_test50), color = &quot;red&quot;) +
  geom_line(aes(y = pred_test100), color = &quot;green&quot;) +
  geom_line(aes(y = pred_test150), color = &quot;violet&quot;) +
  geom_line(aes(y = pred_test200), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_test250), color = &quot;red&quot;) +
  geom_line(aes(y = pred_test300), color = &quot;green&quot;) +
  geom_line(aes(y = pred_test350), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_test400), color = &quot;red&quot;) +
  geom_line(aes(y = pred_test450), color = &quot;green&quot;) +  
  geom_line(aes(y = pred_test500), color = &quot;cyan&quot;) +
  geom_line(aes(y = pred_test550), color = &quot;violet&quot;) +
  ggtitle(&quot;Predictions on test set&quot;)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/pred_test.png" width="400" /></p>
</div>
<p>That’s not as good as on the training set, but not bad either, given this time series is quite challenging.</p>
<p>Having defined and run our model on a manually chosen example split, let’s now revert to our overall re-sampling frame.</p>
<h3 id="backtesting-the-model-on-all-splits">Backtesting the model on all splits</h3>
<p>To obtain predictions on all splits, we move the above code into a function and apply it to all splits. First, here’s the function. It returns a list of two dataframes, one for the training and test sets each, that contain the model’s predictions together with the actual values.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
obtain_predictions &lt;- function(split) {
  df_trn &lt;- analysis(split)[1:800, , drop = FALSE]
  df_val &lt;- analysis(split)[801:1200, , drop = FALSE]
  df_tst &lt;- assessment(split)
  
  df &lt;- bind_rows(
    df_trn %&gt;% add_column(key = &quot;training&quot;),
    df_val %&gt;% add_column(key = &quot;validation&quot;),
    df_tst %&gt;% add_column(key = &quot;testing&quot;)
  ) %&gt;%
    as_tbl_time(index = index)
  
  rec_obj &lt;- recipe(value ~ ., df) %&gt;%
    step_sqrt(value) %&gt;%
    step_center(value) %&gt;%
    step_scale(value) %&gt;%
    prep()
  
  df_processed_tbl &lt;- bake(rec_obj, df)
  
  center_history &lt;- rec_obj$steps[[2]]$means[&quot;value&quot;]
  scale_history  &lt;- rec_obj$steps[[3]]$sds[&quot;value&quot;]
  
  FLAGS &lt;- flags(
    flag_boolean(&quot;stateful&quot;, FALSE),
    flag_boolean(&quot;stack_layers&quot;, FALSE),
    flag_integer(&quot;batch_size&quot;, 10),
    flag_integer(&quot;n_timesteps&quot;, 12),
    flag_integer(&quot;n_epochs&quot;, 100),
    flag_numeric(&quot;dropout&quot;, 0.2),
    flag_numeric(&quot;recurrent_dropout&quot;, 0.2),
    flag_string(&quot;loss&quot;, &quot;logcosh&quot;),
    flag_string(&quot;optimizer_type&quot;, &quot;sgd&quot;),
    flag_integer(&quot;n_units&quot;, 128),
    flag_numeric(&quot;lr&quot;, 0.003),
    flag_numeric(&quot;momentum&quot;, 0.9),
    flag_integer(&quot;patience&quot;, 10)
  )
  
  n_predictions &lt;- FLAGS$n_timesteps
  n_features &lt;- 1
  
  optimizer &lt;- switch(FLAGS$optimizer_type,
                      sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum))
  callbacks &lt;- list(
    callback_early_stopping(patience = FLAGS$patience)
  )
  
  train_vals &lt;- df_processed_tbl %&gt;%
    filter(key == &quot;training&quot;) %&gt;%
    select(value) %&gt;%
    pull()
  valid_vals &lt;- df_processed_tbl %&gt;%
    filter(key == &quot;validation&quot;) %&gt;%
    select(value) %&gt;%
    pull()
  test_vals &lt;- df_processed_tbl %&gt;%
    filter(key == &quot;testing&quot;) %&gt;%
    select(value) %&gt;%
    pull()
  
  train_matrix &lt;-
    build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)
  valid_matrix &lt;-
    build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)
  test_matrix &lt;-
    build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)
  
  X_train &lt;- train_matrix[, 1:FLAGS$n_timesteps]
  y_train &lt;-
    train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_train &lt;-
    X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_train &lt;-
    y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_valid &lt;- valid_matrix[, 1:FLAGS$n_timesteps]
  y_valid &lt;-
    valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_valid &lt;-
    X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_valid &lt;-
    y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_test &lt;- test_matrix[, 1:FLAGS$n_timesteps]
  y_test &lt;-
    test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_test &lt;-
    X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_test &lt;-
    y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_train &lt;- reshape_X_3d(X_train)
  X_valid &lt;- reshape_X_3d(X_valid)
  X_test &lt;- reshape_X_3d(X_test)
  
  y_train &lt;- reshape_X_3d(y_train)
  y_valid &lt;- reshape_X_3d(y_valid)
  y_test &lt;- reshape_X_3d(y_test)
  
  model &lt;- keras_model_sequential()
  
  model %&gt;%
    layer_lstm(
      units            = FLAGS$n_units,
      batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
      dropout = FLAGS$dropout,
      recurrent_dropout = FLAGS$recurrent_dropout,
      return_sequences = TRUE
    )     %&gt;% time_distributed(layer_dense(units = 1))
  
  model %&gt;%
    compile(
      loss = FLAGS$loss,
      optimizer = optimizer,
      metrics = list(&quot;mean_squared_error&quot;)
    )
  
  model %&gt;% fit(
    x          = X_train,
    y          = y_train,
    validation_data = list(X_valid, y_valid),
    batch_size = FLAGS$batch_size,
    epochs     = FLAGS$n_epochs,
    callbacks = callbacks
  )
  
  
  pred_train &lt;- model %&gt;%
    predict(X_train, batch_size = FLAGS$batch_size) %&gt;%
    .[, , 1]
  
  # Retransform values
  pred_train &lt;- (pred_train * scale_history + center_history) ^ 2
  compare_train &lt;- df %&gt;% filter(key == &quot;training&quot;)
  
  for (i in 1:nrow(pred_train)) {
    varname &lt;- paste0(&quot;pred_train&quot;, i)
    compare_train &lt;-
      mutate(compare_train, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_train[i, ],
        rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  
  pred_test &lt;- model %&gt;%
    predict(X_test, batch_size = FLAGS$batch_size) %&gt;%
    .[, , 1]
  
  # Retransform values
  pred_test &lt;- (pred_test * scale_history + center_history) ^ 2
  compare_test &lt;- df %&gt;% filter(key == &quot;testing&quot;)
  
  for (i in 1:nrow(pred_test)) {
    varname &lt;- paste0(&quot;pred_test&quot;, i)
    compare_test &lt;-
      mutate(compare_test, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_test[i, ],
        rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  list(train = compare_train, test = compare_test)
  
}</code></pre>
</div>
<p>Mapping the function over all splits yields a list of predictions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
all_split_preds &lt;- rolling_origin_resamples %&gt;%
     mutate(predict = map(splits, obtain_predictions))</code></pre>
</div>
<p>Calculate RMSE on all splits:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
calc_rmse &lt;- function(df) {
  coln &lt;- colnames(df)[4:ncol(df)]
  cols &lt;- map(coln, quo(sym(.)))
  map_dbl(cols, function(col)
    rmse(
      df,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&gt;% mean()
}

all_split_preds &lt;- all_split_preds %&gt;% unnest(predict)
all_split_preds_train &lt;- all_split_preds[seq(1, 11, by = 2), ]
all_split_preds_test &lt;- all_split_preds[seq(2, 12, by = 2), ]

all_split_rmses_train &lt;- all_split_preds_train %&gt;%
  mutate(rmse = map_dbl(predict, calc_rmse)) %&gt;%
  select(id, rmse)

all_split_rmses_test &lt;- all_split_preds_test %&gt;%
  mutate(rmse = map_dbl(predict, calc_rmse)) %&gt;%
  select(id, rmse)</code></pre>
</div>
<p>How does it look? Here’s RMSE on the training set for the 6 splits.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
all_split_rmses_train</code></pre>
</div>
<pre><code>
# A tibble: 6 x 2
  id      rmse
  &lt;chr&gt;  &lt;dbl&gt;
1 Slice1  22.2
2 Slice2  20.9
3 Slice3  18.8
4 Slice4  23.5
5 Slice5  22.1
6 Slice6  21.1</code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
all_split_rmses_test</code></pre>
</div>
<pre><code>
# A tibble: 6 x 2
  id      rmse
  &lt;chr&gt;  &lt;dbl&gt;
1 Slice1  21.6
2 Slice2  20.6
3 Slice3  21.3
4 Slice4  31.4
5 Slice5  35.2
6 Slice6  31.4</code></pre>
<p>Looking at these numbers, we see something interesting: Generalization performance is much better for the first three slices of the time series than for the latter ones. This confirms our impression, stated above, that there seems to be some hidden development going on, rendering forecasting more difficult.</p>
<p>And here are visualizations of the predictions on the respective training and test sets.</p>
<p>First, the training sets:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot_train &lt;- function(slice, name) {
  ggplot(slice, aes(x = index, y = value)) + geom_line() +
    geom_line(aes(y = pred_train1), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_train50), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train100), color = &quot;green&quot;) +
    geom_line(aes(y = pred_train150), color = &quot;violet&quot;) +
    geom_line(aes(y = pred_train200), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_train250), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train300), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train350), color = &quot;green&quot;) +
    geom_line(aes(y = pred_train400), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_train450), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train500), color = &quot;green&quot;) +
    geom_line(aes(y = pred_train550), color = &quot;violet&quot;) +
    geom_line(aes(y = pred_train600), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_train650), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train700), color = &quot;red&quot;) +
    geom_line(aes(y = pred_train750), color = &quot;green&quot;) +
    ggtitle(name)
}

train_plots &lt;- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train)
p_body_train  &lt;- plot_grid(plotlist = train_plots, ncol = 3)
p_title_train &lt;- ggdraw() + 
  draw_label(&quot;Backtested Predictions: Training Sets&quot;, size = 18, fontface = &quot;bold&quot;)

plot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05))</code></pre>
</div>
<p><br/></p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/backtested_train.png" width="400" /></p>
</div>
<p>And the test sets:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot_test &lt;- function(slice, name) {
  ggplot(slice, aes(x = index, y = value)) + geom_line() +
    geom_line(aes(y = pred_test1), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_test50), color = &quot;red&quot;) +
    geom_line(aes(y = pred_test100), color = &quot;green&quot;) +
    geom_line(aes(y = pred_test150), color = &quot;violet&quot;) +
    geom_line(aes(y = pred_test200), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_test250), color = &quot;red&quot;) +
    geom_line(aes(y = pred_test300), color = &quot;green&quot;) +
    geom_line(aes(y = pred_test350), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_test400), color = &quot;red&quot;) +
    geom_line(aes(y = pred_test450), color = &quot;green&quot;) +  
    geom_line(aes(y = pred_test500), color = &quot;cyan&quot;) +
    geom_line(aes(y = pred_test550), color = &quot;violet&quot;) +
    ggtitle(name)
}

test_plots &lt;- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test)

p_body_test  &lt;- plot_grid(plotlist = test_plots, ncol = 3)
p_title_test &lt;- ggdraw() + 
  draw_label(&quot;Backtested Predictions: Test Sets&quot;, size = 18, fontface = &quot;bold&quot;)

plot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05))</code></pre>
</div>
<p><br/></p>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/backtested_test.png" width="400" /></p>
</div>
<p>This has been a long post, and necessarily will have left a lot of questions open, first and foremost: How do we obtain good settings for the hyperparameters (learning rate, number of epochs, dropout)? How do we choose the length of the hidden state? Or even, can we have an intuition how well LSTM will perform on a given dataset (with its specific characteristics)? We will tackle questions like the above in upcoming posts.</p>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2018-06-25-sunspots-lstm/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Predicting%20Sunspot%20Frequency%20with%20Keras&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-06-25-sunspots-lstm%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-06-25-sunspots-lstm%2F&amp;title=Predicting%20Sunspot%20Frequency%20with%20Keras">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/';
  this.page.identifier = 'posts/2018-06-25-sunspots-lstm/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    document.querySelector("a[href='#category:R']").parentNode.style.display = "None";
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerText == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Dancho &amp; Keydana (2018, June 25). RStudio AI Blog: Predicting Sunspot Frequency with Keras. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{dancho2018predicting,
  author = {Dancho, Matt and Keydana, Sigrid},
  title = {RStudio AI Blog: Predicting Sunspot Frequency with Keras},
  url = {https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/},
  year = {2018}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
