<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Attention-based Image Captioning with Keras</title>

<meta property="description" itemprop="description" content="Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2018-09-17"/>
<meta property="article:created" itemprop="dateCreated" content="2018-09-17"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Attention-based Image Captioning with Keras"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png"/>
<meta property="og:image:width" content="627"/>
<meta property="og:image:height" content="269"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Attention-based Image Captioning with Keras"/>
<meta property="twitter:description" content="Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png"/>
<meta property="twitter:image:width" content="627"/>
<meta property="twitter:image:height" content="269"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Attention-based Image Captioning with Keras"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2018/09/17"/>
<meta name="citation_publication_date" content="2018/09/17"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Show, attend and tell: Neural image caption generation with visual attention;citation_publication_date=2015;citation_volume=abs/1502.03044;citation_author=Kelvin Xu;citation_author=Jimmy Ba;citation_author=Ryan Kiros;citation_author=Kyunghyun Cho;citation_author=Aaron C. Courville;citation_author=Ruslan Salakhutdinov;citation_author=Richard S. Zemel;citation_author=Yoshua Bengio"/>
  <meta name="citation_reference" content="citation_title=Bottom-up and top-down attention for image captioning and VQA;citation_publication_date=2017;citation_volume=abs/1707.07998;citation_author=Peter Anderson;citation_author=Xiaodong He;citation_author=Chris Buehler;citation_author=Damien Teney;citation_author=Mark Johnson;citation_author=Stephen Gould;citation_author=Lei Zhang"/>
  <meta name="citation_reference" content="citation_title=A multimodal attentive translator for image captioning;citation_publication_date=2017;citation_volume=abs/1702.05658;citation_author=Chang Liu;citation_author=Fuchun Sun;citation_author=Changhu Wang;citation_author=Feng Wang;citation_author=Alan L. Yuille"/>
  <meta name="citation_reference" content="citation_title=A topic-guided attention for image captioning;citation_publication_date=2018;citation_volume=abs/1807.03514v1;citation_author=Zhihao Zhu;citation_author=Zhan Xue;citation_author=Zejian Yuan"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","bibliography","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Attention-based Image Captioning with Keras"]},{"type":"character","attributes":{},"value":["Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["http://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["09-17-2018"]},{"type":"character","attributes":{},"value":["Natural Language Processing","TensorFlow/Keras","Image Recognition & Image Processing"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/showattendandtell.png"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","image_captioning_files/bowser-1.9.3/bowser.min.js","image_captioning_files/distill-2.2.21/template.v2.js","image_captioning_files/jquery-1.11.3/jquery.min.js","image_captioning_files/webcomponents-2.0.0/webcomponents.js","images/attention.png","images/COCO_train2014_000000089158.jpg","images/COCO_train2014_000000118167.jpg","images/COCO_train2014_000000510592.jpg","images/showattendandtell.png","images/training_examples.png","images/validation_examples.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createIndex() {
  var options = {
    keys: [
      "title",
      "categories",
      "description",
      "contents"
    ]
  };
  return new window.Fuse([],options);
}

function createFuseIndex() {

  // create fuse index
  var options = { keys: ["title", "description", "contents"] };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
            keys: [
              { name: 'title', weight: 20 },
              { name: 'categories', weight: 15 },
              { name: 'description', weight: 10 },
              { name: 'contents', weight: 5 },
            ],
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
            .filter(function(item) { return !!item.description; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description}
                </div>
                <div class="search-item-preview">
                  <img src="${suggestion.preview ? offsetURL(suggestion.preview) : ''}"</img>
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: hidden;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Attention-based Image Captioning with Keras","description":"Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"http://www.rstudio.com/","orcidID":""}],"publishedDate":"2018-09-17T00:00:00.000+00:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Attention-based Image Captioning with Keras</h1>
<p><p>Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="http://www.rstudio.com/" class="uri">http://www.rstudio.com/</a>
  
<br/>09-17-2018
</div>

<div class="d-article">
<p>In image captioning, an algorithm is given an image and tasked with producing a sensible caption. It is a challenging task for several reasons, not the least being that it involves a notion of <em>saliency</em> or <em>relevance</em>. This is why recent deep learning approaches mostly include some “attention” mechanism (sometimes even more than one) to help focusing on relevant image features.</p>
<p>In this post, we demonstrate a formulation of image captioning as an encoder-decoder problem, enhanced by spatial attention over image grid cells. The idea comes from a recent paper on <em>Neural Image Caption Generation with Visual Attention</em> <span class="citation" data-cites="XuBKCCSZB15">(Xu et al. <a href="#ref-XuBKCCSZB15" role="doc-biblioref">2015</a>)</span>, and employs the same kind of attention algorithm as detailed in our post on <a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/">machine translation</a>.</p>
<p>We’re porting Python code from a recent <a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb?linkId=54343050&amp;pli=1#scrollTo=io7ws3ReRPGv">Google Colaboratory notebook</a>, using Keras with TensorFlow eager execution to simplify our lives.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>The code shown here will work with the current CRAN versions of <code>tensorflow</code>, <code>keras</code>, and <code>tfdatasets</code>. Check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tensorflow)
install_tensorflow()</code></pre>
</div>
<p>will get you version 1.10.</p>
<p>When loading libraries, please make sure you’re executing the first 4 lines in this exact order. We need to make sure we’re using the TensorFlow implementation of Keras (<code>tf.keras</code> in Python land), and we have to enable eager execution before using TensorFlow in any way.</p>
<p>No need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager-image-captioning.R">eager-image-captioning.R</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
use_implementation(&quot;tensorflow&quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &quot;silent&quot;)

np &lt;- import(&quot;numpy&quot;)

library(tfdatasets)
library(purrr)
library(stringr)
library(glue)
library(rjson)
library(rlang)
library(dplyr)
library(magick)</code></pre>
</div>
<h2 id="the-dataset">The dataset</h2>
<p><a href="http://cocodataset.org">MS-COCO</a> (“Common Objects in Context”) is one of, perhaps <em>the</em>, reference dataset in image captioning (object detection and segmentation, too). We’ll be using the <a href="http://images.cocodataset.org/zips/train2014.zip">training images</a> and <a href="http://images.cocodataset.org/annotations/annotations_trainval2014.zip">annotations</a> from 2014 - be warned, depending on your location, the download can take a <em>long</em> time.</p>
<p>After unpacking, let’s define where the images and captions are.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
annotation_file &lt;- &quot;train2014/annotations/captions_train2014.json&quot;
image_path &lt;- &quot;train2014/train2014&quot;</code></pre>
</div>
<p>The annotations are in JSON format, and there are 414113 of them! Luckily for us we didn’t have to download that many images - every image comes with 5 different captions, for better generalizability.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
annotations &lt;- fromJSON(file = annotation_file)
annot_captions &lt;- annotations[[4]]

num_captions &lt;- length(annot_captions)</code></pre>
</div>
<p>We store both annotations and image paths in lists, for later loading.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
all_captions &lt;- vector(mode = &quot;list&quot;, length = num_captions)
all_img_names &lt;- vector(mode = &quot;list&quot;, length = num_captions)

for (i in seq_len(num_captions)) {
  caption &lt;- paste0(&quot;&lt;start&gt; &quot;,
                    annot_captions[[i]][[&quot;caption&quot;]],
                    &quot; &lt;end&gt;&quot;
                    )
  image_id &lt;- annot_captions[[i]][[&quot;image_id&quot;]]
  full_coco_image_path &lt;- sprintf(
    &quot;%s/COCO_train2014_%012d.jpg&quot;,
    image_path,
    image_id
  )
  all_img_names[[i]] &lt;- full_coco_image_path
  all_captions[[i]] &lt;- caption
}</code></pre>
</div>
<p>Depending on your computing environment, you will for sure want to restrict the number of examples used. This post will use 30000 captioned images, chosen randomly, and set aside 20% for validation.</p>
<p>Below, we take random samples, split into training and validation parts. The companion code will also store the indices on disk, so you can pick up on verification and analysis later.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
num_examples &lt;- 30000

random_sample &lt;- sample(1:num_captions, size = num_examples)
train_indices &lt;- sample(random_sample, size = length(random_sample) * 0.8)
validation_indices &lt;- setdiff(random_sample, train_indices)

sample_captions &lt;- all_captions[random_sample]
sample_images &lt;- all_img_names[random_sample]
train_captions &lt;- all_captions[train_indices]
train_images &lt;- all_img_names[train_indices]
validation_captions &lt;- all_captions[validation_indices]
validation_images &lt;- all_img_names[validation_indices]</code></pre>
</div>
<h2 id="interlude">Interlude</h2>
<p>Before really diving into the technical stuff, let’s take a moment to reflect on this task. In typical image-related deep learning walk-throughs, we’re used to seeing well-defined problems - even if in some cases, the solution may be hard. Take, for example, the stereotypical <em>dog vs. cat</em> problem. Some dogs may look like cats and some cats may look like dogs, but that’s about it: All in all, in the usual world we live in, it should be a more or less binary question.</p>
<p>If, on the other hand, we ask people to describe what they see in a scene, it’s to be expected from the outset that we’ll get different answers. Still, how much consensus there is will very much depend on the concrete dataset we’re using.</p>
<p>Let’s take a look at some picks from the very first 20 training items sampled randomly above.</p>
<figure>
<img src="images/COCO_train2014_000000510592.jpg" class="external" style="width:50.0%" alt="" /><figcaption>Figure from MS-COCO 2014</figcaption>
</figure>
<p>Now this image does not leave much room for decision what to focus on, and received a very factual caption indeed: “There is a plate with one slice of bacon a half of orange and bread”. If the dataset were all like this, we’d think a machine learning algorithm should do pretty well here.</p>
<p>Picking another one from the first 20:</p>
<figure>
<img src="images/COCO_train2014_000000118167.jpg" class="external" style="width:50.0%" alt="" /><figcaption>Figure from MS-COCO 2014</figcaption>
</figure>
<p>What would be salient information to you here? The caption provided goes “A smiling little boy has a checkered shirt”. Is the look of the shirt as important as that? You might as well focus on the scenery, - or even something on a completely different level: The age of the photo, or it being an analog one.</p>
<p>Let’s take a final example.</p>
<figure>
<img src="images/COCO_train2014_000000089158.jpg" class="external" style="width:70.0%" alt="" /><figcaption>From MS-COCO 2014</figcaption>
</figure>
<p>What would you say about this scene? The official label we sampled here is “A group of people posing in a funny way for the camera”. Well …</p>
<p>Please don’t forget that for each image, the dataset includes five different captions (although our n = 30000 samples probably won’t). So this is not saying the dataset is biased - not at all. Instead, we want to point out the ambiguities and difficulties inherent in the task. Actually, given those difficulties, it’s all the more amazing that the task we’re tackling here - having a network automatically generate image captions - should be possible at all!</p>
<p>Now let’s see how we can do this.</p>
<h2 id="extract-image-features">Extract image features</h2>
<p>For the encoding part of our encoder-decoder network, we will make use of <em>InceptionV3</em> to extract image features. In principle, which features to extract is up to experimentation, - here we just use the last layer before the fully connected top:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
image_model &lt;- application_inception_v3(
  include_top = FALSE,
  weights = &quot;imagenet&quot;
)</code></pre>
</div>
<p>For an image size of 299x299, the output will be of size <code>(batch_size, 8, 8, 2048)</code>, that is, we are making use of 2048 feature maps.</p>
<p><em>InceptionV3</em> being a “big model”, where every pass through the model takes time, we want to precompute features in advance and store them on disk. We’ll use <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a> to stream images to the model. This means all our preprocessing has to employ tensorflow functions: That’s why we’re not using the more familiar <code>image_load</code> from keras below.</p>
<p>Our custom <code>load_image</code> will read in, resize and preprocess the images as required for use with <em>InceptionV3</em>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
load_image &lt;- function(image_path) {
  img &lt;-
    tf$read_file(image_path) %&gt;%
    tf$image$decode_jpeg(channels = 3) %&gt;%
    tf$image$resize_images(c(299L, 299L)) %&gt;%
    tf$keras$applications$inception_v3$preprocess_input()
  list(img, image_path)
}</code></pre>
</div>
<p>Now we’re ready to save the extracted features to disk. The <code>(batch_size, 8, 8, 2048)</code>-sized features will be flattened to <code>(batch_size, 64, 2048)</code>. The latter shape is what our encoder, soon to be discussed, will receive as input.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
preencode &lt;- unique(sample_images) %&gt;% unlist() %&gt;% sort()
num_unique &lt;- length(preencode)

# adapt this according to your system&#39;s capacities  
batch_size_4save &lt;- 1
image_dataset &lt;-
  tensor_slices_dataset(preencode) %&gt;%
  dataset_map(load_image) %&gt;%
  dataset_batch(batch_size_4save)
  
save_iter &lt;- make_iterator_one_shot(image_dataset)
  
until_out_of_range({
  
  save_count &lt;- save_count + batch_size_4save
  batch_4save &lt;- save_iter$get_next()
  img &lt;- batch_4save[[1]]
  path &lt;- batch_4save[[2]]
  batch_features &lt;- image_model(img)
  batch_features &lt;- tf$reshape(
    batch_features,
    list(dim(batch_features)[1], -1L, dim(batch_features)[4]
  )
                               )
  for (i in 1:dim(batch_features)[1]) {
    np$save(path[i]$numpy()$decode(&quot;utf-8&quot;),
            batch_features[i, , ]$numpy())
  }
    
})</code></pre>
</div>
<p>Before we get to the encoder and decoder models though, we need to take care of the captions.</p>
<h2 id="processing-the-captions">Processing the captions</h2>
<p>We’re using keras <code>text_tokenizer</code> and the text processing functions <code>texts_to_sequences</code> and <code>pad_sequences</code> to transform ascii text into a matrix.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# we will use the 5000 most frequent words only
top_k &lt;- 5000
tokenizer &lt;- text_tokenizer(
  num_words = top_k,
  oov_token = &quot;&lt;unk&gt;&quot;,
  filters = &#39;!&quot;#$%&amp;()*+.,-/:;=?@[\\]^_`{|}~ &#39;)
tokenizer$fit_on_texts(sample_captions)

train_captions_tokenized &lt;-
  tokenizer %&gt;% texts_to_sequences(train_captions)
validation_captions_tokenized &lt;-
  tokenizer %&gt;% texts_to_sequences(validation_captions)

# pad_sequences will use 0 to pad all captions to the same length
tokenizer$word_index[&quot;&lt;pad&gt;&quot;] &lt;- 0

# create a lookup dataframe that allows us to go in both directions
word_index_df &lt;- data.frame(
  word = tokenizer$word_index %&gt;% names(),
  index = tokenizer$word_index %&gt;% unlist(use.names = FALSE),
  stringsAsFactors = FALSE
)
word_index_df &lt;- word_index_df %&gt;% arrange(index)

decode_caption &lt;- function(text) {
  paste(map(text, function(number)
    word_index_df %&gt;%
      filter(index == number) %&gt;%
      select(word) %&gt;%
      pull()),
    collapse = &quot; &quot;)
}

# pad all sequences to the same length (the maximum length, in our case)
# could experiment with shorter padding (truncating the very longest captions)
caption_lengths &lt;- map(
  all_captions[1:num_examples],
  function(c) str_split(c,&quot; &quot;)[[1]] %&gt;% length()
  ) %&gt;% unlist()
max_length &lt;- fivenum(caption_lengths)[5]

train_captions_padded &lt;-  pad_sequences(
  train_captions_tokenized,
  maxlen = max_length,
  padding = &quot;post&quot;,
  truncating = &quot;post&quot;
)

validation_captions_padded &lt;- pad_sequences(
  validation_captions_tokenized,
  maxlen = max_length,
  padding = &quot;post&quot;,
  truncating = &quot;post&quot;
)</code></pre>
</div>
<h2 id="loading-the-data-for-training">Loading the data for training</h2>
<p>Now that we’ve taken care of pre-extracting the features and preprocessing the captions, we need a way to stream them to our captioning model. For that, we’re using <code>tensor_slices_dataset</code> from <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a>, passing in the list of paths to the images and the preprocessed captions. Loading the images is then performed as a TensorFlow graph operation (using <a href="https://www.tensorflow.org/api_docs/python/tf/py_func">tf$pyfunc</a>).</p>
<p>The original Colab code also shuffles the data on every iteration. Depending on your hardware, this may take a long time, and given the size of the dataset it is not strictly necessary to get reasonable results. (The results reported below were obtained without shuffling.)</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
batch_size &lt;- 10
buffer_size &lt;- num_examples

map_func &lt;- function(img_name, cap) {
  p &lt;- paste0(img_name$decode(&quot;utf-8&quot;), &quot;.npy&quot;)
  img_tensor &lt;- np$load(p)
  img_tensor &lt;- tf$cast(img_tensor, tf$float32)
  list(img_tensor, cap)
}

train_dataset &lt;-
  tensor_slices_dataset(list(train_images, train_captions_padded)) %&gt;%
  dataset_map(
    function(item1, item2) tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))
  ) %&gt;%
  # optionally shuffle the dataset
  # dataset_shuffle(buffer_size) %&gt;%
  dataset_batch(batch_size)</code></pre>
</div>
<h2 id="captioning-model">Captioning model</h2>
<p>The model is basically the same as that discussed in the <a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/">machine translation post</a>. Please refer to that article for an explanation of the concepts, as well as a detailed walk-through of the tensor shapes involved at every step. Here, we provide the tensor shapes as comments in the code snippets, for quick overview/comparison.</p>
<p>However, if you develop your own models, with eager execution you can simply insert debugging/logging statements at arbitrary places in the code - even in model definitions. So you can have a function</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
maybecat &lt;- function(context, x) {
  if (debugshapes) {
    name &lt;- enexpr(x)
    dims &lt;- paste0(dim(x), collapse = &quot; &quot;)
    cat(context, &quot;: shape of &quot;, name, &quot;: &quot;, dims, &quot;\n&quot;, sep = &quot;&quot;)
  }
}</code></pre>
</div>
<p>And if you now set</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
debugshapes &lt;- FALSE</code></pre>
</div>
<p>you can trace - not only tensor shapes, but actual tensor values through your models, as shown below for the encoder. (We don’t display any debugging statements after that, but the <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager-image-captioning.R">sample code</a> has many more.)</p>
<h3 id="encoder">Encoder</h3>
<p>Now it’s time to define some some sizing-related hyperparameters and housekeeping variables:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# for encoder output
embedding_dim &lt;- 256
# decoder (LSTM) capacity
gru_units &lt;- 512
# for decoder output
vocab_size &lt;- top_k
# number of feature maps gotten from Inception V3
features_shape &lt;- 2048
# shape of attention features (flattened from 8x8)
attention_features_shape &lt;- 64</code></pre>
</div>
<p>The encoder in this case is just a fully connected layer, taking in the features extracted from Inception V3 (in flattened form, as they were written to disk), and embedding them in 256-dimensional space.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
cnn_encoder &lt;- function(embedding_dim, name = NULL) {
    
  keras_model_custom(name = name, function(self) {
      
    self$fc &lt;- layer_dense(units = embedding_dim, activation = &quot;relu&quot;)
      
    function(x, mask = NULL) {
      # input shape: (batch_size, 64, features_shape)
      maybecat(&quot;encoder input&quot;, x)
      # shape after fc: (batch_size, 64, embedding_dim)
      x &lt;- self$fc(x)
      maybecat(&quot;encoder output&quot;, x)
      x
    }
  })
}</code></pre>
</div>
<h3 id="attention-module">Attention module</h3>
<p>Unlike in the machine translation post, here the attention module is separated out into its own custom model. The logic is the same though:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
attention_module &lt;- function(gru_units, name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$W1 = layer_dense(units = gru_units)
    self$W2 = layer_dense(units = gru_units)
    self$V = layer_dense(units = 1)
      
    function(inputs, mask = NULL) {
      features &lt;- inputs[[1]]
      hidden &lt;- inputs[[2]]
      # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)
      # hidden shape == (batch_size, gru_units)
      # hidden_with_time_axis shape == (batch_size, 1, gru_units)
      hidden_with_time_axis &lt;- k_expand_dims(hidden, axis = 2)
        
      # score shape == (batch_size, 64, 1)
      score &lt;- self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))
      # attention_weights shape == (batch_size, 64, 1)
      attention_weights &lt;- k_softmax(score, axis = 2)
      # context_vector shape after sum == (batch_size, embedding_dim)
      context_vector &lt;- k_sum(attention_weights * features, axis = 2)
        
      list(context_vector, attention_weights)
    }
  })
}</code></pre>
</div>
<h3 id="decoder">Decoder</h3>
<p>The decoder at each time step calls the attention module with the features it got from the encoder and its last hidden state, and receives back an attention vector. The attention vector gets concatenated with the current input and further processed by a GRU and two fully connected layers, the last of which gives us the (unnormalized) probabilities for the next word in the caption.</p>
<p>The <em>current input</em> at each time step here is the previous word: the correct one during training (<em>teacher forcing</em>), the last generated one during inference.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rnn_decoder &lt;- function(embedding_dim, gru_units, vocab_size, name = NULL) {
    
  keras_model_custom(name = name, function(self) {
      
    self$gru_units &lt;- gru_units
    self$embedding &lt;- layer_embedding(input_dim = vocab_size, 
                                      output_dim = embedding_dim)
    self$gru &lt;- if (tf$test$is_gpu_available()) {
      layer_cudnn_gru(
        units = gru_units,
        return_sequences = TRUE,
        return_state = TRUE,
        recurrent_initializer = &#39;glorot_uniform&#39;
      )
    } else {
      layer_gru(
        units = gru_units,
        return_sequences = TRUE,
        return_state = TRUE,
        recurrent_initializer = &#39;glorot_uniform&#39;
      )
    }
      
    self$fc1 &lt;- layer_dense(units = self$gru_units)
    self$fc2 &lt;- layer_dense(units = vocab_size)
      
    self$attention &lt;- attention_module(self$gru_units)
      
    function(inputs, mask = NULL) {
      x &lt;- inputs[[1]]
      features &lt;- inputs[[2]]
      hidden &lt;- inputs[[3]]
        
      c(context_vector, attention_weights) %&lt;-% 
        self$attention(list(features, hidden))
        
      # x shape after passing through embedding == (batch_size, 1, embedding_dim)
      x &lt;- self$embedding(x)
        
      # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)
      x &lt;- k_concatenate(list(k_expand_dims(context_vector, 2), x))
        
      # passing the concatenated vector to the GRU
      c(output, state) %&lt;-% self$gru(x)
        
      # shape == (batch_size, 1, gru_units)
      x &lt;- self$fc1(output)
        
      # x shape == (batch_size, gru_units)
      x &lt;- k_reshape(x, c(-1, dim(x)[[3]]))
        
      # output shape == (batch_size, vocab_size)
      x &lt;- self$fc2(x)
        
      list(x, state, attention_weights)
        
    }
  })
}</code></pre>
</div>
<h3 id="loss-function-and-instantiating-it-all">Loss function, and instantiating it all</h3>
<p>Now that we’ve defined our model (built of three custom models), we still need to actually instantiate it (being precise: the two classes we will access from outside, that is, the encoder and the decoder).</p>
<p>We also need to instantiate an optimizer (Adam will do), and define our loss function (categorical crossentropy). Note that <code>tf$nn$sparse_softmax_cross_entropy_with_logits</code> expects raw logits instead of softmax activations, and that we’re using the <em>sparse</em> variant because our labels are not one-hot-encoded.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
encoder &lt;- cnn_encoder(embedding_dim)
decoder &lt;- rnn_decoder(embedding_dim, gru_units, vocab_size)

optimizer = tf$train$AdamOptimizer()

cx_loss &lt;- function(y_true, y_pred) {
  mask &lt;- 1 - k_cast(y_true == 0L, dtype = &quot;float32&quot;)
  loss &lt;- tf$nn$sparse_softmax_cross_entropy_with_logits(
    labels = y_true,
    logits = y_pred
  ) * mask
  tf$reduce_mean(loss)
}</code></pre>
</div>
<h2 id="training">Training</h2>
<p>Training the captioning model is a time-consuming process, and you will for sure want to save the model’s weights! How does this work with eager execution?</p>
<p>We create a <code>tf$train$Checkpoint</code> object, passing it the objects to be saved: In our case, the encoder, the decoder, and the optimizer. Later, at the end of each epoch, we will ask it to write the respective weights to disk.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
restore_checkpoint &lt;- FALSE

checkpoint_dir &lt;- &quot;./checkpoints_captions&quot;
checkpoint_prefix &lt;- file.path(checkpoint_dir, &quot;ckpt&quot;)
checkpoint &lt;- tf$train$Checkpoint(
  optimizer = optimizer,
  encoder = encoder,
  decoder = decoder
)</code></pre>
</div>
<p>As we’re just starting to train the model, <code>restore_checkpoint</code> is set to false. Later, restoring the weights will be as easy as</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
if (restore_checkpoint) {
  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))
}</code></pre>
</div>
<p>The training loop is structured just like in the machine translation case: We loop over epochs, batches, and the training targets, feeding in the correct previous word at every timestep. Again, <code>tf$GradientTape</code> takes care of recording the forward pass and calculating the gradients, and the optimizer applies the gradients to the model’s weights. As each epoch ends, we also save the weights.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
num_epochs &lt;- 20

if (!restore_checkpoint) {
  for (epoch in seq_len(num_epochs)) {
    
    total_loss &lt;- 0
    progress &lt;- 0
    train_iter &lt;- make_iterator_one_shot(train_dataset)
    
    until_out_of_range({
      
      batch &lt;- iterator_get_next(train_iter)
      loss &lt;- 0
      img_tensor &lt;- batch[[1]]
      target_caption &lt;- batch[[2]]
      
      dec_hidden &lt;- k_zeros(c(batch_size, gru_units))
      
      dec_input &lt;- k_expand_dims(
        rep(list(word_index_df[word_index_df$word == &quot;&lt;start&gt;&quot;, &quot;index&quot;]), 
            batch_size)
      )
      
      with(tf$GradientTape() %as% tape, {
        
        features &lt;- encoder(img_tensor)
        
        for (t in seq_len(dim(target_caption)[2] - 1)) {
          c(preds, dec_hidden, weights) %&lt;-%
            decoder(list(dec_input, features, dec_hidden))
          loss &lt;- loss + cx_loss(target_caption[, t], preds)
          dec_input &lt;- k_expand_dims(target_caption[, t])
        }
        
      })
      
      total_loss &lt;-
        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])
      
      variables &lt;- c(encoder$variables, decoder$variables)
      gradients &lt;- tape$gradient(loss, variables)
      
      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),
                                global_step = tf$train$get_or_create_global_step()
      )
    })
    cat(paste0(
      &quot;\n\nTotal loss (epoch): &quot;,
      epoch,
      &quot;: &quot;,
      (total_loss / k_cast_to_floatx(buffer_size)) %&gt;% as.double() %&gt;% round(4),
      &quot;\n&quot;
    ))
    
    checkpoint$save(file_prefix = checkpoint_prefix)
  }
}</code></pre>
</div>
<h2 id="peeking-at-results">Peeking at results</h2>
<p>Just like in the translation case, it’s interesting to look at model performance during training. The companion code has that functionality integrated, so you can watch model progress for yourself.</p>
<p>The basic function here is <code>get_caption</code>: It gets passed the path to an image, loads it, obtains its features from Inception V3, and then asks the encoder-decoder model to generate a caption. If at any point the model produces the <code>end</code> symbol, we stop early. Otherwise, we continue until we hit the predefined maximum length.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
get_caption &lt;-
  function(image) {
    attention_matrix &lt;-
      matrix(0, nrow = max_length, ncol = attention_features_shape)
    temp_input &lt;- k_expand_dims(load_image(image)[[1]], 1)
    img_tensor_val &lt;- image_model(temp_input)
    img_tensor_val &lt;- k_reshape(
      img_tensor_val,
      list(dim(img_tensor_val)[1], -1, dim(img_tensor_val)[4])
    )
    features &lt;- encoder(img_tensor_val)
    
    dec_hidden &lt;- k_zeros(c(1, gru_units))
    dec_input &lt;-
      k_expand_dims(
        list(word_index_df[word_index_df$word == &quot;&lt;start&gt;&quot;, &quot;index&quot;])
      )
    
    result &lt;- &quot;&quot;
    
    for (t in seq_len(max_length - 1)) {
      
      c(preds, dec_hidden, attention_weights) %&lt;-%
        decoder(list(dec_input, features, dec_hidden))
      attention_weights &lt;- k_reshape(attention_weights, c(-1))
      attention_matrix[t,] &lt;- attention_weights %&gt;% as.double()
      
      pred_idx &lt;- tf$multinomial(exp(preds), num_samples = 1)[1, 1] 
                    %&gt;% as.double()
      pred_word &lt;-
        word_index_df[word_index_df$index == pred_idx, &quot;word&quot;]
      
      if (pred_word == &quot;&lt;end&gt;&quot;) {
        result &lt;-
          paste(result, pred_word)
        attention_matrix &lt;-
          attention_matrix[1:length(str_split(result, &quot; &quot;)[[1]]), , 
                           drop = FALSE]
        return (list(result, attention_matrix))
      } else {
        result &lt;-
          paste(result, pred_word)
        dec_input &lt;- k_expand_dims(list(pred_idx))
      }
    }
    
    list(str_trim(result), attention_matrix)
  }</code></pre>
</div>
<p>With that functionality, now let’s actually do that: peek at results while the network is learning!</p>
<p>We’ve picked 3 examples each from the training and validation sets. Here they are.</p>
<p>First, our picks from the training set:</p>
<div class="l-body-outset">
<figure>
<img src="images/training_examples.png" style="width:100.0%" alt="" /><figcaption>Three picks from the training set</figcaption>
</figure>
</div>
<p>Let’s see the target captions:</p>
<ul>
<li><em>a herd of giraffe standing on top of a grass covered field</em></li>
<li><em>a view of cards driving down a street</em></li>
<li><em>the skateboarding flips his board off of the sidewalk</em></li>
</ul>
<p>Interestingly, here we also have a demonstration of how labeled datasets (like anything human) may contain errors. (The samples were not picked for that; instead, they were chosen - without too much screening - for being rather unequivocal in their visual content.)</p>
<p>Now for the validation candidates.</p>
<div class="l-body-outset">
<figure>
<img src="images/validation_examples.png" style="width:100.0%" alt="" /><figcaption>Three picks from the validation set</figcaption>
</figure>
</div>
<p>and their official captions:</p>
<ul>
<li><em>a left handed pitcher throwing the base ball</em></li>
<li><em>a woman taking a bite of a slice of pizza in a restaraunt</em></li>
<li><em>a woman hitting swinging a tennis racket at a tennis ball on a tennis court</em></li>
</ul>
<p>(Again, any spelling peculiarities have not been introduced by us.)</p>
<h4 id="epoch-1">Epoch 1</h4>
<p>Now, what does our network produce after the first epoch? Remember that this means, having seen each one of the 24000 training images once.</p>
<p>First then, here are the captions for the train images:</p>
<blockquote>
<p>a group of sheep standing in the grass</p>
</blockquote>
<blockquote>
<p>a group of cars driving down a street</p>
</blockquote>
<blockquote>
<p>a man is standing on a street</p>
</blockquote>
<p>Not only is the syntax correct in every case, the content isn’t that bad either!</p>
<p>How about the validation set?</p>
<blockquote>
<p>a baseball player is playing baseball uniform is holding a baseball bat</p>
</blockquote>
<blockquote>
<p>a man is holding a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table</p>
</blockquote>
<blockquote>
<p>a tennis player is holding a tennis court</p>
</blockquote>
<p>This certainly tells that the network has been able to generalize over - let’s not call them concepts, but mappings between visual and textual entities, say It’s true that it will have seen some of these images before, because images come with several captions. You could be more strict setting up your training and validation sets - but here, we don’t really care about objective performance scores and so, it does not really matter.</p>
<p>Let’ skip directly to epoch 20, our last training epoch, and check for further improvements.</p>
<h4 id="epoch-20">Epoch 20</h4>
<p>This is what we get for the training images:</p>
<blockquote>
<p>a group of many tall giraffe standing next to a sheep</p>
</blockquote>
<blockquote>
<p>a view of cards and white gloves on a street</p>
</blockquote>
<blockquote>
<p>a skateboarding flips his board</p>
</blockquote>
<p>And this, for the validation images.</p>
<blockquote>
<p>a baseball catcher and umpire hit a baseball game</p>
</blockquote>
<blockquote>
<p>a man is eating a sandwich</p>
</blockquote>
<blockquote>
<p>a female tennis player is in the court</p>
</blockquote>
<p>I think we might agree that this still leaves room for improvement - but then, we only trained for 20 epochs and on a very small portion of the dataset.</p>
<p>In the above code snippets, you may have noticed the decoder returning an <code>attention_matrix</code> - but we weren’t commenting on it. Now finally, just as in the translation example, have a look what we can make of that.</p>
<h2 id="where-does-the-network-look">Where does the network look?</h2>
<p>We can visualize where the network is “looking” as it generates each word by overlaying the original image and the attention matrix. This example is taken from the 4th epoch.</p>
<p>Here white-ish squares indicate areas receiving stronger focus. Compared to text-to-text translation though, the mapping is inherently less straightforward - where does one “look” when producing words like “and”, “the”, or “in”?</p>
<div class="l-body-outset">
<figure>
<img src="images/attention.png" style="width:100.0%" alt="" /><figcaption>Attention over image areas</figcaption>
</figure>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>It probably goes without saying that much better results are to be expected when training on (much!) more data and for much more time.</p>
<p>Apart from that, there are other options, though. The concept implemented here uses <em>spatial</em> attention over a uniform grid, that is, the attention mechanism guides the decoder <em>where</em> on the grid to look next when generating a caption.</p>
<p>However, this is not the only way, and this is not how it works with humans. A much more plausible approach is a mix of top-down and bottom-up attention. E.g., <span class="citation" data-cites="AndersonHBTJGZ17">(Anderson et al. <a href="#ref-AndersonHBTJGZ17" role="doc-biblioref">2017</a>)</span> use object detection techniques to bottom-up isolate interesting objects, and an LSTM stack wherein the first LSTM computes top-down attention guided by the output word generated by the second one.</p>
<p>Another interesting approach involving attention is using a multimodal attentive translator <span class="citation" data-cites="LiuSWWY17">(Liu et al. <a href="#ref-LiuSWWY17" role="doc-biblioref">2017</a>)</span>, where the image features are encoded and presented in a sequence, such that we end up with sequence models both on the encoding and the decoding sides.</p>
<p>Another alternative is to add a learned <em>topic</em> to the information input <span class="citation" data-cites="Zhu2018">(Zhu, Xue, and Yuan <a href="#ref-Zhu2018" role="doc-biblioref">2018</a>)</span>, which again is a top-down feature found in human cognition.</p>
<p>If you find one of these, or yet another, approach more convincing, an eager execution implementation, in the style of the above, will likely be a sound way of implementing it.</p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-AndersonHBTJGZ17">
<p>Anderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2017. “Bottom-up and Top-down Attention for Image Captioning and VQA.” <em>CoRR</em> abs/1707.07998. <a href="http://arxiv.org/abs/1707.07998">http://arxiv.org/abs/1707.07998</a>.</p>
</div>
<div id="ref-LiuSWWY17">
<p>Liu, Chang, Fuchun Sun, Changhu Wang, Feng Wang, and Alan L. Yuille. 2017. “A Multimodal Attentive Translator for Image Captioning.” <em>CoRR</em> abs/1702.05658. <a href="http://arxiv.org/abs/1702.05658">http://arxiv.org/abs/1702.05658</a>.</p>
</div>
<div id="ref-XuBKCCSZB15">
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” <em>CoRR</em> abs/1502.03044. <a href="http://arxiv.org/abs/1502.03044">http://arxiv.org/abs/1502.03044</a>.</p>
</div>
<div id="ref-Zhu2018">
<p>Zhu, Zhihao, Zhan Xue, and Zejian Yuan. 2018. “A Topic-Guided Attention for Image Captioning.” <em>CoRR</em> abs/1807.03514v1. <a href="https://arxiv.org/abs/1807.03514v1">https://arxiv.org/abs/1807.03514v1</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2018-09-17-eager-captioning/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Attention-based%20Image%20Captioning%20with%20Keras&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-09-17-eager-captioning%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-09-17-eager-captioning%2F&amp;title=Attention-based%20Image%20Captioning%20with%20Keras">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/';
  this.page.identifier = 'posts/2018-09-17-eager-captioning/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    document.querySelector("a[href='#category:R']").parentNode.style.display = "None";
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerText == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2018, Sept. 17). RStudio AI Blog: Attention-based Image Captioning with Keras. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydana2018attention-based,
  author = {Keydana, Sigrid},
  title = {RStudio AI Blog: Attention-based Image Captioning with Keras},
  url = {https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/},
  year = {2018}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
